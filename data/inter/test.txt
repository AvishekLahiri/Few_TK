We	O
prove	O
that	O
the	O
approximation	O
accuracy	Classification-metrics
measured	O
in	O
a	O
local	O
total	O
variation	O
norm	O
is	O
"	O
dimension	O
-	O
free	O
dimension	O
nse	O
that	O
as	O
the	O
overall	O
dimension	O
of	O
the	O
model	O
increases	O
the	O
error	O
bounds	O
we	O
derive	O
do	O
not	O
necessarily	O
degrade	O
.	O

In	O
various	O
applications	O
,	O
ranging	O
from	O
industrial	O
quality	O
control	O
to	O
public	O
health	O
through	O
credit	O
risk	O
analysis	O
for	O
instance	O
,	O
training	O
observations	O
can	O
be	O
right	O
censored	O
,	O
meaning	O
that	O
,	O
rather	O
than	O
on	O
independent	O
copies	O
of	O
$(	O
X	O
,	O
Y	O
)$,	O
statistical	AI/ML/DL-domain
learning	AI/ML/DL-domain
relies	O
on	O
a	O
collection	O
of	O
$	O
n	O
\	O
geq	O
1	O
$	O
independent	O
realizations	O
of	O
the	O
triplet	O
$(	O
X	O
,	O
\;	O
\	O
min	O
\{	O
Y	O
,\;	O
C	O
\},\;	O
\	O
delta	O
)$,	O
where	O
$	O
C	O
$	O
is	O
a	O
nonnegative	O
random	O
variable	O
with	O
unknown	O
distribution	O
modelling	O
censoring	O
and	O
$\	O
delta	O
=\	O
mathbb	O
{	O
I	O
}\{	O
Y	O
\	O
leq	O
C	O
\}$	O
indicates	O
whether	O
the	O
duration	O
is	O
right	O
censored	O
or	O
not	O
.	O

In	O
this	O
paper	O
,	O
we	O
study	O
two	O
challenging	O
problems	O
in	O
explainable	AI/ML/DL-domain
AI	AI/ML/DL-domain
(	AI/ML/DL-domain
XAI	AI/ML/DL-domain
)	AI/ML/DL-domain
and	O
data	O
clustering	O
.	O

The	O
second	O
is	O
implementing	O
discrete	O
$	O
k	O
$-	O
means	O
with	O
a	O
differentiable	O
neural	O
network	O
that	O
embraces	O
the	O
advantages	O
of	O
parallel	O
computing	O
online	O
clustering	O
and	O
clustering	NLP-focus
-	NLP-focus
favorable	NLP-focus
representation	NLP-focus
learning	NLP-focus
.	O

First	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
most	O
existing	O
XAI	AI/ML/DL-domain
works	O
focus	O
on	O
supervised	O
learning	O
paradigms	O
.	O

This	O
work	O
is	O
one	O
of	O
the	O
few	O
XAI	AI/ML/DL-domain
studies	O
on	O
unsupervised	O
learning	O
in	O
particular	O
,	O
data	O
clustering	O
.	O

In	O
contrast	O
,	O
most	O
existing	O
XAI	AI/ML/DL-domain
studies	O
resort	O
to	O
various	O
means	O
for	O
understanding	O
a	O
black	O
-	O
box	O
model	O
with	O
post	O
-	O
hoc	O
explanations	O
.	O

Deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
uses	O
neural	O
networks	O
which	O
are	O
parameterised	O
by	O
their	O
weights	O
.	O

We	O
also	O
test	O
the	O
theory	O
with	O
CNN	O
image	O
classifiers	O
on	O
several	O
datasets	O
and	O
with	O
GPT	NLP-algorithm/tool
-	NLP-algorithm/tool
type	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

An	O
adequate	O
representation	O
of	O
graph	O
data	O
is	O
vital	O
to	O
the	O
learning	O
performance	O
of	O
a	O
statistical	O
or	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
model	O
for	O
graph	O
-	O
structured	O
data	O
.	O

The	O
effectiveness	O
for	O
constructing	O
the	O
decimated	O
framelet	O
system	O
and	O
the	O
FGT	O
is	O
demonstrated	O
by	O
a	O
simulated	O
example	O
of	O
random	O
graphs	O
and	O
real	O
-	O
world	O
applications	O
,	O
including	O
multiresolution	O
analysis	O
for	O
traffic	O
network	O
and	O
representation	AI/ML/DL-domain
learning	AI/ML/DL-domain
of	O
graph	O
neural	O
networks	O
for	O
graph	O
classification	O
tasks	O
.	O

In	O
the	O
category	O
of	O
“	AI/ML/DL-domain
zero	AI/ML/DL-domain
-	AI/ML/DL-domain
shot	AI/ML/DL-domain
”	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	O
when	O
only	O
the	O
corrupted	O
image	O
is	O
used	O
for	O
training	O
,	O
we	O
observed	O
the	O
evolutionary	O
variational	O
algorithm	O
to	O
significantly	O
improve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
many	O
benchmark	O
settings	O
.	O

We	O
show	O
that	O
the	O
accuracy	Classification-metrics
of	O
approximations	O
lies	O
within	O
$(	O
1	O
+\	O
mathcal	O
{	O
O	O
}({\	O
varepsilon	O
}))$	O
of	O
the	O
true	O
leverage	O
scores	O
with	O
high	O
probability	O
.	O

Decision	O
tree	O
learning	O
is	O
a	O
widely	O
used	O
approach	O
in	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
favoured	O
in	O
applications	O
that	O
require	O
concise	O
and	O
interpretable	O
models	O
Heuristic	O
methods	O
.	O

Heuristic	O
methods	O
are	O
traditionally	O
used	O
to	O
quickly	O
produce	O
models	O
with	O
reasonably	O
high	O
accuracy	Classification-metrics
.	O

A	O
commonly	O
criticised	O
point	O
,	O
however	O
,	O
is	O
that	O
the	O
resulting	O
trees	O
may	O
not	O
necessarily	O
be	O
the	O
best	O
representation	O
of	O
the	O
data	O
in	O
terms	O
of	O
accuracy	Classification-metrics
and	O
size	O
.	O

Many	O
current	O
applications	O
in	O
data	AI/ML/DL-domain
science	AI/ML/DL-domain
need	O
rich	O
model	O
classes	O
to	O
adequately	O
represent	O
the	O
statistics	O
rich	O
model	O
classes	O
the	O
observations	O
.	O

We	O
explore	O
this	O
framework	O
by	O
studying	O
rich	O
model	O
classes	O
that	O
may	O
only	O
admit	O
pointwise	O
consistency	O
guarantees	O
,	O
yet	O
enough	O
information	O
about	O
the	O
unknown	O
model	O
driving	O
the	O
observations	O
needed	O
to	O
gauge	O
estimator	Classification-metrics
accuracy	Classification-metrics
can	O
be	O
inferred	O
from	O
the	O
sample	O
at	O
hand	O
.	O

Moreover	O
,	O
the	O
reformulation	O
of	O
an	O
LIP	O
as	O
the	O
min	O
-	O
max	O
problem	O
provided	O
in	O
this	O
article	O
is	O
crucial	O
in	O
developing	O
methods	O
to	O
solve	O
the	O
dictionary	O
learning	O
problem	O
with	O
almost	O
sure	O
recovery	O
constraints	O
.	O
meta	AI/ML/DL-domain
-	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
new	O
theoretical	O
framework	O
to	O
provide	O
such	O
convergence	O
guarantee	O
for	O
two	O
types	O
of	O
objective	O
functions	O
that	O
are	O
of	O
interest	O
in	O
practice	O
:	O
(	O
a	O
)	O
resampling	O
case	O
(	O
e	O
.	O
g	O
.,	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
,	O
where	O
loss	O
functions	O
take	O
the	O
form	O
in	O
expectation	O
and	O
new	O
data	O
are	O
sampled	O
as	O
the	O
algorithm	O
runs	O
;	O
and	O
(	O
b	O
)	O
finite	O
-	O
sum	O
case	O
(	O
e	O
.	O
g	O
.,	O
supervised	O
learning	O
loss	O
functions	O
nctions	O
take	O
the	O
finite	O
-	O
sum	O
form	O
with	O
given	O
samples	O
.	O

This	O
estimator	O
minimizes	O
a	O
new	O
general	O
excess	O
risk	O
bound	O
for	O
statistical	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

Through	O
both	O
simulation	O
studies	O
and	O
real	O
-	O
world	O
data	O
examples	O
,	O
we	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
in	O
comparison	O
to	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
survival	O
analysis	O
models	O
.	O

We	O
present	O
a	O
novel	O
class	O
of	O
projected	O
methods	O
to	O
perform	O
statistical	AI/ML/DL-domain
analysis	AI/ML/DL-domain
on	O
a	O
data	O
set	O
of	O
probability	O
distributions	O
on	O
the	O
real	O
line	O
,	O
with	O
the	O
2	Statistical/Mathematical-metrics
-	Statistical/Mathematical-metrics
Wasserstein	Statistical/Mathematical-metrics
metric	O
.	O

We	O
present	O
a	O
novel	O
class	O
of	O
projected	O
methods	O
to	O
perform	O
statistical	AI/ML/DL-domain
analysis	AI/ML/DL-domain
on	O
a	O
data	O
set	O
of	O
probability	O
distributions	O
on	O
the	O
real	O
line	O
,	O
with	O
the	O
2	Statistical/Mathematical-metrics
-	Statistical/Mathematical-metrics
Wasserstein	Statistical/Mathematical-metrics
metric	O
.	O

We	O
develop	O
a	O
rigorous	O
and	O
general	O
framework	O
for	O
constructing	O
information	O
-	O
theoretic	O
divergences	O
divergences	O
both	O
$	O
f	O
$-	O
divergences	O
and	O
integral	Statistical/Mathematical-metrics
probability	Statistical/Mathematical-metrics
metrics	Statistical/Mathematical-metrics
(	Statistical/Mathematical-metrics
IPMs	Statistical/Mathematical-metrics
)	Statistical/Mathematical-metrics
such	O
as	O
the	O
$	Statistical/Mathematical-metrics
1	Statistical/Mathematical-metrics
$-	Statistical/Mathematical-metrics
Wasserstein	Statistical/Mathematical-metrics
distance	Statistical/Mathematical-metrics
.	O

The	O
$(	O
f	O
,\	O
Gamma	O
)$-	O
divergences	O
inherit	O
features	O
from	O
IPMs	Statistical/Mathematical-metrics
such	O
as	O
the	O
ability	O
to	O
compare	O
distributions	O
which	O
are	O
not	O
absolutely	O
continuous	O
,	O
as	O
well	O
as	O
from	O
$	O
f	O
$-	O
divergences	O
,	O
namely	O
the	O
strict	O
concavity	O
of	O
their	O
variational	O
representations	O
and	O
the	O
ability	O
to	O
control	O
heavy	O
-	O
tailed	O
distributions	O
for	O
particular	O
choices	O
of	O
$	O
f	O
$.	O

When	O
combined	O
,	O
these	O
features	O
establish	O
a	O
divergence	O
with	O
improved	O
properties	O
for	O
estimation	O
statistical	AI/ML/DL-domain
learning	AI/ML/DL-domain
and	O
uncertainty	O
quantification	O
statistical	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

Our	O
proposal	O
--	O
the	O
Correlation	O
-	O
Assisted	O
Missing	O
data	O
(	O
CAM	O
)	O
estimator	O
--	O
works	O
by	O
exploiting	O
the	O
relationship	O
between	O
the	O
observations	O
with	O
missing	O
features	O
and	O
those	O
without	O
missing	O
features	O
in	O
order	O
to	O
obtain	O
improved	O
prediction	Classification-metrics
accuracy	Classification-metrics
.	O

Our	O
experiments	O
show	O
that	O
conditioning	O
augmentation	O
prevents	O
compounding	O
error	O
during	O
sampling	O
in	O
a	O
cascaded	O
model	O
,	O
helping	O
us	O
to	O
train	O
cascading	O
pipelines	O
achieving	O
FID	Statistical/Mathematical-metrics
scores	O
of	O
1	Numerical-result
.	Numerical-result
48	Numerical-result
at	O
64x64	O
3	Numerical-result
.	Numerical-result
52	Numerical-result
at	O
128x128	O
and	O
4	Numerical-result
.	Numerical-result
88	Numerical-result
at	O
256x256	O
resolutions	O
,	O
outperforming	O
BigGAN	O
-	O
deep	O
and	O
classification	Classification-metrics
accuracy	Classification-metrics
scores	O
of	O
63	Numerical-result
.	Numerical-result
02	Numerical-result
%	Numerical-result
(	O
top	O
-	O
1	O
)	O
and	O
84	Numerical-result
.	Numerical-result
06	Numerical-result
%	Numerical-result
(	O
top	O
-	O
5	O
)	O
at	O
256x256	O
outperforming	O
VQ	O
-	O
VAE	O
-	O
2	O
.	O
parameters	O
.	O

Our	O
experiments	O
show	O
that	O
conditioning	O
augmentation	O
prevents	O
compounding	O
error	O
during	O
sampling	O
in	O
a	O
cascaded	O
model	O
,	O
helping	O
us	O
to	O
train	O
cascading	O
pipelines	O
achieving	O
FID	Statistical/Mathematical-metrics
scores	O
of	O
1	Numerical-result
.	Numerical-result
48	Numerical-result
at	O
64x64	O
3	Numerical-result
.	Numerical-result
52	Numerical-result
at	O
128x128	O
and	O
4	Numerical-result
.	Numerical-result
88	Numerical-result
at	O
256x256	O
resolutions	O
,	O
outperforming	O
BigGAN	O
-	O
deep	O
and	O
classification	Classification-metrics
accuracy	Classification-metrics
scores	O
of	O
63	Numerical-result
.	Numerical-result
02	Numerical-result
%	Numerical-result
(	O
top	O
-	O
1	O
)	O
and	O
84	Numerical-result
.	Numerical-result
06	Numerical-result
%	Numerical-result
(	O
top	O
-	O
5	O
)	O
at	O
256x256	O
outperforming	O
VQ	O
-	O
VAE	O
-	O
2	O
.	O
parameters	O
.	O

Our	O
experiments	O
show	O
that	O
conditioning	O
augmentation	O
prevents	O
compounding	O
error	O
during	O
sampling	O
in	O
a	O
cascaded	O
model	O
,	O
helping	O
us	O
to	O
train	O
cascading	O
pipelines	O
achieving	O
FID	Statistical/Mathematical-metrics
scores	O
of	O
1	Numerical-result
.	Numerical-result
48	Numerical-result
at	O
64x64	O
3	Numerical-result
.	Numerical-result
52	Numerical-result
at	O
128x128	O
and	O
4	Numerical-result
.	Numerical-result
88	Numerical-result
at	O
256x256	O
resolutions	O
,	O
outperforming	O
BigGAN	O
-	O
deep	O
and	O
classification	Classification-metrics
accuracy	Classification-metrics
scores	O
of	O
63	Numerical-result
.	Numerical-result
02	Numerical-result
%	Numerical-result
(	O
top	O
-	O
1	O
)	O
and	O
84	Numerical-result
.	Numerical-result
06	Numerical-result
%	Numerical-result
(	O
top	O
-	O
5	O
)	O
at	O
256x256	O
outperforming	O
VQ	O
-	O
VAE	O
-	O
2	O
.	O
parameters	O
.	O

An	O
long	O
-	O
standing	O
open	O
problem	O
is	O
to	O
find	O
a	O
computationally	O
tractable	O
way	O
to	O
extract	O
an	O
innovations	O
sequence	O
of	O
non	O
-	O
Gaussian	O
processes	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

It	O
contains	O
functionalities	O
for	O
valid	O
statistical	O
inference	O
on	O
causal	O
parameters	O
parameters	O
timation	O
of	O
nuisance	O
parameters	O
is	O
based	O
on	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
methods	O
.	O
DoubleML	O
.	O

Algorithm	O
parameters	O
,	O
in	O
particular	O
hyperparameters	O
of	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
algorithms	O
can	O
substantially	O
impact	O
their	O
performance	O
.	O

Our	O
goal	O
is	O
to	O
provide	O
an	O
easy	O
-	O
to	O
-	O
use	O
library	O
comprising	O
a	O
large	O
amount	O
of	O
Self	AI/ML/DL-domain
-	AI/ML/DL-domain
supervised	AI/ML/DL-domain
Learning	AI/ML/DL-domain
(	AI/ML/DL-domain
SSL	AI/ML/DL-domain
)	AI/ML/DL-domain
methods	O
,	O
that	O
can	O
be	O
easily	O
extended	O
and	O
fine	O
-	O
tuned	O
by	O
the	O
community	O
.	O
solo	O
-	O
learn	O
.	O

solo	O
-	O
learn	O
opens	O
up	O
avenues	O
for	O
exploiting	O
large	O
-	O
budget	O
SSL	AI/ML/DL-domain
SSL	AI/ML/DL-domain
tions	O
on	O
inexpensive	O
smaller	O
infrastructures	O
and	O
seeks	O
to	O
democratize	O
SSL	O
by	O
making	O
it	O
accessible	O
to	O
all	O
.	O
source	O
code	O
.	O

The	O
source	O
code	O
is	O
available	O
at	O
https	O
://	O
github	O
.	O
com	O
/	O
vturrisi	O
/	O
solo	O
-	O
learn	O
.	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

However	O
,	O
the	O
exact	O
tradeoff	O
between	O
fairness	O
and	O
accuracy	Classification-metrics
is	O
not	O
entirely	O
clear	O
,	O
even	O
for	O
the	O
basic	O
paradigm	O
of	O
classification	O
problems	O
.	O

In	O
this	O
paper	O
,	O
we	O
characterize	O
an	O
inherent	O
tradeoff	O
between	O
statistical	Statistical/Mathematical-metrics
parity	Statistical/Mathematical-metrics
and	O
accuracy	Classification-metrics
in	O
the	O
classification	O
setting	O
by	O
providing	O
a	O
lower	O
bound	O
on	O
the	O
sum	O
of	O
group	O
-	O
wise	O
errors	O
of	O
any	O
fair	O
classifiers	O
.	O

In	O
this	O
paper	O
,	O
we	O
characterize	O
an	O
inherent	O
tradeoff	O
between	O
statistical	Statistical/Mathematical-metrics
parity	Statistical/Mathematical-metrics
and	O
accuracy	Classification-metrics
in	O
the	O
classification	O
setting	O
by	O
providing	O
a	O
lower	O
bound	O
on	O
the	O
sum	O
of	O
group	O
-	O
wise	O
errors	O
of	O
any	O
fair	O
classifiers	O
.	O

Our	O
impossibility	O
theorem	O
could	O
be	O
interpreted	O
as	O
a	O
certain	O
uncertainty	O
principle	O
in	O
fairness	O
:	O
if	O
the	O
base	O
rates	O
differ	O
among	O
groups	O
,	O
then	O
any	O
fair	O
classifier	O
satisfying	O
statistical	Statistical/Mathematical-metrics
parity	Statistical/Mathematical-metrics
has	O
to	O
incur	O
a	O
large	O
error	O
on	O
at	O
least	O
one	O
of	O
the	O
groups	O
.	O
lower	O
bound	O
.	O

To	O
show	O
that	O
our	O
lower	O
bound	O
is	O
tight	O
,	O
assuming	O
oracle	O
access	O
to	O
Bayes	O
(	O
potentially	O
unfair	O
)	O
classifiers	O
we	O
also	O
construct	O
an	O
algorithm	O
that	O
returns	O
a	O
randomized	O
classifier	O
which	O
is	O
both	O
optimal	O
(	O
in	O
terms	O
of	O
accuracy	Classification-metrics
and	O
fair	O
.	O

On	O
the	O
upside	O
,	O
we	O
prove	O
that	O
if	O
the	O
group	O
-	O
wise	O
Bayes	O
optimal	O
classifiers	O
are	O
close	O
,	O
then	O
learning	O
fair	O
representations	O
leads	O
to	O
an	O
alternative	O
notion	O
of	O
fairness	O
,	O
known	O
as	O
the	O
accuracy	Classification-metrics
parity	Classification-metrics
which	O
states	O
that	O
the	O
error	O
rates	O
are	O
close	O
between	O
groups	O
.	O

Finally	O
,	O
we	O
also	O
conduct	O
experiments	O
on	O
real	O
-	O
world	O
datasets	O
to	O
confirm	O
our	O
theoretical	O
findings	O
.	O
Latent	NLP-algorithm/tool
Dirichlet	NLP-algorithm/tool
Allocation	NLP-algorithm/tool
machine	AI/ML/DL-domain
-	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

Finally	O
,	O
we	O
also	O
conduct	O
experiments	O
on	O
real	O
-	O
world	O
datasets	O
to	O
confirm	O
our	O
theoretical	O
findings	O
.	O
Latent	NLP-algorithm/tool
Dirichlet	NLP-algorithm/tool
Allocation	NLP-algorithm/tool
machine	AI/ML/DL-domain
-	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

This	O
paper	O
addresses	O
the	O
ongoing	O
concern	O
that	O
formal	O
procedures	O
for	O
determining	O
the	O
optimal	O
LDA	NLP-algorithm/tool
configuration	O
do	O
not	O
exist	O
by	O
introducing	O
a	O
set	O
of	O
parametric	O
tests	O
that	O
rely	O
on	O
the	O
assumed	O
multinomial	O
distribution	O
specification	O
underlying	O
the	O
original	O
LDA	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Our	O
methodology	O
defines	O
a	O
set	O
of	O
rigorous	O
statistical	O
procedures	O
that	O
identify	O
and	O
evaluate	O
the	O
optimal	NLP-algorithm/tool
topic	NLP-algorithm/tool
model	NLP-algorithm/tool
U	O
.	O
S	O
.	O
Presidential	O
Inaugural	O
Address	O
Corpus	O
.	O

We	O
further	O
validate	O
the	O
method	O
through	O
a	O
simulation	O
study	O
confirming	O
the	O
superiority	O
of	O
our	O
approach	O
compared	O
to	O
other	O
standard	O
heuristic	O
metrics	O
like	O
the	O
perplexity	NLP-metrics
index	NLP-metrics
.	O
causal	O
classification	O
.	O

We	O
compare	O
our	O
model	O
against	O
popular	O
alternatives	O
on	O
simulated	O
and	O
real	O
datasets	O
and	O
find	O
the	O
performance	O
is	O
competitive	O
,	O
while	O
the	O
fully	O
Bayesian	O
procedure	O
enables	O
the	O
quantification	O
of	O
model	O
uncertainties	O
.	O
Statistical	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

It	O
is	O
not	O
suitable	O
for	O
use	O
with	O
very	O
small	O
mini	O
-	O
batch	O
sizes	O
or	O
online	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

One	O
benefit	O
is	O
that	O
BNP	O
is	O
not	O
constrained	O
on	O
the	O
mini	O
-	O
batch	O
size	O
and	O
works	O
in	O
the	O
online	AI/ML/DL-domain
learning	AI/ML/DL-domain
BN	O
ting	O
.	O

Under	O
ordinary	O
smoothness	O
assumptions	O
more	O
caution	O
is	O
needed	O
as	O
a	O
polynomial	O
deviation	O
in	O
the	O
sample	O
sizes	O
could	O
drastically	O
deteriorate	O
the	O
convergence	O
to	O
the	O
truth	O
.	O
Clustering	O
unsupervised	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

Clustering	O
problems	O
are	O
fundamental	O
to	O
unsupervised	O
learning	O
.	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
AI	AI/ML/DL-domain
.	O

We	O
first	O
define	O
a	O
functional	O
differential	O
graph	O
that	O
captures	O
the	O
differences	O
between	O
two	O
functional	O
graphical	O
models	O
and	O
formally	O
characterize	O
when	O
the	O
functional	O
differential	O
graph	O
is	O
well	O
defined	O
.	O
FuDGE	Data/Mining/Information/Retrieval-technique
.	O

This	O
is	O
particularly	O
beneficial	O
in	O
settings	O
where	O
the	O
individual	O
graphs	O
are	O
dense	O
but	O
the	O
differential	O
graph	O
is	O
sparse	O
.	O
FuDGE	Data/Mining/Information/Retrieval-technique
functional	O
differential	O
graph	O
.	O

Motivated	O
by	O
analyzing	O
long	O
-	O
term	O
physiological	O
time	O
series	O
,	O
we	O
design	O
a	O
robust	O
and	O
scalable	O
spectral	O
embedding	O
algorithm	O
that	O
we	O
refer	O
to	O
as	O
RObust	Data/Mining/Information/Retrieval-technique
and	Data/Mining/Information/Retrieval-technique
Scalable	Data/Mining/Information/Retrieval-technique
Embedding	Data/Mining/Information/Retrieval-technique
via	Data/Mining/Information/Retrieval-technique
LANdmark	Data/Mining/Information/Retrieval-technique
Diffusion	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
Roseland	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
.	O

The	O
key	O
is	O
designing	O
a	O
diffusion	O
process	O
on	O
the	O
dataset	O
where	O
the	O
diffusion	O
is	O
done	O
via	O
a	O
small	O
subset	O
called	O
the	O
landmark	O
set	O
.	O
Roseland	Data/Mining/Information/Retrieval-technique
.	O

Specifically	O
,	O
when	O
there	O
are	O
$	O
n	O
$	O
data	O
points	O
in	O
$\	O
mathbb	O
{	O
R	O
}^	O
q	O
$	O
and	O
$	O
n	O
^\	O
beta	O
$	O
points	O
in	O
the	O
landmark	O
set	O
,	O
where	O
$\	O
beta	O
\	O
in	O
(	O
0	O
,	O
1	O
)$,	O
the	O
computational	O
complexity	O
of	O
Roseland	Data/Mining/Information/Retrieval-technique
is	O
$	O
O	O
(	O
n	O
^{	O
1	O
+	O
2	O
\	O
beta	O
}+	O
qn	O
^{	O
1	O
+\	O
beta	O
})$,	O
while	O
that	O
of	O
Nystrom	O
Roseland	Data/Mining/Information/Retrieval-technique
2	O
.	O
81	O
\	O
beta	O
}+	O
qn	O
^{	O
1	O
+	O
2	O
\	O
beta	O
})$.	O

To	O
demonstrate	O
the	O
potential	O
of	O
Roseland	O
,	O
we	O
apply	O
it	O
to	O
{	O
three	O
}	O
datasets	O
and	O
compare	O
it	O
with	O
several	O
other	O
existing	O
algorithms	O
Roseland	Data/Mining/Information/Retrieval-technique
spectral	O
clustering	O
MNIST	O
.	O

First	O
,	O
we	O
apply	O
Roseland	O
to	O
the	O
task	O
of	O
spectral	O
clustering	O
using	O
the	O
MNIST	O
dataset	O
(	O
70	O
,	O
000	O
images	O
,	O
achieving	O
85	Numerical-result
\%	Numerical-result
accuracy	Classification-metrics
when	O
the	O
dataset	O
is	O
clean	O
and	O
78	Numerical-result
\%	Numerical-result
accuracy	Classification-metrics
when	O
the	O
dataset	O
is	O
noisy	O
.	O
subsampling	O
.	O

First	O
,	O
we	O
apply	O
Roseland	O
to	O
the	O
task	O
of	O
spectral	O
clustering	O
using	O
the	O
MNIST	O
dataset	O
(	O
70	O
,	O
000	O
images	O
,	O
achieving	O
85	Numerical-result
\%	Numerical-result
accuracy	Classification-metrics
when	O
the	O
dataset	O
is	O
clean	O
and	O
78	Numerical-result
\%	Numerical-result
accuracy	Classification-metrics
when	O
the	O
dataset	O
is	O
noisy	O
.	O
subsampling	O
.	O

Compared	O
with	O
other	O
subsampling	O
schemes	O
,	O
overall	O
Roseland	Data/Mining/Information/Retrieval-technique
achieves	O
a	O
better	O
performance	O
.	O
image	O
segmentation	O
COCO	O
.	O

Second	O
,	O
we	O
apply	O
Roseland	Data/Mining/Information/Retrieval-technique
to	O
the	O
task	O
of	O
image	O
segmentation	O
using	O
images	O
from	O
COCO	O
.	O

Finally	O
,	O
we	O
demonstrate	O
how	O
to	O
apply	O
Roseland	Data/Mining/Information/Retrieval-technique
to	O
explore	O
long	O
-	O
term	O
arterial	O
blood	O
pressure	O
waveform	O
dynamics	O
during	O
a	O
liver	O
transplant	O
operation	O
lasting	O
for	O
12	O
hours	O
.	O

In	O
a	O
wide	O
variety	O
of	O
our	O
simulations	O
,	O
CD	O
-	O
split	O
and	O
HPD	O
-	O
split	O
have	O
better	O
conditional	O
coverage	O
and	O
yield	O
smaller	O
prediction	O
regions	O
than	O
other	O
methods	O
.	O
Error	O
decomposition	O
analysis	O
ensemble	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

The	O
second	O
,	O
graph	O
regularized	O
neural	O
network	O
,	O
leverages	O
graphs	O
to	O
augment	O
neural	O
network	O
losses	O
with	O
a	O
regularization	O
objective	O
for	O
semi	AI/ML/DL-domain
-	AI/ML/DL-domain
supervised	AI/ML/DL-domain
learning	AI/ML/DL-domain
graph	O
neural	O
networks	O
differentiable	O
functions	O
.	O

Specifically	O
,	O
we	O
propose	O
the	O
GraphEDM	O
framework	O
,	O
which	O
generalizes	O
popular	O
algorithms	O
for	O
semi	AI/ML/DL-domain
-	AI/ML/DL-domain
supervised	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	O
e	O
.	O
g	O
.	O
GraphSage	O
GCN	O
GAT	O
unsupervised	O
learning	O
DeepWalk	O
node2vec	O
.	O

Current	O
practice	O
assumes	O
(	O
i	O
)	O
and	O
(	O
ii	O
)	O
are	O
met	O
,	O
then	O
postulates	O
a	O
functional	O
form	O
with	O
limited	O
input	O
from	O
the	O
data	O
.	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

Prediction	O
validity	O
checks	O
that	O
error	O
terms	O
--	O
which	O
should	O
be	O
independent	O
from	O
the	O
instrument	O
--	O
cannot	O
be	O
modeled	O
with	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
any	O
better	O
than	O
a	O
model	O
that	O
is	O
identically	O
zero	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
Prediction	O
for	O
Enormous	O
and	O
Correlated	O
Output	O
Spaces	O
(	O
PECOS	O
)	O
framework	O
,	O
a	O
versatile	O
and	O
modular	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
framework	O
for	O
solving	O
prediction	O
problems	O
for	O
very	O
large	O
output	O
spaces	O
,	O
and	O
apply	O
it	O
to	O
the	O
eXtreme	O
Multilabel	O
Ranking	O
(	O
XMR	O
)	O
PECOS	O
PECOS	O
en	O
an	O
input	O
instance	O
,	O
find	O
and	O
rank	O
the	O
most	O
relevant	O
items	O
from	O
an	O
enormous	O
but	O
fixed	O
and	O
finite	O
output	O
space	O
.	O

When	O
applied	O
to	O
eXtreme	O
Multilabel	O
Ranking	O
where	O
the	O
input	O
instances	O
are	O
in	O
textual	O
form	O
,	O
we	O
find	O
that	O
the	O
recursive	O
Transformer	O
matcher	O
gives	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Classification-metrics
results	O
,	O
at	O
the	O
cost	O
of	O
two	O
orders	O
of	O
magnitude	O
increased	O
training	O
time	O
compared	O
to	O
the	O
recursive	O
linear	O
matcher	O
dataset	O
.	O

For	O
example	O
,	O
on	O
a	O
dataset	O
where	O
the	O
output	O
space	O
is	O
of	O
size	O
2	O
.	O
8	O
million	O
,	O
the	O
recursive	O
Transformer	O
matcher	O
results	O
in	O
a	O
6	Numerical-result
%	Numerical-result
increase	O
in	O
precision	Classification-metrics
@	Classification-metrics
1	Classification-metrics
(	O
from	O
48	Numerical-result
.	Numerical-result
6	Numerical-result
%	Numerical-result
to	O
54	O
.	O
2	O
%)	O
over	O
the	O
recursive	O
linear	O
matcher	O
but	O
takes	O
100x	O
more	O
time	O
to	O
train	O
.	O

For	O
example	O
,	O
on	O
a	O
dataset	O
where	O
the	O
output	O
space	O
is	O
of	O
size	O
2	O
.	O
8	O
million	O
,	O
the	O
recursive	O
Transformer	O
matcher	O
results	O
in	O
a	O
6	Numerical-result
%	Numerical-result
increase	O
in	O
precision	Classification-metrics
@	Classification-metrics
1	Classification-metrics
(	O
from	O
48	Numerical-result
.	Numerical-result
6	Numerical-result
%	Numerical-result
to	O
54	O
.	O
2	O
%)	O
over	O
the	O
recursive	O
linear	O
matcher	O
but	O
takes	O
100x	O
more	O
time	O
to	O
train	O
.	O

Retrieval	NLP-algorithm/tool
Augment	NLP-algorithm/tool
Generation	NLP-algorithm/tool
(	NLP-algorithm/tool
RAG	NLP-algorithm/tool
)	NLP-algorithm/tool
is	O
a	O
recent	O
advancement	O
in	O
Open	NLP-focus
-	NLP-focus
Domain	NLP-focus
Question	NLP-focus
Answering	NLP-focus
(	NLP-focus
ODQA	NLP-focus
)	NLP-focus
.	O

Retrieval	NLP-algorithm/tool
Augment	NLP-algorithm/tool
Generation	NLP-algorithm/tool
(	NLP-algorithm/tool
RAG	NLP-algorithm/tool
)	NLP-algorithm/tool
is	O
a	O
recent	O
advancement	O
in	O
Open	NLP-focus
-	NLP-focus
Domain	NLP-focus
Question	NLP-focus
Answering	NLP-focus
(	NLP-focus
ODQA	NLP-focus
)	NLP-focus
.	O

RAG	NLP-algorithm/tool
has	O
only	O
been	O
trained	O
and	O
explored	O
with	O
a	O
Wikipedia	O
-	O
based	O
external	O
knowledge	O
base	O
and	O
is	O
not	O
optimized	O
for	O
use	O
in	O
other	O
specialized	O
domains	O
such	O
as	O
healthcare	O
and	O
news	O
.	O

In	O
this	O
paper	O
,	O
we	O
evaluate	O
the	O
impact	O
of	O
joint	O
training	O
of	O
the	O
retriever	O
and	O
generator	O
components	O
of	O
RAG	NLP-algorithm/tool
for	O
the	O
task	O
of	O
domain	O
adaptation	O
in	O
ODQA	NLP-focus
.	O

In	O
this	O
paper	O
,	O
we	O
evaluate	O
the	O
impact	O
of	O
joint	O
training	O
of	O
the	O
retriever	O
and	O
generator	O
components	O
of	O
RAG	NLP-algorithm/tool
for	O
the	O
task	O
of	O
domain	O
adaptation	O
in	O
ODQA	NLP-focus
.	O

We	O
propose	O
RAG	O
-	O
end2end	O
RAG	NLP-algorithm/tool
extension	O
to	O
RAG	O
that	O
can	O
adapt	O
to	O
a	O
domain	O
-	O
specific	O
knowledge	O
base	O
by	O
updating	O
all	O
components	O
of	O
the	O
external	O
knowledge	O
base	O
during	O
training	O
.	O

Our	O
novel	O
contribution	O
is	O
that	O
,	O
unlike	O
RAG	NLP-algorithm/tool
RAG	O
-	O
end2end	O
does	O
joint	O
training	O
of	O
the	O
retriever	O
and	O
generator	O
for	O
the	O
end	NLP-focus
QA	NLP-focus
task	NLP-focus
and	O
domain	NLP-focus
adaptation	NLP-focus
.	O

Our	O
novel	O
contribution	O
is	O
that	O
,	O
unlike	O
RAG	NLP-algorithm/tool
RAG	O
-	O
end2end	O
does	O
joint	O
training	O
of	O
the	O
retriever	O
and	O
generator	O
for	O
the	O
end	NLP-focus
QA	NLP-focus
task	NLP-focus
and	O
domain	NLP-focus
adaptation	NLP-focus
.	O

We	O
evaluate	O
our	O
approach	O
with	O
datasets	O
from	O
three	O
domains	O
:	O
COVID	O
-	O
19	O
News	O
and	O
Conversations	O
and	O
achieve	O
significant	O
performance	O
improvements	O
compared	O
to	O
the	O
original	O
RAG	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Many	O
studies	O
have	O
shown	O
that	O
transformers	O
are	O
able	O
to	O
predict	O
subject	NLP-focus
-	NLP-focus
verb	NLP-focus
agreement	NLP-focus
demonstrating	O
their	O
ability	O
to	O
uncover	O
an	O
abstract	NLP-focus
representation	NLP-focus
of	O
the	O
sentence	O
in	O
an	O
unsupervised	O
way	O
.	O

(	O
2021	O
)	O
found	O
that	O
transformers	O
were	O
also	O
able	O
to	O
predict	O
the	O
object	NLP-focus
-	NLP-focus
past	NLP-focus
participle	NLP-focus
agreement	NLP-focus
in	O
French	O
the	O
modeling	O
of	O
which	O
in	O
formal	O
grammar	O
is	O
fundamentally	O
different	O
from	O
that	O
of	O
subject	NLP-focus
-	NLP-focus
verb	NLP-focus
agreement	NLP-focus
and	O
relies	O
on	O
a	O
movement	O
and	O
an	O
anaphora	O
resolution	O
transformers	O
erstand	O
transformers	O
’	O
internal	O
working	O
,	O
we	O
propose	O
to	O
contrast	O
how	O
they	O
handle	O
these	O
two	O
kinds	O
of	O
agreement	O
.	O

Using	O
probing	O
and	O
counterfactual	O
analysis	O
methods	O
,	O
our	O
experiments	O
on	O
French	O
agreements	O
show	O
that	O
(	O
i	O
)	O
the	O
agreement	O
task	O
suffers	O
from	O
several	O
confounders	O
that	O
partially	O
question	O
the	O
conclusions	O
drawn	O
so	O
far	O
and	O
(	O
ii	O
)	O
transformers	O
handle	O
subject	NLP-focus
-	NLP-focus
verb	NLP-focus
and	O
object	NLP-focus
-	NLP-focus
past	NLP-focus
participle	NLP-focus
agreements	NLP-focus
in	O
a	O
way	O
that	O
is	O
consistent	O
with	O
their	O
modeling	O
in	O
theoretical	O
linguistics	O
.	O

Legal	O
outcome	O
prediction	O
the	O
prediction	O
of	O
positive	O
outcome	O
is	O
an	O
increasingly	O
popular	O
task	O
in	O
AI	AI/ML/DL-domain
.	O

In	O
contrast	O
,	O
we	O
turn	O
our	O
focus	O
to	O
negative	O
outcomes	O
here	O
,	O
and	O
introduce	O
a	O
new	O
task	O
of	O
negative	NLP-focus
outcome	NLP-focus
prediction	NLP-focus
.	O

Where	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
outcome	O
prediction	O
model	O
we	O
used	O
predicts	O
positive	O
outcomes	O
at	O
75	Numerical-result
.	Numerical-result
06	Numerical-result
F1	Classification-metrics
it	O
predicts	O
negative	O
outcomes	O
at	O
only	O
10	Numerical-result
.	Numerical-result
09	Numerical-result
F1	Classification-metrics
worse	O
than	O
a	O
random	O
baseline	O
.	O

Where	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
outcome	O
prediction	O
model	O
we	O
used	O
predicts	O
positive	O
outcomes	O
at	O
75	Numerical-result
.	Numerical-result
06	Numerical-result
F1	Classification-metrics
it	O
predicts	O
negative	O
outcomes	O
at	O
only	O
10	Numerical-result
.	Numerical-result
09	Numerical-result
F1	Classification-metrics
worse	O
than	O
a	O
random	O
baseline	O
.	O

Our	O
first	O
model	O
significantly	O
improves	O
positive	NLP-focus
outcome	NLP-focus
prediction	NLP-focus
score	O
to	O
77	Numerical-result
.	Numerical-result
15	Numerical-result
F1	Classification-metrics
and	O
our	O
second	O
model	O
more	O
than	O
doubles	O
the	O
negative	NLP-focus
outcome	NLP-focus
prediction	NLP-focus
performance	O
to	O
24	Numerical-result
.	Numerical-result
01	Numerical-result
F1	Classification-metrics
.	O

Our	O
first	O
model	O
significantly	O
improves	O
positive	NLP-focus
outcome	NLP-focus
prediction	NLP-focus
score	O
to	O
77	Numerical-result
.	Numerical-result
15	Numerical-result
F1	Classification-metrics
and	O
our	O
second	O
model	O
more	O
than	O
doubles	O
the	O
negative	NLP-focus
outcome	NLP-focus
prediction	NLP-focus
performance	O
to	O
24	Numerical-result
.	Numerical-result
01	Numerical-result
F1	Classification-metrics
.	O

Our	O
first	O
model	O
significantly	O
improves	O
positive	NLP-focus
outcome	NLP-focus
prediction	NLP-focus
score	O
to	O
77	Numerical-result
.	Numerical-result
15	Numerical-result
F1	Classification-metrics
and	O
our	O
second	O
model	O
more	O
than	O
doubles	O
the	O
negative	NLP-focus
outcome	NLP-focus
prediction	NLP-focus
performance	O
to	O
24	Numerical-result
.	Numerical-result
01	Numerical-result
F1	Classification-metrics
.	O

Despite	O
this	O
improvement	O
,	O
shifting	O
focus	O
to	O
negative	O
outcomes	O
reveals	O
that	O
there	O
is	O
still	O
much	O
room	O
for	O
improvement	O
for	O
outcome	NLP-focus
prediction	NLP-focus
models	NLP-focus
https	O
://	O
github	O
.	O
com	O
/	O
valvoda	O
/	O
Negative	O
-	O
Precedent	O
-	O
in	O
-	O
Legal	O
-	O
Outcome	O
-	O
Prediction	O
.	O

Localizing	O
a	O
semantic	NLP-algorithm/tool
parser	NLP-algorithm/tool
to	O
support	O
new	O
languages	O
requires	O
effective	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
generalization	NLP-focus
.	O

Localizing	O
a	O
semantic	NLP-algorithm/tool
parser	NLP-algorithm/tool
to	O
support	O
new	O
languages	O
requires	O
effective	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
generalization	NLP-focus
.	O

Recent	O
work	O
has	O
found	O
success	O
with	O
machine	NLP-focus
-	NLP-focus
translation	NLP-focus
or	O
zero	O
-	O
shot	O
methods	O
although	O
these	O
approaches	O
can	O
struggle	O
to	O
model	O
how	O
native	O
speakers	O
ask	O
questions	O
.	O

We	O
consider	O
how	O
to	O
effectively	O
leverage	O
minimal	O
annotated	O
examples	O
in	O
new	O
languages	O
for	O
few	NLP-focus
-	NLP-focus
shot	NLP-focus
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
semantic	NLP-focus
parsing	NLP-focus
.	O

We	O
introduce	O
a	O
first	O
-	O
order	O
meta	O
-	O
learning	O
algorithm	O
to	O
train	O
a	O
semantic	NLP-algorithm/tool
parser	NLP-algorithm/tool
with	O
maximal	O
sample	O
efficiency	O
during	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
transfer	NLP-focus
.	O

We	O
introduce	O
a	O
first	O
-	O
order	O
meta	O
-	O
learning	O
algorithm	O
to	O
train	O
a	O
semantic	NLP-algorithm/tool
parser	NLP-algorithm/tool
with	O
maximal	O
sample	O
efficiency	O
during	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
transfer	NLP-focus
.	O

Our	O
algorithm	O
uses	O
high	O
-	O
resource	O
languages	O
to	O
train	O
the	O
parser	NLP-algorithm/tool
and	O
simultaneously	O
optimizes	O
for	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
generalization	NLP-focus
to	O
lower	O
-	O
resource	O
languages	O
.	O

Our	O
algorithm	O
uses	O
high	O
-	O
resource	O
languages	O
to	O
train	O
the	O
parser	NLP-algorithm/tool
and	O
simultaneously	O
optimizes	O
for	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
generalization	NLP-focus
to	O
lower	O
-	O
resource	O
languages	O
.	O

This	O
paper	O
presents	O
an	O
ontology	O
-	O
aware	O
pretrained	O
language	O
model	O
(	O
OPAL	O
)	O
for	O
end	O
-	O
to	O
-	O
end	O
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialogue	NLP-focus
(	NLP-focus
TOD	NLP-focus
)	NLP-focus
.	O

Unlike	O
chit	NLP-algorithm/tool
-	NLP-algorithm/tool
chat	NLP-algorithm/tool
dialogue	NLP-algorithm/tool
models	NLP-algorithm/tool
task	NLP-algorithm/tool
-	NLP-algorithm/tool
oriented	NLP-algorithm/tool
dialogue	NLP-algorithm/tool
models	NLP-algorithm/tool
fulfill	O
at	O
least	O
two	O
task	O
-	O
specific	O
modules	O
:	O
Dialogue	O
state	O
tracker	O
(	O
DST	O
)	O
and	O
response	O
generator	O
(	O
RG	O
)	O

The	O
dialogue	O
state	O
consists	O
of	O
the	O
domain	NLP-algorithm/tool
-	NLP-algorithm/tool
slot	NLP-algorithm/tool
-	NLP-algorithm/tool
value	NLP-algorithm/tool
triples	NLP-algorithm/tool
which	O
are	O
regarded	O
as	O
the	O
user	O
’	O
s	O
constraints	O
to	O
search	O
the	O
domain	O
-	O
related	O
databases	O
.	O

The	O
large	O
-	O
scale	O
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialogue	NLP-focus
data	NLP-focus
with	O
the	O
annotated	O
structured	O
dialogue	O
state	O
usually	O
are	O
inaccessible	O
.	O

It	O
prevents	O
the	O
development	O
of	O
the	O
pretrained	O
language	O
model	O
for	O
the	O
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialogue	NLP-focus
.	O

To	O
bridge	O
the	O
gap	O
between	O
the	O
pretraining	O
method	O
and	O
downstream	O
tasks	O
,	O
we	O
design	O
two	O
pretraining	O
tasks	O
:	O
ontology	NLP-focus
-	NLP-focus
like	NLP-focus
triple	NLP-focus
recovery	NLP-focus
and	O
next	NLP-focus
-	NLP-focus
text	NLP-focus
generation	NLP-focus
which	O
simulates	O
the	O
DST	O
and	O
RG	O
respectively	O
.	O

The	O
second	O
phase	O
is	O
to	O
fine	O
-	O
tune	O
the	O
pretrained	O
model	O
on	O
the	O
TOD	NLP-focus
data	O
.	O

Applied	O
to	O
Japanese	O
place	O
names	O
we	O
demonstrate	O
the	O
utility	O
of	O
the	O
model	O
to	O
finding	O
and	O
proposing	O
corrections	O
for	O
errors	O
in	O
Google	O
Maps	O
.	O
To	O
demonstrate	O
the	O
utility	O
of	O
this	O
approach	O
to	O
structurally	O
similar	O
problems	O
,	O
we	O
also	O
report	O
on	O
an	O
application	O
to	O
a	O
totally	O
different	O
task	O
:	O
Cognate	NLP-focus
reflex	NLP-focus
prediction	NLP-focus
in	O
comparative	O
historical	O
linguistics	O
.	O

Today	O
’	O
s	O
probabilistic	NLP-algorithm/tool
language	NLP-algorithm/tool
generators	NLP-algorithm/tool
fall	O
short	O
when	O
it	O
comes	O
to	O
producing	O
coherent	O
and	O
fluent	O
text	O
despite	O
the	O
fact	O
that	O
the	O
underlying	O
models	O
perform	O
well	O
under	O
standard	O
metrics	O
(	O
e	O
.	O
g	O
.,	O
perplexity	NLP-metrics
.	O

Today	O
’	O
s	O
probabilistic	NLP-algorithm/tool
language	NLP-algorithm/tool
generators	NLP-algorithm/tool
fall	O
short	O
when	O
it	O
comes	O
to	O
producing	O
coherent	O
and	O
fluent	O
text	O
despite	O
the	O
fact	O
that	O
the	O
underlying	O
models	O
perform	O
well	O
under	O
standard	O
metrics	O
(	O
e	O
.	O
g	O
.,	O
perplexity	NLP-metrics
.	O

This	O
discrepancy	O
has	O
puzzled	O
the	O
language	NLP-focus
generation	NLP-focus
community	O
for	O
the	O
last	O
few	O
years	O
.	O

In	O
this	O
work	O
,	O
we	O
posit	O
that	O
the	O
abstraction	O
of	O
natural	NLP-focus
language	NLP-focus
generation	NLP-focus
as	O
a	O
discrete	O
stochastic	O
process	O
which	O
allows	O
for	O
an	O
information	O
-	O
theoretic	O
analysis	O
can	O
provide	O
new	O
insights	O
into	O
the	O
behavior	O
of	O
probabilistic	NLP-algorithm/tool
language	NLP-algorithm/tool
generators	NLP-algorithm/tool
for	O
example	O
,	O
why	O
high	O
-	O
probability	O
texts	O
can	O
be	O
dull	O
or	O
repetitive	O
.	O

In	O
this	O
work	O
,	O
we	O
posit	O
that	O
the	O
abstraction	O
of	O
natural	NLP-focus
language	NLP-focus
generation	NLP-focus
as	O
a	O
discrete	O
stochastic	O
process	O
which	O
allows	O
for	O
an	O
information	O
-	O
theoretic	O
analysis	O
can	O
provide	O
new	O
insights	O
into	O
the	O
behavior	O
of	O
probabilistic	NLP-algorithm/tool
language	NLP-algorithm/tool
generators	NLP-algorithm/tool
for	O
example	O
,	O
why	O
high	O
-	O
probability	O
texts	O
can	O
be	O
dull	O
or	O
repetitive	O
.	O

We	O
formally	O
define	O
the	O
set	O
of	O
strings	O
that	O
meet	O
this	O
criterion	O
:	O
Those	O
for	O
which	O
each	O
word	O
has	O
an	O
information	O
content	O
close	O
to	O
the	O
expected	O
information	O
content	O
namely	O
,	O
the	O
conditional	Statistical/Mathematical-metrics
entropy	Statistical/Mathematical-metrics
of	O
our	O
model	O
.	O

Automatic	O
and	O
human	O
evaluations	O
show	O
that	O
,	O
in	O
comparison	O
to	O
nucleus	O
and	O
top	O
-	O
k	O
sampling	O
,	O
locally	O
typical	O
sampling	O
offers	O
competitive	O
performance	O
(	O
in	O
both	O
abstractive	NLP-focus
summarization	NLP-focus
and	O
story	NLP-focus
generation	NLP-focus
in	O
terms	O
of	O
quality	O
while	O
consistently	O
reducing	O
degenerate	O
repetitions	O
.	O

Multilingual	NLP-focus
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialogue	NLP-focus
(	NLP-focus
ToD	NLP-focus
)	NLP-focus
facilitates	O
access	O
to	O
services	O
and	O
information	O
for	O
many	O
(	O
communities	O
of	O
)	O
speakers	O
.	O

In	O
this	O
work	O
,	O
to	O
tackle	O
these	O
limitations	O
we	O
propose	O
a	O
novel	O
outline	O
-	O
based	O
annotation	O
process	O
for	O
multilingual	NLP-focus
ToD	NLP-focus
datasets	O
,	O
where	O
domain	O
-	O
specific	O
abstract	O
schemata	O
of	O
dialogue	O
are	O
mapped	O
into	O
natural	O
language	O
outlines	O
.	O

Through	O
this	O
process	O
we	O
annotate	O
a	O
new	O
large	O
-	O
scale	O
dataset	O
for	O
evaluation	O
of	O
multilingual	O
and	O
cross	O
-	O
lingual	O
ToD	NLP-algorithm/tool
systems	NLP-algorithm/tool
.	O

Finally	O
,	O
we	O
benchmark	O
a	O
series	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
for	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
ToD	NLP-focus
setting	O
reference	O
scores	O
for	O
future	O
work	O
and	O
demonstrating	O
that	O
cod	O
prevents	O
over	O
-	O
inflated	O
performance	O
,	O
typically	O
met	O
with	O
prior	O
translation	O
-	O
based	O
ToD	O
datasets	O
.	O

Most	O
previous	O
work	O
in	O
music	NLP-focus
emotion	NLP-focus
recognition	NLP-focus
assumes	O
a	O
single	O
or	O
a	O
few	O
song	O
-	O
level	O
labels	O
for	O
the	O
whole	O
song	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
method	O
to	O
predict	NLP-focus
emotion	NLP-focus
dynamics	NLP-focus
in	O
song	O
lyrics	O
without	O
song	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
supervision	NLP-algorithm/tool
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
method	O
to	O
predict	NLP-focus
emotion	NLP-focus
dynamics	NLP-focus
in	O
song	O
lyrics	O
without	O
song	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
supervision	NLP-algorithm/tool
.	O

We	O
frame	O
each	O
song	O
as	O
a	O
time	O
series	O
and	O
employ	O
a	O
State	O
Space	O
Model	O
(	O
SSM	O
),	O
combining	O
a	O
sentence	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
emotion	NLP-algorithm/tool
predictor	NLP-algorithm/tool
with	O
an	O
Expectation	O
-	O
Maximization	O
(	O
EM	O
)	O
procedure	O
to	O
generate	O
the	O
full	O
emotion	O
dynamics	O
.	O

Drawing	O
on	O
studies	O
of	O
word	NLP-focus
acquisition	NLP-focus
in	O
children	O
,	O
we	O
evaluate	O
multiple	O
predictors	O
for	O
words	O
’	O
ages	O
of	O
acquisition	O
in	O
LSTMs	O
BERT	NLP-algorithm/tool
and	O
GPT	NLP-algorithm/tool
-	NLP-algorithm/tool
2	NLP-algorithm/tool
.	O

Drawing	O
on	O
studies	O
of	O
word	NLP-focus
acquisition	NLP-focus
in	O
children	O
,	O
we	O
evaluate	O
multiple	O
predictors	O
for	O
words	O
’	O
ages	O
of	O
acquisition	O
in	O
LSTMs	O
BERT	NLP-algorithm/tool
and	O
GPT	NLP-algorithm/tool
-	NLP-algorithm/tool
2	NLP-algorithm/tool
.	O

We	O
find	O
that	O
the	O
effects	O
of	O
concreteness	O
,	O
word	O
length	O
and	O
lexical	O
class	O
are	O
pointedly	O
different	O
in	O
children	O
and	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
reinforcing	O
the	O
importance	O
of	O
interaction	O
and	O
sensorimotor	O
experience	O
in	O
child	NLP-focus
language	NLP-focus
acquisition	NLP-focus
.	O

We	O
find	O
that	O
the	O
effects	O
of	O
concreteness	O
,	O
word	O
length	O
and	O
lexical	O
class	O
are	O
pointedly	O
different	O
in	O
children	O
and	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
reinforcing	O
the	O
importance	O
of	O
interaction	O
and	O
sensorimotor	O
experience	O
in	O
child	NLP-focus
language	NLP-focus
acquisition	NLP-focus
.	O

Language	NLP-algorithm/tool
models	NLP-algorithm/tool
rely	O
far	O
more	O
on	O
word	O
frequency	O
than	O
children	O
,	O
but	O
,	O
like	O
children	O
,	O
they	O
exhibit	O
slower	O
learning	O
of	O
words	O
in	O
longer	O
utterances	O
.	O

We	O
present	O
an	O
event	NLP-focus
structure	NLP-focus
classification	NLP-focus
empirically	O
derived	O
from	O
inferential	O
properties	O
annotated	O
on	O
sentence	O
-	O
and	O
document	O
-	O
level	O
Universal	NLP-algorithm/tool
Decompositional	NLP-algorithm/tool
Semantics	NLP-algorithm/tool
(	NLP-algorithm/tool
UDS	NLP-algorithm/tool
)	NLP-algorithm/tool
graphs	NLP-algorithm/tool
.	O

We	O
present	O
an	O
event	NLP-focus
structure	NLP-focus
classification	NLP-focus
empirically	O
derived	O
from	O
inferential	O
properties	O
annotated	O
on	O
sentence	O
-	O
and	O
document	O
-	O
level	O
Universal	NLP-algorithm/tool
Decompositional	NLP-algorithm/tool
Semantics	NLP-algorithm/tool
(	NLP-algorithm/tool
UDS	NLP-algorithm/tool
)	NLP-algorithm/tool
graphs	NLP-algorithm/tool
.	O

Existing	O
table	O
question	NLP-focus
answering	NLP-focus
datasets	O
contain	O
abundant	O
factual	O
questions	O
that	O
primarily	O
evaluate	O
a	O
QA	NLP-focus
system	NLP-focus
s	O
comprehension	O
of	O
query	O
and	O
tabular	O
data	O
.	O

To	O
complement	O
the	O
existing	O
datasets	O
and	O
to	O
reveal	O
the	O
challenging	O
nature	O
of	O
the	O
table	NLP-focus
-	NLP-focus
based	NLP-focus
question	NLP-focus
answering	NLP-focus
task	NLP-focus
we	O
introduce	O
FeTaQA	O
dataset	O
ataset	O
with	O
10K	O
Wikipedia	O
-	O
based	O
table	O
question	O
ion	O
,	O
free	O
-	O
form	O
answer	O
supporting	O
table	O
cells	O
\}	O
pairs	O
.	O

FeTaQA	O
is	O
collected	O
from	O
noteworthy	O
descriptions	O
of	O
Wikipedia	O
tables	O
that	O
contain	O
information	O
people	O
tend	O
to	O
seek	O
;	O
generation	O
of	O
these	O
descriptions	O
requires	O
advanced	O
processing	O
that	O
humans	O
perform	O
on	O
a	O
daily	O
basis	O
:	O
Understand	O
the	O
question	O
and	O
table	O
,	O
retrieve	O
,	O
integrate	O
,	O
infer	O
,	O
and	O
conduct	O
text	NLP-focus
planning	NLP-focus
and	O
surface	O
realization	O
to	O
generate	O
an	O
answer	O
.	O

We	O
provide	O
two	O
benchmark	O
methods	O
for	O
the	O
proposed	O
task	O
:	O
a	O
pipeline	O
method	O
based	O
on	O
semantic	NLP-algorithm/tool
parsing	NLP-algorithm/tool
based	O
QA	NLP-focus
systems	NLP-focus
and	O
an	O
end	O
-	O
to	O
-	O
end	O
method	O
based	O
on	O
large	NLP-algorithm/tool
pretrained	NLP-algorithm/tool
text	NLP-algorithm/tool
generation	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
show	O
that	O
FeTaQA	O
poses	O
a	O
challenge	O
for	O
both	O
methods	O
.	O

We	O
provide	O
two	O
benchmark	O
methods	O
for	O
the	O
proposed	O
task	O
:	O
a	O
pipeline	O
method	O
based	O
on	O
semantic	NLP-algorithm/tool
parsing	NLP-algorithm/tool
based	O
QA	NLP-focus
systems	NLP-focus
and	O
an	O
end	O
-	O
to	O
-	O
end	O
method	O
based	O
on	O
large	NLP-algorithm/tool
pretrained	NLP-algorithm/tool
text	NLP-algorithm/tool
generation	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
show	O
that	O
FeTaQA	O
poses	O
a	O
challenge	O
for	O
both	O
methods	O
.	O

With	O
the	O
success	O
of	O
large	O
-	O
scale	O
pre	O
-	O
training	O
and	O
multilingual	NLP-algorithm/tool
modeling	NLP-algorithm/tool
in	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
recent	O
years	O
have	O
seen	O
a	O
proliferation	O
of	O
large	O
,	O
Web	O
-	O
mined	O
text	O
datasets	O
covering	O
hundreds	O
of	O
languages	O
.	O

Pipelined	NLP-algorithm/tool
NLP	NLP-algorithm/tool
systems	NLP-algorithm/tool
have	O
largely	O
been	O
superseded	O
by	O
end	O
-	O
to	O
-	O
end	O
neural	O
modeling	O
yet	O
nearly	O
all	O
commonly	O
used	O
models	O
still	O
require	O
an	O
explicit	O
tokenization	NLP-algorithm/tool
step	O
.	O

While	O
recent	O
tokenization	NLP-algorithm/tool
approaches	O
based	O
on	O
data	O
-	O
derived	O
subword	O
lexicons	O
are	O
less	O
brittle	O
than	O
manually	NLP-algorithm/tool
engineered	NLP-algorithm/tool
tokenizers	NLP-algorithm/tool
these	O
techniques	O
are	O
not	O
equally	O
suited	O
to	O
all	O
languages	O
,	O
and	O
the	O
use	O
of	O
any	O
fixed	O
vocabulary	O
may	O
limit	O
a	O
model	O
’	O
s	O
ability	O
to	O
adapt	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
Canine	O
a	O
neural	O
encoder	O
that	O
operates	O
directly	O
on	O
character	O
sequences	O
—	O
without	O
explicit	O
tokenization	NLP-algorithm/tool
or	O
vocabulary	O
—	O
and	O
a	O
pre	O
-	O
training	O
strategy	O
that	O
operates	O
either	O
directly	O
on	O
characters	O
or	O
optionally	O
uses	O
subwords	O
as	O
a	O
soft	O
inductive	O
bias	O
.	O

Canine	O
outperforms	O
a	O
comparable	O
mBert	NLP-algorithm/tool
model	NLP-algorithm/tool
by	O
5	O
.	O
7	O
F1	Classification-metrics
on	O
TyDi	O
QA	O
a	O
challenging	O
multilingual	O
benchmark	O
despite	O
having	O
fewer	O
model	O
parameters	O
.	O

Canine	O
outperforms	O
a	O
comparable	O
mBert	NLP-algorithm/tool
model	NLP-algorithm/tool
by	O
5	O
.	O
7	O
F1	Classification-metrics
on	O
TyDi	O
QA	O
a	O
challenging	O
multilingual	O
benchmark	O
despite	O
having	O
fewer	O
model	O
parameters	O
.	O

However	O
,	O
annotators	O
may	O
systematically	O
disagree	O
with	O
one	O
another	O
,	O
often	O
reflecting	O
their	O
individual	O
biases	O
and	O
values	O
,	O
especially	O
in	O
the	O
case	O
of	O
subjective	O
tasks	O
such	O
as	O
detecting	NLP-focus
affect	O
aggression	O
and	O
hate	O
speech	O
.	O

In	O
order	O
to	O
address	O
this	O
,	O
we	O
investigate	O
the	O
efficacy	O
of	O
multi	NLP-algorithm/tool
-	NLP-algorithm/tool
annotator	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

Our	O
approach	O
also	O
provides	O
a	O
way	O
to	O
estimate	NLP-focus
uncertainty	NLP-focus
in	O
predictions	O
,	O
which	O
we	O
demonstrate	O
better	O
correlate	O
with	O
annotation	O
disagreements	O
than	O
traditional	O
methods	O
.	O

Recent	O
efforts	O
to	O
create	O
challenge	O
benchmarks	O
that	O
test	O
the	O
abilities	O
of	O
natural	NLP-algorithm/tool
language	NLP-algorithm/tool
understanding	NLP-algorithm/tool
models	NLP-algorithm/tool
have	O
largely	O
depended	O
on	O
human	O
annotations	O
.	O

In	O
this	O
work	O
,	O
we	O
introduce	O
the	O
“	O
Break	O
,	O
Perturb	O
,	O
Build	O
”	O
(	O
BPB	O
)	O
framework	O
for	O
automatic	NLP-focus
reasoning	NLP-focus
-	NLP-focus
oriented	NLP-focus
perturbation	NLP-focus
of	O
question	O
-	O
answer	O
pairs	O
.	O

BPB	O
represents	O
a	O
question	O
by	O
decomposing	O
it	O
into	O
the	O
reasoning	O
steps	O
that	O
are	O
required	O
to	O
answer	O
it	O
,	O
symbolically	O
perturbs	O
the	O
decomposition	NLP-algorithm/tool
and	O
then	O
generates	O
new	O
question	O
-	O
answer	O
pairs	O
.	O

We	O
demonstrate	O
the	O
effectiveness	O
of	O
BPB	O
by	O
creating	O
evaluation	O
sets	O
for	O
three	O
reading	NLP-focus
comprehension	NLP-focus
(	NLP-focus
RC	NLP-focus
)	NLP-focus
benchmarks	O
,	O
generating	O
thousands	O
of	O
high	O
-	O
quality	O
examples	O
without	O
human	O
intervention	O
.	O

We	O
evaluate	O
a	O
range	O
of	O
RC	NLP-algorithm/tool
models	NLP-algorithm/tool
on	O
our	O
evaluation	O
sets	O
,	O
which	O
reveals	O
large	O
performance	O
gaps	O
on	O
generated	O
examples	O
compared	O
to	O
the	O
original	O
data	O
.	O

Discourse	NLP-focus
parsing	NLP-focus
has	O
been	O
studied	O
for	O
decades	O
.	O

In	O
this	O
paper	O
,	O
we	O
report	O
and	O
discuss	O
the	O
effectiveness	O
and	O
limitations	O
of	O
bootstrapping	O
methods	O
for	O
adapting	O
modern	O
BERT	NLP-algorithm/tool
based	O
discourse	NLP-algorithm/tool
dependency	NLP-algorithm/tool
parsers	NLP-algorithm/tool
to	O
out	O
-	O
of	O
-	O
domain	O
text	O
without	O
relying	O
on	O
additional	O
human	O
supervision	O
.	O

Specifically	O
,	O
we	O
investigate	O
self	O
-	O
training	O
co	O
-	O
training	O
tri	O
-	O
training	O
and	O
asymmetric	O
tri	O
-	O
training	O
of	O
graph	O
-	O
based	O
and	O
transition	O
-	O
based	O
discourse	O
dependency	O
parsing	O
models	O
,	O
as	O
well	O
as	O
confidence	O
measures	O
and	O
sample	O
selection	O
criteria	O
in	O
two	O
adaptation	O
scenarios	O
:	O
monologue	NLP-focus
adaptation	NLP-focus
between	O
scientific	O
disciplines	O
and	O
dialogue	NLP-focus
genre	NLP-focus
adaptation	NLP-focus
.	O

We	O
also	O
release	O
COVID	O
-	O
19	O
Discourse	O
Dependency	O
Treebank	O
(	O
COVID19	O
-	O
DTB	O
)	O
a	O
new	O
manually	O
annotated	O
resource	O
for	O
discourse	NLP-focus
dependency	NLP-focus
parsing	NLP-focus
of	O
biomedical	O
paper	O
abstracts	O
.	O

The	O
experimental	O
results	O
show	O
that	O
bootstrapping	O
is	O
significantly	O
and	O
consistently	O
effective	O
for	O
unsupervised	O
domain	O
adaptation	O
of	O
discourse	NLP-focus
dependency	NLP-focus
parsing	NLP-focus
but	O
the	O
low	O
coverage	O
of	O
accurately	O
predicted	O
pseudo	O
labels	O
is	O
a	O
bottleneck	O
for	O
further	O
improvement	O
.	O

We	O
mine	O
the	O
parallel	O
sentences	O
from	O
the	O
Web	O
by	O
combining	O
many	O
corpora	O
,	O
tools	O
,	O
and	O
methods	O
:	O
(	O
a	O
)	O
Web	O
-	O
crawled	O
monolingual	O
corpora	O
(	O
b	O
)	O
document	O
OCR	O
for	O
extracting	O
sentences	O
from	O
scanned	O
documents	O
,	O
(	O
c	O
)	O
multilingual	NLP-algorithm/tool
representation	NLP-algorithm/tool
models	NLP-algorithm/tool
for	O
aligning	NLP-focus
sentences	NLP-focus
and	O
(	O
d	O
)	O
approximate	O
nearest	O
neighbor	O
search	O
for	O
searching	O
in	O
a	O
large	O
collection	O
of	O
sentences	O
.	O

We	O
mine	O
the	O
parallel	O
sentences	O
from	O
the	O
Web	O
by	O
combining	O
many	O
corpora	O
,	O
tools	O
,	O
and	O
methods	O
:	O
(	O
a	O
)	O
Web	O
-	O
crawled	O
monolingual	O
corpora	O
(	O
b	O
)	O
document	O
OCR	O
for	O
extracting	O
sentences	O
from	O
scanned	O
documents	O
,	O
(	O
c	O
)	O
multilingual	NLP-algorithm/tool
representation	NLP-algorithm/tool
models	NLP-algorithm/tool
for	O
aligning	NLP-focus
sentences	NLP-focus
and	O
(	O
d	O
)	O
approximate	O
nearest	O
neighbor	O
search	O
for	O
searching	O
in	O
a	O
large	O
collection	O
of	O
sentences	O
.	O

We	O
trained	O
multilingual	NLP-algorithm/tool
NMT	NLP-algorithm/tool
models	NLP-algorithm/tool
spanning	O
all	O
these	O
languages	O
on	O
Samanantar	O
which	O
outperform	O
existing	O
models	O
and	O
baselines	O
on	O
publicly	O
available	O
benchmarks	O
,	O
such	O
as	O
FLORES	O
Samanantar	O
ng	O
the	O
utility	O
of	O
Samanantar	O
.	O

In	O
the	O
summarization	NLP-focus
domain	O
,	O
a	O
key	O
requirement	O
for	O
summaries	O
is	O
to	O
be	O
factually	O
consistent	O
with	O
the	O
input	O
document	O
.	O

Previous	O
work	O
has	O
found	O
that	O
natural	NLP-algorithm/tool
language	NLP-algorithm/tool
inference	NLP-algorithm/tool
(	NLP-algorithm/tool
NLI	NLP-algorithm/tool
)	NLP-algorithm/tool
models	NLP-algorithm/tool
do	O
not	O
perform	O
competitively	O
when	O
applied	O
to	O
inconsistency	O
detection	O
.	O

In	O
this	O
work	O
,	O
we	O
revisit	O
the	O
use	O
of	O
NLI	O
for	O
inconsistency	O
detection	O
,	O
finding	O
that	O
past	O
work	O
suffered	O
from	O
a	O
mismatch	O
in	O
input	NLP-focus
granularity	NLP-focus
between	O
NLI	O
datasets	O
(	O
sentence	O
-	O
level	O
inconsistency	NLP-focus
detection	NLP-focus
ction	O
(	O
document	O
level	O
.	O

We	O
provide	O
a	O
highly	O
effective	O
and	O
light	O
-	O
weight	O
method	O
called	O
SummaCConv	O
that	O
enables	O
NLI	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
be	O
successfully	O
used	O
for	O
this	O
task	O
by	O
segmenting	NLP-focus
documents	NLP-focus
into	O
sentence	O
units	O
and	O
aggregating	O
scores	O
between	O
pairs	O
of	O
sentences	O
.	O

We	O
provide	O
a	O
highly	O
effective	O
and	O
light	O
-	O
weight	O
method	O
called	O
SummaCConv	O
that	O
enables	O
NLI	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
be	O
successfully	O
used	O
for	O
this	O
task	O
by	O
segmenting	NLP-focus
documents	NLP-focus
into	O
sentence	O
units	O
and	O
aggregating	O
scores	O
between	O
pairs	O
of	O
sentences	O
.	O

On	O
this	O
dataset	O
,	O
SummaCConv	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
with	O
a	O
balanced	O
accuracy	Classification-metrics
of	O
74	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
a	O
5	O
\\%	O
improvement	O
compared	O
with	O
prior	O
work	O
.	O

On	O
this	O
dataset	O
,	O
SummaCConv	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
with	O
a	O
balanced	O
accuracy	Classification-metrics
of	O
74	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
a	O
5	O
\\%	O
improvement	O
compared	O
with	O
prior	O
work	O
.	O

Fact	NLP-focus
-	NLP-focus
checking	NLP-focus
has	O
become	O
increasingly	O
important	O
due	O
to	O
the	O
speed	O
with	O
which	O
both	O
information	O
and	O
misinformation	O
can	O
spread	O
in	O
the	O
modern	O
media	O
ecosystem	O
.	O

Therefore	O
,	O
researchers	O
have	O
been	O
exploring	O
how	O
fact	O
-	O
checking	O
can	O
be	O
automated	O
,	O
using	O
techniques	O
based	O
on	O
natural	O
language	O
processing	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
knowledge	NLP-focus
representation	NLP-focus
and	O
databases	O
to	O
automatically	O
predict	O
the	O
veracity	O
of	O
claims	O
.	O

Therefore	O
,	O
researchers	O
have	O
been	O
exploring	O
how	O
fact	O
-	O
checking	O
can	O
be	O
automated	O
,	O
using	O
techniques	O
based	O
on	O
natural	O
language	O
processing	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
knowledge	NLP-focus
representation	NLP-focus
and	O
databases	O
to	O
automatically	O
predict	O
the	O
veracity	O
of	O
claims	O
.	O

In	O
this	O
paper	O
,	O
we	O
survey	O
automated	NLP-focus
fact	NLP-focus
-	NLP-focus
checking	NLP-focus
stemming	NLP-focus
from	O
natural	O
language	O
processing	O
and	O
discuss	O
its	O
connections	O
to	O
related	O
tasks	O
and	O
disciplines	O
.	O

This	O
paper	O
presents	O
a	O
new	O
task	O
of	O
predicting	O
the	O
coverage	O
of	O
a	O
text	O
document	O
for	O
relation	NLP-focus
extraction	NLP-focus
(	NLP-focus
RE	NLP-focus
)	NLP-focus
Does	O
the	O
document	O
contain	O
many	O
relational	O
tuples	O
for	O
a	O
given	O
entity	O
?	O
Coverage	O
predictions	O
are	O
useful	O
in	O
selecting	O
the	O
best	O
documents	O
for	O
knowledge	NLP-focus
base	NLP-focus
construction	NLP-focus
with	O
large	O
input	O
corpora	O
.	O

We	O
employ	O
methods	O
combining	O
features	O
with	O
statistical	O
models	O
like	O
TF	O
-	O
IDF	O
and	O
language	O
models	O
like	O
BERT	NLP-algorithm/tool
.	O

The	O
model	O
combining	O
features	O
and	O
BERT	NLP-algorithm/tool
HERB	NLP-algorithm/tool
achieves	O
an	O
F1	Classification-metrics
score	Classification-metrics
of	O
up	O
to	O
46	Numerical-result
\\%	Numerical-result
.	O

The	O
model	O
combining	O
features	O
and	O
BERT	NLP-algorithm/tool
HERB	NLP-algorithm/tool
achieves	O
an	O
F1	Classification-metrics
score	Classification-metrics
of	O
up	O
to	O
46	Numerical-result
\\%	Numerical-result
.	O

The	O
model	O
combining	O
features	O
and	O
BERT	NLP-algorithm/tool
HERB	NLP-algorithm/tool
achieves	O
an	O
F1	Classification-metrics
score	Classification-metrics
of	O
up	O
to	O
46	Numerical-result
\\%	Numerical-result
.	O

We	O
demonstrate	O
the	O
utility	O
of	O
coverage	O
predictions	O
on	O
two	O
use	O
cases	O
:	O
KB	NLP-focus
construction	NLP-focus
and	O
claim	NLP-focus
refutation	NLP-focus
.	O

Pretrained	O
contextualized	O
language	O
models	O
such	O
as	O
BERT	NLP-algorithm/tool
and	O
T5	NLP-algorithm/tool
have	O
established	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
ad	O
-	O
hoc	O
search	O
.	O

We	O
present	O
a	O
new	O
comprehensive	O
framework	O
for	O
Analyzing	O
the	O
Behavior	O
of	O
Neural	O
IR	O
ModeLs	O
(	O
ABNIRML	O
)	O
which	O
includes	O
new	O
types	O
of	O
diagnostic	O
probes	O
that	O
allow	O
us	O
to	O
test	O
several	O
characteristics	O
—	O
such	O
as	O
writing	NLP-focus
styles	NLP-focus
factuality	NLP-focus
sensitivity	NLP-focus
to	NLP-focus
paraphrasing	NLP-focus
and	O
word	NLP-focus
order	NLP-focus
that	O
are	O
not	O
addressed	O
by	O
previous	O
techniques	O
.	O

Other	O
results	O
are	O
more	O
surprising	O
,	O
such	O
as	O
that	O
some	O
models	O
(	O
e	O
.	O
g	O
.,	O
T5	NLP-algorithm/tool
and	O
ColBERT	NLP-algorithm/tool
are	O
biased	O
towards	O
factually	O
correct	O
(	O
rather	O
than	O
simply	O
relevant	O
)	O
texts	O
.	O

Further	O
,	O
some	O
characteristics	O
vary	O
even	O
for	O
the	O
same	O
base	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
other	O
characteristics	O
can	O
appear	O
due	O
to	O
random	O
variations	O
during	O
model	O
training	O
1	O
.	O

We	O
introduce	O
a	O
neuro	O
-	O
symbolic	O
natural	O
logic	O
framework	O
based	O
on	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
with	O
introspective	O
revision	O
.	O

However	O
,	O
most	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
are	O
trained	O
on	O
snapshots	O
of	O
data	O
collected	O
at	O
a	O
specific	O
moment	O
in	O
time	O
.	O

We	O
introduce	O
a	O
diagnostic	O
dataset	O
aimed	O
at	O
probing	NLP-focus
LMs	NLP-focus
for	O
factual	O
knowledge	O
LMs	NLP-algorithm/tool
changes	O
over	O
time	O
and	O
highlight	O
problems	O
with	O
LMs	O
at	O
either	O
end	O
of	O
the	O
spectrum	O
—	O
those	O
trained	O
on	O
specific	O
slices	O
of	O
temporal	O
data	O
temporal	O
data	O
ose	O
trained	O
on	O
a	O
wide	O
range	O
of	O
temporal	O
data	O
.	O

We	O
introduce	O
a	O
diagnostic	O
dataset	O
aimed	O
at	O
probing	NLP-focus
LMs	NLP-focus
for	O
factual	O
knowledge	O
LMs	NLP-algorithm/tool
changes	O
over	O
time	O
and	O
highlight	O
problems	O
with	O
LMs	O
at	O
either	O
end	O
of	O
the	O
spectrum	O
—	O
those	O
trained	O
on	O
specific	O
slices	O
of	O
temporal	O
data	O
temporal	O
data	O
ose	O
trained	O
on	O
a	O
wide	O
range	O
of	O
temporal	O
data	O
.	O

We	O
present	O
mGENRE	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
system	O
for	O
the	O
Multilingual	NLP-focus
Entity	NLP-focus
Linking	NLP-focus
(	NLP-focus
MEL	NLP-focus
)	NLP-focus
problem	O
—	O
the	O
task	O
of	O
resolving	O
language	O
-	O
specific	O
mentions	O
to	O
a	O
multilingual	NLP-algorithm/tool
Knowledge	NLP-algorithm/tool
Base	NLP-algorithm/tool
(	NLP-algorithm/tool
KB	NLP-algorithm/tool
)	NLP-algorithm/tool
.	O

We	O
present	O
mGENRE	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
system	O
for	O
the	O
Multilingual	NLP-focus
Entity	NLP-focus
Linking	NLP-focus
(	NLP-focus
MEL	NLP-focus
)	NLP-focus
problem	O
—	O
the	O
task	O
of	O
resolving	O
language	O
-	O
specific	O
mentions	O
to	O
a	O
multilingual	NLP-algorithm/tool
Knowledge	NLP-algorithm/tool
Base	NLP-algorithm/tool
(	NLP-algorithm/tool
KB	NLP-algorithm/tool
)	NLP-algorithm/tool
.	O

While	O
prior	O
MEL	NLP-focus
works	O
use	O
a	O
single	O
representation	O
for	O
each	O
entity	O
we	O
match	O
against	O
entity	O
names	O
of	O
as	O
many	O
languages	O
as	O
possible	O
,	O
which	O
allows	O
exploiting	O
language	O
connections	O
between	O
source	O
input	O
and	O
target	O
name	O
.	O

This	O
leads	O
to	O
over	O
50	Numerical-result
\\%	Numerical-result
improvements	O
in	O
average	Classification-metrics
accuracy	Classification-metrics
.	O

This	O
leads	O
to	O
over	O
50	Numerical-result
\\%	Numerical-result
improvements	O
in	O
average	Classification-metrics
accuracy	Classification-metrics
.	O

Most	O
widely	O
used	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
operate	O
on	O
sequences	O
of	O
tokens	O
corresponding	O
to	O
word	O
or	O
subword	O
units	O
.	O

By	O
comparison	O
,	O
token	NLP-algorithm/tool
-	NLP-algorithm/tool
free	NLP-algorithm/tool
models	NLP-algorithm/tool
that	O
operate	O
directly	O
on	O
raw	O
text	O
(	O
bytes	O
or	O
characters	O
)	O
have	O
many	O
benefits	O
:	O
They	O
can	O
process	O
text	O
in	O
any	O
language	O
out	O
of	O
the	O
box	O
,	O
they	O
are	O
more	O
robust	O
to	O
noise	O
,	O
and	O
they	O
minimize	O
technical	O
debt	O
by	O
removing	O
complex	O
and	O
error	O
-	O
prone	O
text	NLP-algorithm/tool
preprocessing	NLP-algorithm/tool
pipelines	O
.	O

Because	O
byte	O
or	O
character	O
sequences	O
are	O
longer	O
than	O
token	O
sequences	O
past	O
work	O
on	O
token	NLP-focus
-	NLP-focus
free	NLP-focus
models	NLP-focus
has	O
often	O
introduced	O
new	O
model	O
architectures	O
designed	O
to	O
amortize	O
the	O
cost	O
of	O
operating	O
directly	O
on	O
raw	O
text	O
.	O

In	O
this	O
paper	O
,	O
we	O
show	O
that	O
a	O
standard	O
Transformer	O
architecture	O
can	O
be	O
used	O
with	O
minimal	O
modifications	O
to	O
process	NLP-focus
byte	NLP-focus
sequences	NLP-focus
.	O

We	O
characterize	O
the	O
trade	O
-	O
offs	O
in	O
terms	O
of	O
parameter	O
count	O
training	O
FLOPs	O
and	O
inference	O
speed	O
and	O
show	O
that	O
byte	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
competitive	O
with	O
their	O
token	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
counterparts	NLP-algorithm/tool
.	O

We	O
also	O
demonstrate	O
that	O
byte	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
significantly	O
more	O
robust	O
to	O
noise	O
and	O
perform	O
better	O
on	O
tasks	O
that	O
are	O
sensitive	O
to	O
spelling	O
and	O
pronunciation	O
.	O

Persuasion	O
games	O
are	O
fundamental	O
in	O
economics	O
and	O
AI	AI/ML/DL-domain
research	AI/ML/DL-domain
and	O
serve	O
as	O
the	O
basis	O
for	O
important	O
applications	O
.	O

We	O
introduce	O
the	O
Probabilistic	O
Worldbuilding	O
Model	O
(	O
PWM	O
)	O
a	O
new	O
fully	O
symbolic	O
Bayesian	O
model	O
of	O
semantic	NLP-focus
parsing	NLP-focus
and	O
reasoning	NLP-focus
as	O
a	O
first	O
step	O
in	O
a	O
research	O
program	O
toward	O
more	O
domain	O
-	O
and	O
task	O
-	O
general	O
NLU	O
and	O
AI	AI/ML/DL-domain
.	O

We	O
introduce	O
the	O
Probabilistic	O
Worldbuilding	O
Model	O
(	O
PWM	O
)	O
a	O
new	O
fully	O
symbolic	O
Bayesian	O
model	O
of	O
semantic	NLP-focus
parsing	NLP-focus
and	O
reasoning	NLP-focus
as	O
a	O
first	O
step	O
in	O
a	O
research	O
program	O
toward	O
more	O
domain	O
-	O
and	O
task	O
-	O
general	O
NLU	O
and	O
AI	AI/ML/DL-domain
.	O

Text	NLP-focus
augmentation	NLP-focus
is	O
an	O
effective	O
technique	O
in	O
alleviating	O
overfitting	O
in	O
NLP	O
tasks	O
.	O

In	O
existing	O
methods	O
,	O
text	NLP-focus
augmentation	NLP-focus
and	O
downstream	O
tasks	O
are	O
mostly	O
performed	O
separately	O
.	O

To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
a	O
three	O
-	O
level	O
optimization	O
framework	O
to	O
perform	O
text	NLP-focus
augmentation	NLP-focus
and	O
the	O
downstream	O
task	O
end	O
-	O
to	O
-	O
end	O
.	O

A	O
text	NLP-algorithm/tool
summarization	NLP-algorithm/tool
model	NLP-algorithm/tool
is	O
trained	O
to	O
perform	O
data	NLP-focus
augmentation	NLP-focus
at	O
the	O
first	O
stage	O
.	O

A	O
text	NLP-algorithm/tool
summarization	NLP-algorithm/tool
model	NLP-algorithm/tool
is	O
trained	O
to	O
perform	O
data	NLP-focus
augmentation	NLP-focus
at	O
the	O
first	O
stage	O
.	O

Each	O
summarization	O
example	O
is	O
associated	O
with	O
a	O
weight	O
to	O
account	O
for	O
its	O
domain	O
difference	O
with	O
the	O
text	NLP-focus
classification	NLP-focus
data	O
.	O

At	O
the	O
second	O
stage	O
,	O
we	O
use	O
the	O
model	O
trained	O
at	O
the	O
first	O
stage	O
to	O
perform	O
text	NLP-focus
augmentation	NLP-focus
train	O
ain	O
a	O
text	NLP-algorithm/tool
classification	NLP-algorithm/tool
model	NLP-algorithm/tool
on	O
the	O
augmented	O
texts	O
.	O

At	O
the	O
second	O
stage	O
,	O
we	O
use	O
the	O
model	O
trained	O
at	O
the	O
first	O
stage	O
to	O
perform	O
text	NLP-focus
augmentation	NLP-focus
train	O
ain	O
a	O
text	NLP-algorithm/tool
classification	NLP-algorithm/tool
model	NLP-algorithm/tool
on	O
the	O
augmented	O
texts	O
.	O

At	O
the	O
third	O
stage	O
,	O
we	O
evaluate	O
the	O
text	NLP-algorithm/tool
classification	NLP-algorithm/tool
model	NLP-algorithm/tool
trained	O
at	O
the	O
second	O
stage	O
and	O
update	O
weights	O
of	O
summarization	O
examples	O
by	O
minimizing	O
the	O
validation	O
loss	O
.	O

We	O
evaluate	O
our	O
method	O
on	O
several	O
text	NLP-focus
classification	NLP-focus
datasets	O
where	O
the	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O

Using	O
our	O
framework	O
,	O
we	O
compare	O
numerous	O
attribution	O
methods	O
for	O
text	NLP-focus
classification	NLP-focus
and	O
question	NLP-focus
answering	NLP-focus
and	O
observe	O
quantitative	O
differences	O
that	O
are	O
consistent	O
(	O
to	O
a	O
moderate	O
to	O
high	O
degree	O
)	O
across	O
different	O
student	O
model	O
architectures	O
and	O
learning	O
strategies	O
1	O
.	O

Accurately	O
extracting	NLP-focus
structured	NLP-focus
content	NLP-focus
from	NLP-focus
PDFs	NLP-focus
is	O
a	O
critical	O
first	O
step	O
for	O
NLP	O
over	O
scientific	O
papers	O
.	O

Recent	O
work	O
has	O
improved	O
extraction	O
accuracy	O
by	O
incorporating	O
elementary	O
layout	O
information	O
,	O
for	O
example	O
,	O
each	O
token	O
’	O
s	O
2D	O
position	O
on	O
the	O
page	O
,	O
into	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
pretraining	NLP-algorithm/tool
.	O

In	O
our	O
I	O
-	O
VILA	O
approach	O
,	O
we	O
show	O
that	O
simply	O
inserting	O
special	O
tokens	O
denoting	O
layout	O
group	O
boundaries	O
into	O
model	O
inputs	O
can	O
lead	O
to	O
a	O
1	O
.	O
9	O
\\%	O
Macro	O
F1	O
improvement	O
Macro	Classification-metrics
F1	Classification-metrics
classification	O
.	O

In	O
the	O
H	O
-	O
VILA	O
approach	O
,	O
we	O
show	O
that	O
hierarchical	O
encoding	O
of	O
layout	O
-	O
groups	O
can	O
result	O
in	O
up	O
to	O
47	O
\\%	O
inference	O
time	O
reduction	O
with	O
less	O
than	O
0	O
.	O
8	O
\\%	O
Macro	O
F1	O
loss	O
Macro	Classification-metrics
F1	Classification-metrics
.	O

Unlike	O
prior	O
layout	O
-	O
aware	O
approaches	O
,	O
our	O
methods	O
do	O
not	O
require	O
expensive	O
additional	O
pretraining	O
only	O
fine	O
-	O
tuning	O
which	O
we	O
show	O
can	O
reduce	O
training	O
cost	O
by	O
up	O
to	O
95	Numerical-result
\\%	Numerical-result
.	O

Experiments	O
are	O
conducted	O
on	O
a	O
newly	O
curated	O
evaluation	O
suite	O
,	O
S2	NLP-metrics
-	NLP-metrics
VLUE	NLP-metrics
that	O
unifies	O
existing	O
automatically	O
labeled	O
datasets	O
and	O
includes	O
a	O
new	O
dataset	O
of	O
manual	O
annotations	O
covering	O
diverse	O
papers	O
from	O
19	O
scientific	O
disciplines	O
.	O

Using	O
morphological	NLP-focus
segmentation	NLP-focus
as	O
the	O
test	O
case	O
,	O
we	O
compare	O
three	O
broad	O
classes	O
of	O
models	O
with	O
different	O
parameterizations	O
taking	O
data	O
from	O
11	O
languages	O
across	O
6	O
language	O
families	O
.	O

We	O
present	O
PADA	O
An	O
example	O
-	O
based	O
autoregressive	O
Prompt	O
learning	O
algorithm	O
for	O
on	O
-	O
the	O
-	O
fly	O
Any	O
-	O
Domain	O
Adaptation	O
based	O
on	O
the	O
T5	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Given	O
a	O
test	O
example	O
,	O
PADA	O
first	O
generates	O
a	O
unique	O
prompt	O
for	O
it	O
and	O
then	O
,	O
conditioned	O
on	O
this	O
prompt	O
,	O
labels	O
the	O
example	O
with	O
respect	O
to	O
the	O
NLP	NLP-focus
prediction	NLP-focus
task	NLP-focus
.	O

In	O
experiments	O
with	O
3	O
tasks	O
(	O
text	NLP-focus
classification	NLP-focus
and	O
sequence	NLP-focus
tagging	NLP-focus
,	O
for	O
a	O
total	O
of	O
14	O
multi	O
-	O
source	O
adaptation	O
scenarios	O
PADA	O
substantially	O
outperforms	O
strong	O
baselines	O
.	O
1	O
.	O

However	O
,	O
long	NLP-focus
text	NLP-focus
modeling	NLP-focus
modeling	O
many	O
distinct	O
abilities	O
in	O
contrast	O
to	O
short	O
texts	O
,	O
such	O
as	O
the	O
modeling	O
of	O
long	NLP-focus
-	NLP-focus
range	NLP-focus
discourse	NLP-focus
and	O
commonsense	NLP-focus
relations	NLP-focus
and	O
the	O
coherence	O
and	O
controllability	O
of	O
generation	O
.	O

Therefore	O
,	O
we	O
propose	O
a	O
story	O
-	O
centric	O
benchmark	O
named	O
LOT	O
for	O
evaluating	O
Chinese	NLP-focus
long	NLP-focus
text	NLP-focus
modeling	NLP-focus
which	O
aggregates	O
two	O
understanding	O
tasks	O
and	O
two	O
generation	O
tasks	O
.	O

We	O
pretrain	O
LongLM	O
on	O
120G	O
Chinese	O
novels	O
with	O
two	O
generative	O
tasks	O
including	O
text	NLP-focus
infilling	NLP-focus
and	O
conditional	NLP-focus
continuation	NLP-focus
.	O

Extensive	O
experiments	O
show	O
that	O
LongLM	O
outperforms	O
similar	O
-	O
sized	O
pretraining	NLP-algorithm/tool
models	NLP-algorithm/tool
substantially	O
on	O
both	O
the	O
understanding	O
and	O
generation	O
tasks	O
in	O
LOT	O
.	O

We	O
introduce	O
a	O
large	O
and	O
diverse	O
Czech	O
corpus	O
annotated	O
for	O
grammatical	NLP-focus
error	NLP-focus
correction	NLP-focus
(	NLP-focus
GEC	NLP-focus
)	NLP-focus
with	O
the	O
aim	O
to	O
contribute	O
to	O
the	O
still	O
scarce	O
data	O
resources	O
in	O
this	O
domain	O
for	O
languages	O
other	O
than	O
English	O
.	O

We	O
compare	O
several	O
Czech	NLP-focus
GEC	NLP-focus
systems	NLP-focus
including	O
several	O
Transformer	O
based	O
ones	O
,	O
setting	O
a	O
strong	O
baseline	O
to	O
future	O
research	O
.	O

Finally	O
,	O
we	O
meta	O
-	O
evaluate	O
common	O
GEC	NLP-focus
metrics	O
against	O
human	O
judgments	O
on	O
our	O
data	O
.	O

In	O
a	O
conversational	NLP-focus
question	NLP-focus
answering	NLP-focus
scenario	O
,	O
a	O
questioner	O
seeks	O
to	O
extract	O
information	O
about	O
a	O
topic	O
through	O
a	O
series	O
of	O
interdependent	O
questions	O
and	O
answers	O
.	O

However	O
,	O
current	O
datasets	O
for	O
conversational	NLP-focus
question	NLP-focus
answering	NLP-focus
are	O
limiting	O
in	O
two	O
ways	O
:	O
1	O
)	O
they	O
do	O
not	O
contain	O
topic	O
switches	O
;	O
and	O
2	O
)	O
they	O
assume	O
the	O
reference	O
text	O
for	O
the	O
conversation	O
is	O
given	O
,	O
that	O
is	O
,	O
the	O
setting	O
is	O
not	O
open	O
-	O
domain	O
.	O

Our	O
best	O
model	O
achieves	O
F1	Classification-metrics
of	O
55	Numerical-result
.	Numerical-result
8	Numerical-result
falling	O
short	O
of	O
human	O
performance	O
by	O
14	Numerical-result
.	Numerical-result
2	Numerical-result
points	O
,	O
indicating	O
the	O
difficulty	O
of	O
our	O
dataset	O
.	O

Our	O
best	O
model	O
achieves	O
F1	Classification-metrics
of	O
55	Numerical-result
.	Numerical-result
8	Numerical-result
falling	O
short	O
of	O
human	O
performance	O
by	O
14	Numerical-result
.	Numerical-result
2	Numerical-result
points	O
,	O
indicating	O
the	O
difficulty	O
of	O
our	O
dataset	O
.	O

We	O
propose	O
a	O
novel	O
framework	O
for	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
content	NLP-focus
flagging	NLP-focus
with	O
limited	O
target	O
-	O
language	O
data	O
,	O
which	O
significantly	O
outperforms	O
prior	O
work	O
in	O
terms	O
of	O
predictive	O
performance	O
.	O

Our	O
evaluation	O
results	O
on	O
eight	O
languages	O
from	O
two	O
different	O
datasets	O
for	O
abusive	NLP-focus
language	NLP-focus
detection	NLP-focus
show	O
sizable	O
improvements	O
of	O
up	O
to	O
9	Numerical-result
.	Numerical-result
5	Numerical-result
F1	Classification-metrics
points	O
absolute	O
(	O
for	O
Italian	O
)	O
over	O
strong	O
baselines	O
.	O

Our	O
evaluation	O
results	O
on	O
eight	O
languages	O
from	O
two	O
different	O
datasets	O
for	O
abusive	NLP-focus
language	NLP-focus
detection	NLP-focus
show	O
sizable	O
improvements	O
of	O
up	O
to	O
9	Numerical-result
.	Numerical-result
5	Numerical-result
F1	Classification-metrics
points	O
absolute	O
(	O
for	O
Italian	O
)	O
over	O
strong	O
baselines	O
.	O

Our	O
evaluation	O
results	O
on	O
eight	O
languages	O
from	O
two	O
different	O
datasets	O
for	O
abusive	NLP-focus
language	NLP-focus
detection	NLP-focus
show	O
sizable	O
improvements	O
of	O
up	O
to	O
9	Numerical-result
.	Numerical-result
5	Numerical-result
F1	Classification-metrics
points	O
absolute	O
(	O
for	O
Italian	O
)	O
over	O
strong	O
baselines	O
.	O

On	O
average	O
,	O
we	O
achieve	O
3	Numerical-result
.	Numerical-result
6	Numerical-result
absolute	O
F1	Classification-metrics
points	O
of	O
improvement	O
for	O
the	O
three	O
languages	O
in	O
the	O
Jigsaw	O
Multilingual	O
dataset	O
and	O
2	Numerical-result
.	Numerical-result
14	Numerical-result
points	O
for	O
the	O
WUL	O
dataset	O
.	O

On	O
average	O
,	O
we	O
achieve	O
3	Numerical-result
.	Numerical-result
6	Numerical-result
absolute	O
F1	Classification-metrics
points	O
of	O
improvement	O
for	O
the	O
three	O
languages	O
in	O
the	O
Jigsaw	O
Multilingual	O
dataset	O
and	O
2	Numerical-result
.	Numerical-result
14	Numerical-result
points	O
for	O
the	O
WUL	O
dataset	O
.	O

To	O
address	O
these	O
crucial	O
gaps	O
towards	O
both	O
improved	O
and	O
efficient	O
cross	O
-	O
modal	O
retrieval	O
we	O
propose	O
a	O
novel	O
fine	O
-	O
tuning	O
framework	O
that	O
turns	O
any	O
pretrained	NLP-algorithm/tool
text	NLP-algorithm/tool
-	NLP-algorithm/tool
image	NLP-algorithm/tool
multi	NLP-algorithm/tool
-	NLP-algorithm/tool
modal	NLP-algorithm/tool
model	NLP-algorithm/tool
into	O
an	O
efficient	O
retrieval	O
model	O
.	O

Our	O
experiments	O
on	O
a	O
series	O
of	O
standard	O
cross	O
-	O
modal	O
retrieval	O
benchmarks	O
in	O
monolingual	O
multilingual	O
and	O
zero	O
-	O
shot	O
setups	O
,	O
demonstrate	O
improved	O
accuracy	Classification-metrics
and	O
huge	O
efficiency	O
benefits	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
cross	O
-	O
encoders	O
1	O
.	O

One	O
of	O
the	O
biggest	O
challenges	O
hindering	O
progress	O
in	O
low	O
-	O
resource	O
and	O
multilingual	NLP-focus
machine	NLP-focus
translation	NLP-focus
is	O
the	O
lack	O
of	O
good	O
evaluation	O
benchmarks	O
.	O

The	O
resulting	O
dataset	O
enables	O
better	O
assessment	O
of	O
model	O
quality	O
on	O
the	O
long	O
tail	O
of	O
low	O
-	O
resource	O
languages	O
including	O
the	O
evaluation	O
of	O
many	NLP-algorithm/tool
-	NLP-algorithm/tool
to	NLP-algorithm/tool
-	NLP-algorithm/tool
many	NLP-algorithm/tool
multilingual	NLP-algorithm/tool
translation	NLP-algorithm/tool
systems	NLP-algorithm/tool
as	O
all	O
translations	O
are	O
fully	O
aligned	O
.	O

By	O
publicly	O
releasing	O
such	O
a	O
high	O
-	O
quality	O
and	O
high	O
-	O
coverage	O
dataset	O
we	O
hope	O
to	O
foster	O
progress	O
in	O
the	O
machine	NLP-focus
translation	NLP-focus
community	O
and	O
beyond	O
.	O

Multihop	NLP-focus
reasoning	NLP-focus
remains	O
an	O
elusive	O
goal	O
as	O
existing	O
multihop	O
benchmarks	O
are	O
known	O
to	O
be	O
largely	O
solvable	O
via	O
shortcuts	O
.	O

Can	O
we	O
create	O
a	O
question	O
answering	O
(	O
QA	O
)	O
dataset	O
that	O
,	O
by	O
construction	O
,	O
requires	O
proper	O
multihop	NLP-focus
reasoning	NLP-focus
To	O
this	O
end	O
,	O
we	O
introduce	O
a	O
bottom	O
–	O
up	O
approach	O
that	O
systematically	O
selects	O
composable	O
pairs	O
of	O
single	O
-	O
hop	O
questions	O
that	O
are	O
connected	O
,	O
that	O
is	O
,	O
where	O
one	O
reasoning	O
step	O
critically	O
relies	O
on	O
information	O
from	O
another	O
.	O

We	O
present	O
a	O
memory	O
-	O
augmented	O
approach	O
to	O
condition	O
an	O
autoregressive	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
on	O
a	O
knowledge	O
graph	O
.	O

We	O
represent	O
the	O
graph	O
as	O
a	O
collection	O
of	O
relation	O
triples	O
and	O
retrieve	O
relevant	O
relations	O
for	O
a	O
given	O
context	O
to	O
improve	O
text	NLP-focus
generation	NLP-focus
.	O

Experiments	O
on	O
WikiText	O
-	O
103	O
WMT19	O
and	O
enwik8	O
English	O
datasets	O
demonstrate	O
that	O
our	O
approach	O
produces	O
a	O
better	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
in	O
terms	O
of	O
perplexity	NLP-metrics
and	O
bits	O
per	O
character	O
.	O

Experiments	O
on	O
WikiText	O
-	O
103	O
WMT19	O
and	O
enwik8	O
English	O
datasets	O
demonstrate	O
that	O
our	O
approach	O
produces	O
a	O
better	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
in	O
terms	O
of	O
perplexity	NLP-metrics
and	O
bits	O
per	O
character	O
.	O

Our	O
model	O
provides	O
a	O
simple	O
yet	O
effective	O
way	O
to	O
combine	O
an	O
autoregressive	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
a	O
knowledge	O
graph	O
for	O
more	O
coherent	O
and	O
logical	O
generation	O
.	O

Existing	O
methods	O
to	O
measure	O
sentence	NLP-focus
similarity	NLP-focus
are	O
faced	O
with	O
two	O
challenges	O
:	O
(	O
1	O
)	O
labeled	O
datasets	O
are	O
usually	O
limited	O
in	O
size	O
,	O
making	O
them	O
insufficient	O
to	O
train	O
supervised	O
neural	O
models	O
and	O
(	O
2	O
)	O
there	O
is	O
a	O
training	O
-	O
test	O
gap	O
for	O
unsupervised	NLP-algorithm/tool
language	NLP-algorithm/tool
modeling	NLP-algorithm/tool
(	NLP-algorithm/tool
LM	NLP-algorithm/tool
)	NLP-algorithm/tool
based	O
models	O
to	O
compute	O
semantic	O
scores	O
between	O
sentences	O
,	O
since	O
sentence	O
-	O
level	O
semantics	O
training	O
xplicitly	O
modeled	O
at	O
training	O
.	O

Existing	O
methods	O
to	O
measure	O
sentence	NLP-focus
similarity	NLP-focus
are	O
faced	O
with	O
two	O
challenges	O
:	O
(	O
1	O
)	O
labeled	O
datasets	O
are	O
usually	O
limited	O
in	O
size	O
,	O
making	O
them	O
insufficient	O
to	O
train	O
supervised	O
neural	O
models	O
and	O
(	O
2	O
)	O
there	O
is	O
a	O
training	O
-	O
test	O
gap	O
for	O
unsupervised	NLP-algorithm/tool
language	NLP-algorithm/tool
modeling	NLP-algorithm/tool
(	NLP-algorithm/tool
LM	NLP-algorithm/tool
)	NLP-algorithm/tool
based	O
models	O
to	O
compute	O
semantic	O
scores	O
between	O
sentences	O
,	O
since	O
sentence	O
-	O
level	O
semantics	O
training	O
xplicitly	O
modeled	O
at	O
training	O
.	O

Our	O
experiments	O
show	O
that	O
models	O
based	O
solely	O
on	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
perform	O
substantially	O
worse	O
than	O
humans	O
on	O
these	O
tasks	O
.	O

The	O
task	O
of	O
ultra	NLP-focus
-	NLP-focus
fine	NLP-focus
entity	NLP-focus
typing	NLP-focus
(	NLP-focus
UFET	NLP-focus
)	NLP-focus
seeks	O
to	O
predict	O
diverse	O
and	O
free	O
-	O
form	O
words	O
or	O
phrases	O
that	O
describe	O
the	O
appropriate	O
types	O
of	O
entities	O
mentioned	O
in	O
sentences	O
.	O

This	O
causes	O
two	O
issues	O
:	O
(	O
i	O
)	O
the	O
classifiers	O
do	O
not	O
capture	O
the	O
type	O
semantics	O
because	O
types	O
are	O
often	O
converted	O
into	O
indices	O
;	O
(	O
ii	O
)	O
systems	O
developed	O
in	O
this	O
way	O
are	O
limited	O
to	O
predicting	O
within	O
a	O
pre	O
-	O
defined	O
type	O
set	O
,	O
and	O
often	O
fall	O
short	O
of	O
generalizing	O
to	O
types	O
that	O
are	O
rarely	O
seen	O
or	O
unseen	O
in	O
training	O
This	O
work	O
presents	O
LITE	O
a	O
new	O
approach	O
that	O
formulates	O
entity	NLP-focus
typing	NLP-focus
as	O
a	O
natural	O
language	O
inference	O
(	O
NLI	O
)	O
problem	O
,	O
making	O
use	O
of	O
(	O
i	O
)	O
the	O
indirect	O
supervision	O
NLI	O
NLI	O
to	O
infer	O
type	O
information	O
meaningfully	O
represented	O
as	O
textual	O
hypotheses	O
and	O
alleviate	O
the	O
data	O
scarcity	O
issue	O
,	O
as	O
well	O
as	O
(	O
ii	O
)	O
a	O
learning	O
-	O
to	O
-	O
rank	O
objective	O
to	O
avoid	O
the	O
pre	O
-	O
defining	O
of	O
a	O
type	O
set	O
.	O

Experiments	O
show	O
that	O
,	O
with	O
limited	O
training	O
data	O
LITE	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
UFET	NLP-focus
task	O
.	O

In	O
addition	O
,	O
LITE	O
demonstrates	O
its	O
strong	O
generalizability	O
by	O
not	O
only	O
yielding	O
best	O
results	O
on	O
other	O
fine	NLP-focus
-	NLP-focus
grained	NLP-focus
entity	NLP-focus
typing	NLP-focus
benchmarks	O
,	O
more	O
importantly	O
,	O
a	O
pre	O
-	O
trained	O
LITE	O
system	O
works	O
well	O
on	O
new	O
data	O
containing	O
unseen	O
types	O
.	O
1	O
.	O

For	O
query	NLP-focus
-	NLP-focus
focused	NLP-focus
summarization	NLP-focus
(	NLP-focus
QFS	NLP-focus
)	NLP-focus
labeled	O
training	O
data	O
in	O
the	O
form	O
of	O
queries	O
,	O
documents	O
,	O
and	O
summaries	O
is	O
not	O
readily	O
available	O
.	O

We	O
provide	O
a	O
unified	O
modeling	O
framework	O
for	O
any	O
kind	O
of	O
summarization	NLP-focus
under	O
the	O
assumption	O
that	O
all	O
summaries	O
are	O
a	O
response	O
to	O
a	O
query	O
which	O
is	O
observed	O
in	O
the	O
case	O
of	O
QFS	NLP-focus
and	O
latent	O
in	O
the	O
case	O
of	O
generic	NLP-focus
summarization	NLP-focus
.	O

Our	O
framework	O
formulates	O
summarization	NLP-focus
as	O
a	O
generative	O
process	O
and	O
jointly	O
optimizes	O
a	O
latent	NLP-algorithm/tool
query	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
a	O
conditional	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Our	O
framework	O
formulates	O
summarization	NLP-focus
as	O
a	O
generative	O
process	O
and	O
jointly	O
optimizes	O
a	O
latent	NLP-algorithm/tool
query	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
a	O
conditional	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Despite	O
learning	O
from	O
generic	NLP-focus
summarization	NLP-focus
data	O
only	O
,	O
our	O
approach	O
outperforms	O
strong	O
comparison	O
systems	O
across	O
benchmarks	O
,	O
query	O
types	O
,	O
document	O
settings	O
,	O
and	O
target	O
domains	O
.	O
1	O
.	O

Mining	O
an	O
argument	O
structure	O
from	O
text	O
is	O
an	O
important	O
step	O
for	O
tasks	O
such	O
as	O
argument	NLP-focus
search	NLP-focus
and	O
summarization	NLP-focus
.	O

While	O
studies	O
on	O
argument	NLP-focus
(	NLP-focus
ation	NLP-focus
)	NLP-focus
mining	NLP-focus
have	O
proposed	O
promising	O
neural	O
network	O
models	O
they	O
usually	O
suffer	O
from	O
a	O
shortage	O
of	O
training	O
data	O
.	O

To	O
address	O
this	O
issue	O
,	O
we	O
expand	O
the	O
training	O
data	O
with	O
various	O
auxiliary	NLP-focus
argument	NLP-focus
mining	NLP-focus
corpora	O
and	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
cross	O
-	O
corpus	O
training	O
method	O
called	O
Multi	O
-	O
Task	O
Argument	O
Mining	O
(	O
MT	O
-	O
AM	O
)	O

To	O
evaluate	O
our	O
approach	O
,	O
we	O
conducted	O
experiments	O
for	O
the	O
main	O
argument	NLP-focus
mining	NLP-focus
tasks	O
on	O
several	O
well	O
-	O
established	O
argument	O
mining	O
corpora	O
.	O

Do	O
the	O
prevalent	O
*	NLP-algorithm/tool
BERT	NLP-algorithm/tool
-	NLP-algorithm/tool
family	NLP-algorithm/tool
of	NLP-algorithm/tool
models	NLP-algorithm/tool
do	O
so	O
?	O
In	O
this	O
paper	O
,	O
we	O
study	O
this	O
question	O
using	O
the	O
problem	O
of	O
reasoning	O
on	O
tabular	O
data	O
.	O

Our	O
experiments	O
demonstrate	O
that	O
a	O
RoBERTa	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
model	NLP-algorithm/tool
representative	O
of	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
fails	O
at	O
reasoning	O
on	O
the	O
following	O
counts	O
:	O
it	O
(	O
a	O
)	O
ignores	O
relevant	O
parts	O
of	O
the	O
evidence	O
,	O
(	O
b	O
)	O
is	O
over	O
-	O
sensitive	O
to	O
annotation	O
artifacts	O
,	O
and	O
(	O
c	O
)	O
relies	O
on	O
the	O
knowledge	O
encoded	O
in	O
the	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
rather	O
than	O
the	O
evidence	O
presented	O
in	O
its	O
tabular	O
inputs	O
.	O

In	O
this	O
paper	O
,	O
we	O
quantify	O
the	O
calibration	O
of	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
for	O
text	NLP-focus
regression	NLP-focus
both	O
intrinsically	O
and	O
extrinsically	O
.	O

In	O
this	O
paper	O
,	O
we	O
quantify	O
the	O
calibration	O
of	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
for	O
text	NLP-focus
regression	NLP-focus
both	O
intrinsically	O
and	O
extrinsically	O
.	O

Our	O
experiments	O
on	O
three	O
regression	NLP-algorithm/tool
tasks	O
in	O
both	O
self	O
-	O
training	O
and	O
active	O
-	O
learning	O
settings	O
show	O
that	O
uncertainty	O
estimation	O
can	O
be	O
used	O
to	O
increase	O
overall	O
performance	O
and	O
enhance	O
model	O
generalization	O
.	O

We	O
consider	O
the	O
task	O
of	O
data	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
text	NLP-focus
generation	NLP-focus
which	O
aims	O
to	O
create	O
textual	O
output	O
from	O
non	O
-	O
linguistic	O
input	O
.	O

We	O
focus	O
on	O
generating	NLP-focus
long	NLP-focus
-	NLP-focus
form	NLP-focus
text	NLP-focus
that	O
is	O
,	O
documents	O
with	O
multiple	O
paragraphs	O
,	O
and	O
propose	O
a	O
neural	O
model	O
enhanced	O
with	O
a	O
planning	O
component	O
responsible	O
for	O
organizing	O
high	O
-	O
level	O
information	O
in	O
a	O
coherent	O
and	O
meaningful	O
way	O
.	O

In	O
view	O
of	O
this	O
,	O
we	O
conduct	O
an	O
extensive	O
study	O
of	O
Pet	NLP-algorithm/tool
a	O
method	O
that	O
combines	O
textual	O
instructions	O
with	O
example	O
-	O
based	O
finetuning	O
.	O

We	O
show	O
that	O
,	O
if	O
correctly	O
configured	O
,	O
Pet	NLP-algorithm/tool
performs	O
strongly	O
in	O
true	O
few	O
-	O
shot	O
settings	O
without	O
a	O
dev	O
set	O
.	O

Crucial	O
for	O
this	O
strong	O
performance	O
is	O
a	O
number	O
of	O
design	O
choices	O
,	O
including	O
Pet	NLP-algorithm/tool
’	NLP-algorithm/tool
s	NLP-algorithm/tool
ability	O
to	O
intelligently	O
handle	O
multiple	O
prompts	O
.	O

We	O
put	O
our	O
findings	O
to	O
a	O
real	O
-	O
world	O
test	O
by	O
running	O
Pet	NLP-algorithm/tool
on	O
RAFT	O
a	O
benchmark	O
of	O
tasks	O
taken	O
from	O
realistic	O
NLP	O
applications	O
for	O
which	O
no	O
labeled	O
dev	O
or	O
test	O
sets	O
are	O
available	O
.	O

Pet	NLP-algorithm/tool
achieves	O
a	O
new	O
state	O
of	O
the	O
art	O
on	O
RAFT	O
and	O
performs	O
close	O
to	O
non	O
-	O
expert	O
humans	O
for	O
7	O
out	O
of	O
11	O
tasks	O
.	O

To	O
this	O
end	O
,	O
this	O
paper	O
develops	O
the	O
heterogeneous	O
supervised	O
topic	O
model	O
(	O
HSTM	O
)	O
a	O
probabilistic	O
approach	O
to	O
text	NLP-focus
analysis	NLP-focus
and	NLP-focus
prediction	NLP-focus
.	O

Automating	O
the	O
fact	NLP-focus
checking	NLP-focus
(	NLP-focus
FC	NLP-focus
)	NLP-focus
process	O
relies	O
on	O
information	O
obtained	O
from	O
external	O
sources	O
.	O

In	O
this	O
work	O
,	O
we	O
posit	O
that	O
it	O
is	O
crucial	O
for	O
FC	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
make	O
veracity	O
predictions	O
only	O
when	O
there	O
is	O
sufficient	O
evidence	O
and	O
otherwise	O
indicate	O
when	O
it	O
is	O
not	O
enough	O
.	O

To	O
this	O
end	O
,	O
we	O
are	O
the	O
first	O
to	O
study	O
what	O
information	O
FC	NLP-algorithm/tool
models	NLP-algorithm/tool
consider	O
sufficient	O
by	O
introducing	O
a	O
novel	O
task	O
and	O
advancing	O
it	O
with	O
three	O
main	O
contributions	O
.	O

We	O
identify	O
when	O
models	O
consider	O
the	O
remaining	O
evidence	O
(	O
in	O
)	O
sufficient	O
for	O
FC	NLP-focus
based	O
on	O
three	O
trained	O
models	O
with	O
different	O
Transformer	O
FC	NLP-focus
hitectures	O
and	O
three	O
FC	O
datasets	O
.	O

Second	O
,	O
we	O
ask	O
annotators	O
whether	O
the	O
omitted	O
evidence	O
was	O
important	O
for	O
FC	NLP-focus
resulting	O
in	O
a	O
novel	O
diagnostic	O
dataset	O
SufficientFacts1	O
for	O
FC	O
with	O
omitted	O
evidence	O
.	O

We	O
find	O
that	O
models	O
are	O
least	O
successful	O
in	O
detecting	O
missing	O
evidence	O
when	O
adverbial	O
modifiers	O
are	O
omitted	O
(	O
21	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
,	O
whereas	O
it	O
is	O
easiest	O
for	O
omitted	O
date	O
modifiers	O
(	O
63	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
.	O

We	O
find	O
that	O
models	O
are	O
least	O
successful	O
in	O
detecting	O
missing	O
evidence	O
when	O
adverbial	O
modifiers	O
are	O
omitted	O
(	O
21	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
,	O
whereas	O
it	O
is	O
easiest	O
for	O
omitted	O
date	O
modifiers	O
(	O
63	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
.	O

It	O
improves	O
performance	O
for	O
Evidence	NLP-focus
Sufficiency	NLP-focus
Prediction	NLP-focus
by	O
up	O
to	O
17	Numerical-result
.	Numerical-result
8	Numerical-result
F1	Classification-metrics
score	O
,	O
which	O
in	O
turn	O
improves	O
FC	O
performance	O
by	O
up	O
to	O
2	Numerical-result
.	Numerical-result
6	Numerical-result
F1	Classification-metrics
score	O
.	O

It	O
improves	O
performance	O
for	O
Evidence	NLP-focus
Sufficiency	NLP-focus
Prediction	NLP-focus
by	O
up	O
to	O
17	Numerical-result
.	Numerical-result
8	Numerical-result
F1	Classification-metrics
score	O
,	O
which	O
in	O
turn	O
improves	O
FC	O
performance	O
by	O
up	O
to	O
2	Numerical-result
.	Numerical-result
6	Numerical-result
F1	Classification-metrics
score	O
.	O

It	O
improves	O
performance	O
for	O
Evidence	NLP-focus
Sufficiency	NLP-focus
Prediction	NLP-focus
by	O
up	O
to	O
17	Numerical-result
.	Numerical-result
8	Numerical-result
F1	Classification-metrics
score	O
,	O
which	O
in	O
turn	O
improves	O
FC	O
performance	O
by	O
up	O
to	O
2	Numerical-result
.	Numerical-result
6	Numerical-result
F1	Classification-metrics
score	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
novel	O
task	O
termed	O
t	O
ext	NLP-focus
-	NLP-focus
based	NLP-focus
NP	NLP-focus
enrichment	NLP-focus
(	NLP-focus
TNE	NLP-focus
)	NLP-focus
NP	O
which	O
we	O
aim	O
to	O
enrich	O
each	O
NP	O
in	O
a	O
text	O
with	O
all	O
the	O
preposition	O
-	O
mediated	O
relations	O
—	O
either	O
explicit	O
or	O
implicit	O
—	O
that	O
hold	O
between	O
it	O
and	O
other	O
NPs	O
in	O
the	O
text	O
.	O

We	O
build	O
the	O
first	O
large	O
-	O
scale	O
dataset	O
for	O
the	O
problem	O
,	O
provide	O
the	O
formal	O
framing	O
and	O
scope	O
of	O
annotation	O
,	O
analyze	O
the	O
data	O
,	O
and	O
report	O
the	O
results	O
of	O
fine	NLP-algorithm/tool
-	NLP-algorithm/tool
tuned	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
on	O
the	O
task	O
,	O
demonstrating	O
the	O
challenge	O
it	O
poses	O
to	O
current	O
technology	O
.	O

We	O
train	O
neural	O
networks	O
to	O
optimize	O
a	O
Minimum	O
Description	O
Length	O
score	O
,	O
that	O
is	O
,	O
to	O
balance	O
between	O
the	O
complexity	O
of	O
the	O
network	O
and	O
its	O
accuracy	Classification-metrics
at	O
a	O
task	O
.	O

Moreover	O
,	O
they	O
often	O
do	O
so	O
with	O
100	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
.	O

Moreover	O
,	O
they	O
often	O
do	O
so	O
with	O
100	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
.	O

We	O
thus	O
provide	O
formal	O
proofs	O
that	O
their	O
perfect	O
accuracy	Classification-metrics
holds	O
not	O
only	O
on	O
a	O
given	O
test	O
set	O
,	O
but	O
for	O
any	O
input	O
sequence	O
.	O

In	O
Neural	NLP-focus
Machine	NLP-focus
Translation	NLP-focus
it	O
is	O
typically	O
assumed	O
that	O
the	O
sentence	O
with	O
the	O
highest	O
estimated	O
probability	O
should	O
also	O
be	O
the	O
translation	O
with	O
the	O
highest	O
quality	O
as	O
measured	O
by	O
humans	O
.	O

Our	O
experiments	O
show	O
that	O
the	O
combination	O
of	O
a	O
neural	NLP-algorithm/tool
translation	NLP-algorithm/tool
model	NLP-algorithm/tool
with	O
a	O
neural	O
reference	O
-	O
based	O
metric	O
Bleurt	NLP-metrics
results	O
in	O
significant	O
improvement	O
in	O
human	O
evaluations	O
.	O

Our	O
experiments	O
show	O
that	O
the	O
combination	O
of	O
a	O
neural	NLP-algorithm/tool
translation	NLP-algorithm/tool
model	NLP-algorithm/tool
with	O
a	O
neural	O
reference	O
-	O
based	O
metric	O
Bleurt	NLP-metrics
results	O
in	O
significant	O
improvement	O
in	O
human	O
evaluations	O
.	O

This	O
improvement	O
is	O
obtained	O
with	O
translations	O
different	O
from	O
classical	O
beam	O
-	O
search	O
output	O
:	O
These	O
translations	O
have	O
much	O
lower	O
model	O
likelihood	O
and	O
are	O
less	O
favored	O
by	O
surface	O
metrics	O
like	O
Bleu	NLP-metrics
.	O

This	O
paper	O
studies	O
the	O
use	O
of	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
as	O
a	O
source	O
of	O
synthetic	O
unlabeled	O
text	O
for	O
NLP	O
.	O

To	O
generate	O
high	O
-	O
quality	O
task	O
-	O
specific	O
text	O
,	O
we	O
either	O
fine	O
-	O
tune	O
LMs	NLP-algorithm/tool
on	O
inputs	O
from	O
the	O
task	O
of	O
interest	O
,	O
or	O
prompt	O
LMs	NLP-algorithm/tool
e	O
LMs	O
with	O
few	O
examples	O
.	O

We	O
use	O
the	O
best	O
available	O
classifier	O
to	O
annotate	O
synthetic	O
text	O
with	O
soft	O
pseudo	O
labels	O
for	O
knowledge	O
distillation	O
and	O
self	O
-	O
training	O
and	O
use	O
LMs	NLP-algorithm/tool
to	O
obtain	O
hard	O
labels	O
for	O
few	O
-	O
shot	O
learning	O
.	O

We	O
investigate	O
key	O
components	O
of	O
GAL	O
and	O
present	O
theoretical	O
and	O
empirical	O
arguments	O
against	O
the	O
use	O
of	O
class	NLP-algorithm/tool
-	NLP-algorithm/tool
conditional	NLP-algorithm/tool
LMs	NLP-algorithm/tool
to	O
generate	O
synthetic	O
labeled	O
text	O
instead	O
of	O
unlabeled	O
text	O
.	O

While	O
improving	O
neural	NLP-algorithm/tool
dialogue	NLP-algorithm/tool
agents	NLP-algorithm/tool
factual	O
accuracy	Classification-metrics
neural	O
dialogue	O
much	O
research	O
,	O
another	O
important	O
aspect	O
of	O
communication	O
,	O
less	O
studied	O
in	O
the	O
setting	O
of	O
neural	O
dialogue	O
,	O
is	O
transparency	O
about	O
ignorance	O
.	O

While	O
improving	O
neural	NLP-algorithm/tool
dialogue	NLP-algorithm/tool
agents	NLP-algorithm/tool
factual	O
accuracy	Classification-metrics
neural	O
dialogue	O
much	O
research	O
,	O
another	O
important	O
aspect	O
of	O
communication	O
,	O
less	O
studied	O
in	O
the	O
setting	O
of	O
neural	O
dialogue	O
,	O
is	O
transparency	O
about	O
ignorance	O
.	O

In	O
this	O
work	O
,	O
we	O
analyze	O
to	O
what	O
extent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
chit	NLP-algorithm/tool
-	NLP-algorithm/tool
chat	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
linguistically	O
calibrated	O
in	O
the	O
sense	O
that	O
their	O
verbalized	O
expression	O
of	O
doubt	O
(	O
or	O
confidence	O
)	O
matches	O
the	O
likelihood	O
that	O
the	O
model	O
’	O
s	O
responses	O
are	O
factually	O
incorrect	O
(	O
or	O
correct	O
).	O

Reinforcement	AI/ML/DL-domain
Learning	AI/ML/DL-domain
has	O
shown	O
success	O
in	O
a	O
number	O
of	O
complex	O
virtual	O
environments	O
.	O

Interactive	NLP-focus
Fiction	NLP-focus
Games	NLP-focus
(	O
or	O
Text	NLP-focus
Games	NLP-focus
are	O
one	O
such	O
problem	O
type	O
that	O
offer	O
a	O
set	O
of	O
safe	O
,	O
partially	O
observable	O
environments	O
where	O
natural	O
language	O
is	O
required	O
as	O
part	O
of	O
the	O
Reinforcement	AI/ML/DL-domain
Learning	AI/ML/DL-domain
solution	O
.	O

Interactive	NLP-focus
Fiction	NLP-focus
Games	NLP-focus
(	O
or	O
Text	NLP-focus
Games	NLP-focus
are	O
one	O
such	O
problem	O
type	O
that	O
offer	O
a	O
set	O
of	O
safe	O
,	O
partially	O
observable	O
environments	O
where	O
natural	O
language	O
is	O
required	O
as	O
part	O
of	O
the	O
Reinforcement	AI/ML/DL-domain
Learning	AI/ML/DL-domain
solution	O
.	O

Therefore	O
,	O
this	O
survey	O
’	O
s	O
aim	O
is	O
to	O
assist	O
in	O
the	O
development	O
of	O
new	O
Text	NLP-focus
Game	NLP-focus
problem	NLP-focus
settings	O
and	O
solutions	O
for	O
Reinforcement	AI/ML/DL-domain
Learning	AI/ML/DL-domain
informed	O
by	O
natural	O
language	O
.	O

Therefore	O
,	O
this	O
survey	O
’	O
s	O
aim	O
is	O
to	O
assist	O
in	O
the	O
development	O
of	O
new	O
Text	NLP-focus
Game	NLP-focus
problem	NLP-focus
settings	O
and	O
solutions	O
for	O
Reinforcement	AI/ML/DL-domain
Learning	AI/ML/DL-domain
informed	O
by	O
natural	O
language	O
.	O

Specifically	O
,	O
this	O
survey	O
:	O
1	O
)	O
introduces	O
the	O
challenges	O
in	O
Text	NLP-focus
Game	NLP-focus
Reinforcement	NLP-focus
Learning	NLP-focus
problems	O
,	O
2	O
)	O
outlines	O
the	O
generation	O
tools	O
for	O
rendering	O
Text	NLP-focus
Games	NLP-focus
and	O
the	O
subsequent	O
environments	O
generated	O
,	O
and	O
3	O
)	O
compares	O
the	O
agent	O
architectures	O
currently	O
applied	O
to	O
provide	O
a	O
systematic	O
review	O
of	O
benchmark	O
methodologies	O
and	O
opportunities	O
for	O
future	O
researchers	O
.	O

Greedy	O
algorithms	O
for	O
NLP	O
such	O
as	O
transition	NLP-focus
-	NLP-focus
based	NLP-focus
parsing	NLP-focus
are	O
prone	O
to	O
error	O
propagation	O
.	O

In	O
order	O
to	O
implement	O
such	O
a	O
behavior	O
,	O
we	O
use	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
and	O
let	O
the	O
algorithm	O
backtrack	O
in	O
cases	O
where	O
such	O
an	O
action	O
gets	O
a	O
better	O
reward	O
than	O
continuing	O
to	O
explore	O
the	O
current	O
solution	O
.	O

We	O
test	O
this	O
idea	O
on	O
both	O
POS	NLP-algorithm/tool
tagging	NLP-algorithm/tool
and	O
dependency	NLP-algorithm/tool
parsing	NLP-algorithm/tool
and	O
show	O
that	O
backtracking	O
is	O
an	O
effective	O
means	O
to	O
fight	O
against	O
error	O
propagation	O
.	O

Self	O
-	O
labeling	O
shows	O
consistent	O
improvement	O
and	O
notably	O
,	O
for	O
named	NLP-focus
entity	NLP-focus
recognition	NLP-focus
leads	O
to	O
better	O
temporal	O
adaptation	O
than	O
even	O
human	O
annotations	O
.	O

Semantic	NLP-focus
parsing	NLP-focus
(	NLP-focus
SP	NLP-focus
)	NLP-focus
allows	O
humans	O
to	O
leverage	O
vast	O
knowledge	O
resources	O
through	O
natural	O
interaction	O
.	O

We	O
introduce	O
such	O
a	O
dataset	O
,	O
which	O
we	O
call	O
Multilingual	O
Compositional	O
Wikidata	O
Questions	O
(	O
MCWQ	O
)	O
and	O
use	O
it	O
to	O
analyze	O
the	O
compositional	NLP-focus
generalization	NLP-focus
of	O
semantic	NLP-focus
parsers	NLP-focus
in	O
Hebrew	O
,	O
Kannada	O
,	O
Chinese	O
,	O
and	O
English	O
.	O

While	O
within	NLP-focus
-	NLP-focus
language	NLP-focus
generalization	NLP-focus
is	O
comparable	O
across	O
languages	O
,	O
experiments	O
on	O
zero	NLP-focus
-	NLP-focus
shot	NLP-focus
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
transfer	NLP-focus
demonstrate	O
that	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
compositional	NLP-focus
generalization	NLP-focus
fails	O
,	O
even	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pretrained	NLP-algorithm/tool
multilingual	NLP-algorithm/tool
encoders	NLP-algorithm/tool
.	O

While	O
within	NLP-focus
-	NLP-focus
language	NLP-focus
generalization	NLP-focus
is	O
comparable	O
across	O
languages	O
,	O
experiments	O
on	O
zero	NLP-focus
-	NLP-focus
shot	NLP-focus
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
transfer	NLP-focus
demonstrate	O
that	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
compositional	NLP-focus
generalization	NLP-focus
fails	O
,	O
even	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pretrained	NLP-algorithm/tool
multilingual	NLP-algorithm/tool
encoders	NLP-algorithm/tool
.	O

Furthermore	O
,	O
our	O
methodology	O
,	O
dataset	O
,	O
and	O
results	O
will	O
facilitate	O
future	O
research	O
on	O
SP	NLP-focus
in	O
more	O
realistic	O
and	O
diverse	O
settings	O
than	O
has	O
been	O
possible	O
with	O
existing	O
resources	O
.	O

Yet	O
the	O
properties	O
elicited	O
by	O
various	O
decoding	O
strategies	O
do	O
not	O
always	O
transfer	O
across	O
natural	NLP-focus
language	NLP-focus
generation	NLP-focus
tasks	O
.	O

For	O
example	O
,	O
while	O
mode	O
-	O
seeking	O
methods	O
like	O
beam	O
search	O
perform	O
remarkably	O
well	O
for	O
machine	NLP-focus
translation	NLP-focus
they	O
have	O
been	O
observed	O
to	O
lead	O
to	O
incoherent	O
and	O
repetitive	O
text	O
in	O
story	NLP-focus
generation	NLP-focus
.	O

This	O
work	O
—	O
in	O
contrast	O
—	O
provides	O
a	O
comprehensive	O
analysis	O
of	O
the	O
interaction	O
between	O
language	NLP-focus
generation	NLP-focus
tasks	O
and	O
decoding	O
strategies	O
.	O

For	O
example	O
,	O
the	O
nature	O
of	O
the	O
diversity	O
–	O
quality	O
trade	O
-	O
off	O
in	O
language	NLP-focus
generation	NLP-focus
is	O
very	O
task	O
-	O
specific	O
;	O
the	O
length	O
bias	O
often	O
attributed	O
to	O
beam	O
search	O
is	O
not	O
constant	O
across	O
tasks	O
.	O

Fact	NLP-algorithm/tool
verification	NLP-algorithm/tool
systems	NLP-algorithm/tool
typically	O
rely	O
on	O
neural	O
network	O
classifiers	O
for	O
veracity	NLP-focus
prediction	NLP-focus
which	O
lack	O
explainability	O
.	O

Fact	NLP-algorithm/tool
verification	NLP-algorithm/tool
systems	NLP-algorithm/tool
typically	O
rely	O
on	O
neural	O
network	O
classifiers	O
for	O
veracity	NLP-focus
prediction	NLP-focus
which	O
lack	O
explainability	O
.	O

This	O
paper	O
proposes	O
ProoFVer	O
which	O
uses	O
a	O
seq2seq	NLP-algorithm/tool
model	NLP-algorithm/tool
to	O
generate	O
natural	O
logic	O
-	O
based	O
inferences	O
as	O
proofs	O
.	O

Claim	NLP-focus
veracity	NLP-focus
is	O
determined	O
solely	O
based	O
on	O
the	O
sequence	O
of	O
these	O
operators	O
.	O

Currently	O
,	O
ProoFVer	O
has	O
the	O
highest	O
label	O
accuracy	Classification-metrics
and	O
the	O
second	O
best	O
score	O
in	O
the	O
FEVER	O
leaderboard	O
.	O

Furthermore	O
,	O
it	O
improves	O
by	O
13	Numerical-result
.	Numerical-result
21	Numerical-result
\\%	Numerical-result
points	O
over	O
the	O
next	O
best	O
model	O
on	O
a	O
dataset	O
with	O
counterfactual	O
instances	O
,	O
demonstrating	O
its	O
robustness	O
.	O

We	O
investigate	O
the	O
extent	O
to	O
which	O
modern	O
neural	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
susceptible	O
to	O
structural	NLP-focus
priming	NLP-focus
the	O
phenomenon	O
whereby	O
the	O
structure	O
of	O
a	O
sentence	O
makes	O
the	O
same	O
structure	O
more	O
probable	O
in	O
a	O
follow	O
-	O
up	O
sentence	O
.	O

We	O
investigate	O
the	O
extent	O
to	O
which	O
modern	O
neural	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
susceptible	O
to	O
structural	NLP-focus
priming	NLP-focus
the	O
phenomenon	O
whereby	O
the	O
structure	O
of	O
a	O
sentence	O
makes	O
the	O
same	O
structure	O
more	O
probable	O
in	O
a	O
follow	O
-	O
up	O
sentence	O
.	O

We	O
find	O
that	O
Transformer	O
models	O
indeed	O
show	O
evidence	O
of	O
structural	NLP-focus
priming	NLP-focus
but	O
also	O
that	O
the	O
generalizations	O
they	O
learned	O
are	O
to	O
some	O
extent	O
modulated	O
by	O
semantic	O
information	O
.	O

More	O
generally	O
,	O
our	O
study	O
shows	O
that	O
the	O
priming	O
paradigm	O
is	O
a	O
useful	O
,	O
additional	O
tool	O
for	O
gaining	O
insights	O
into	O
the	O
capacities	O
of	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
opens	O
the	O
door	O
to	O
future	O
priming	O
-	O
based	O
investigations	O
that	O
probe	O
the	O
model	O
’	O
s	O
internal	O
states	O
.	O

Popular	O
Bayesian	O
non	O
-	O
parametric	O
models	O
for	O
text	NLP-focus
segmentation	NLP-focus
(	O
Goldwater	O
et	O
al	O
.,	O
2006	O
,	O
2009	O
)	O
use	O
a	O
Dirichlet	O
process	O
to	O
jointly	O
segment	O
sentences	O
and	O
build	O
a	O
lexicon	O
of	O
word	O
types	O
.	O

On	O
the	O
Zero	O
Resource	O
Speech	O
Benchmark	O
2017	O
our	O
model	O
sets	O
a	O
new	O
speech	NLP-focus
segmentation	NLP-focus
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
5	O
languages	O
.	O

Despite	O
lacking	O
a	O
type	O
lexicon	O
,	O
DP	O
-	O
Parse	O
can	O
be	O
pipelined	O
to	O
a	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
learn	O
semantic	O
and	O
syntactic	O
representations	O
as	O
assessed	O
by	O
a	O
new	O
spoken	O
word	O
embedding	O
benchmark	O
.	O

Knowledge	O
-	O
grounded	O
dialogue	O
systems	O
powered	O
by	O
large	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
often	O
generate	O
responses	O
that	O
,	O
while	O
fluent	O
,	O
are	O
not	O
attributable	O
to	O
a	O
relevant	O
source	O
of	O
information	O
.	O

To	O
this	O
end	O
,	O
we	O
introduce	O
the	O
Benchmark	O
for	O
Evaluation	O
of	O
Grounded	O
INteraction	O
(	O
Begin	O
)	O
comprising	O
12k	O
dialogue	O
turns	O
generated	O
by	O
neural	NLP-algorithm/tool
dialogue	NLP-algorithm/tool
systems	NLP-algorithm/tool
trained	O
on	O
three	O
knowledge	O
-	O
grounded	O
dialogue	O
corpora	O
.	O

Thus	O
,	O
we	O
investigate	O
the	O
ability	O
of	O
agents	O
to	O
identify	O
non	O
-	O
cooperative	O
interlocutors	O
while	O
completing	O
a	O
concurrent	O
visual	NLP-focus
-	NLP-focus
dialogue	NLP-focus
task	NLP-focus
.	O

To	O
demonstrate	O
the	O
efficacy	O
of	O
the	O
hybrid	O
framework	O
,	O
we	O
combine	O
existing	O
ILP	O
-	O
based	O
solvers	O
for	O
multi	NLP-focus
-	NLP-focus
hop	NLP-focus
Question	NLP-focus
Answering	NLP-focus
(	NLP-focus
QA	NLP-focus
)	NLP-focus
with	O
Transformer	O
based	O
representations	O
.	O

An	O
extensive	O
empirical	O
evaluation	O
on	O
scientific	O
and	O
commonsense	NLP-focus
QA	NLP-focus
tasks	O
demonstrates	O
that	O
the	O
integration	O
of	O
explicit	O
constraints	O
in	O
a	O
end	O
-	O
to	O
-	O
end	O
differentiable	O
framework	O
can	O
significantly	O
improve	O
the	O
performance	O
of	O
non	O
-	O
differentiable	O
ILP	O
solvers	O
(	O
8	Numerical-result
.	Numerical-result
91	Numerical-result
\\%–	Numerical-result
13	Numerical-result
.	Numerical-result
3	Numerical-result
\\%	Numerical-result
.	O

An	O
extensive	O
empirical	O
evaluation	O
on	O
scientific	O
and	O
commonsense	NLP-focus
QA	NLP-focus
tasks	O
demonstrates	O
that	O
the	O
integration	O
of	O
explicit	O
constraints	O
in	O
a	O
end	O
-	O
to	O
-	O
end	O
differentiable	O
framework	O
can	O
significantly	O
improve	O
the	O
performance	O
of	O
non	O
-	O
differentiable	O
ILP	O
solvers	O
(	O
8	Numerical-result
.	Numerical-result
91	Numerical-result
\\%–	Numerical-result
13	Numerical-result
.	Numerical-result
3	Numerical-result
\\%	Numerical-result
.	O

They	O
have	O
been	O
a	O
classical	O
challenge	O
to	O
NLP	O
including	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
that	O
drive	O
today	O
’	O
s	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

In	O
this	O
work	O
,	O
we	O
take	O
a	O
first	O
-	O
principles	O
approach	O
to	O
build	O
idiomaticity	O
into	O
BART	NLP-algorithm/tool
using	O
an	O
adapter	O
as	O
a	O
lightweight	O
non	O
-	O
compositional	O
language	O
expert	O
trained	O
on	O
idiomatic	O
sentences	O
.	O

The	O
improved	O
capability	O
over	O
baselines	O
(	O
e	O
.	O
g	O
.,	O
BART	NLP-algorithm/tool
is	O
seen	O
via	O
intrinsic	O
and	O
extrinsic	O
methods	O
,	O
where	O
idiom	O
embeddings	O
score	O
0	O
.	O
19	O
points	O
higher	O
in	O
homogeneity	O
score	O
for	O
embedding	O
clustering	O
,	O
and	O
up	O
to	O
25	Numerical-result
\\%	Numerical-result
higher	O
sequence	Classification-metrics
accuracy	Classification-metrics
on	O
the	O
idiom	O
processing	O
tasks	O
of	O
IE	NLP-focus
sense	NLP-focus
disambiguation	NLP-focus
an	NLP-focus
span	NLP-focus
detection	NLP-focus
.	O

The	O
improved	O
capability	O
over	O
baselines	O
(	O
e	O
.	O
g	O
.,	O
BART	NLP-algorithm/tool
is	O
seen	O
via	O
intrinsic	O
and	O
extrinsic	O
methods	O
,	O
where	O
idiom	O
embeddings	O
score	O
0	O
.	O
19	O
points	O
higher	O
in	O
homogeneity	O
score	O
for	O
embedding	O
clustering	O
,	O
and	O
up	O
to	O
25	Numerical-result
\\%	Numerical-result
higher	O
sequence	Classification-metrics
accuracy	Classification-metrics
on	O
the	O
idiom	O
processing	O
tasks	O
of	O
IE	NLP-focus
sense	NLP-focus
disambiguation	NLP-focus
an	NLP-focus
span	NLP-focus
detection	NLP-focus
.	O

The	O
improved	O
capability	O
over	O
baselines	O
(	O
e	O
.	O
g	O
.,	O
BART	NLP-algorithm/tool
is	O
seen	O
via	O
intrinsic	O
and	O
extrinsic	O
methods	O
,	O
where	O
idiom	O
embeddings	O
score	O
0	O
.	O
19	O
points	O
higher	O
in	O
homogeneity	O
score	O
for	O
embedding	O
clustering	O
,	O
and	O
up	O
to	O
25	Numerical-result
\\%	Numerical-result
higher	O
sequence	Classification-metrics
accuracy	Classification-metrics
on	O
the	O
idiom	O
processing	O
tasks	O
of	O
IE	NLP-focus
sense	NLP-focus
disambiguation	NLP-focus
an	NLP-focus
span	NLP-focus
detection	NLP-focus
.	O

The	O
improved	O
capability	O
over	O
baselines	O
(	O
e	O
.	O
g	O
.,	O
BART	NLP-algorithm/tool
is	O
seen	O
via	O
intrinsic	O
and	O
extrinsic	O
methods	O
,	O
where	O
idiom	O
embeddings	O
score	O
0	O
.	O
19	O
points	O
higher	O
in	O
homogeneity	O
score	O
for	O
embedding	O
clustering	O
,	O
and	O
up	O
to	O
25	Numerical-result
\\%	Numerical-result
higher	O
sequence	Classification-metrics
accuracy	Classification-metrics
on	O
the	O
idiom	O
processing	O
tasks	O
of	O
IE	NLP-focus
sense	NLP-focus
disambiguation	NLP-focus
an	NLP-focus
span	NLP-focus
detection	NLP-focus
.	O

This	O
distinction	O
is	O
beginning	O
to	O
fade	O
,	O
with	O
an	O
emerging	O
area	O
of	O
interdisciplinary	O
research	O
at	O
the	O
convergence	O
of	O
causal	O
inference	O
and	O
language	NLP-focus
processing	NLP-focus
.	O

In	O
addition	O
,	O
we	O
explore	O
potential	O
uses	O
of	O
causal	O
inference	O
to	O
improve	O
the	O
robustness	O
,	O
fairness	O
,	O
and	O
interpretability	O
of	O
NLP	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

Generalizing	O
dialogue	NLP-focus
state	NLP-focus
tracking	NLP-focus
(	NLP-focus
DST	NLP-focus
)	NLP-focus
to	O
new	O
data	O
is	O
especially	O
challenging	O
due	O
to	O
the	O
strong	O
reliance	O
on	O
abundant	O
and	O
fine	O
-	O
grained	O
supervision	O
during	O
training	O
.	O

In	O
this	O
paper	O
we	O
propose	O
a	O
training	O
strategy	O
to	O
build	O
extractive	O
DST	NLP-algorithm/tool
models	NLP-algorithm/tool
without	O
the	O
need	O
for	O
fine	O
-	O
grained	O
manual	O
span	O
labels	O
.	O

We	O
combine	O
the	O
strengths	O
of	O
triple	O
copy	O
strategy	O
DST	NLP-focus
and	O
value	O
matching	O
to	O
benefit	O
from	O
complementary	O
predictions	O
without	O
violating	O
the	O
principle	O
of	O
ontology	O
independence	O
.	O

Our	O
experiments	O
demonstrate	O
that	O
an	O
extractive	NLP-algorithm/tool
DST	NLP-algorithm/tool
model	NLP-algorithm/tool
can	O
be	O
trained	O
without	O
manual	O
span	O
labels	O
.	O

Multi	NLP-focus
-	NLP-focus
task	NLP-focus
learning	NLP-focus
in	O
which	O
several	O
tasks	O
are	O
jointly	O
learned	O
by	O
a	O
single	O
model	O
,	O
allows	O
NLP	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
share	O
information	O
from	O
multiple	O
annotations	O
and	O
may	O
facilitate	O
better	O
predictions	O
when	O
the	O
tasks	O
are	O
inter	O
-	O
related	O
.	O

Multi	NLP-focus
-	NLP-focus
task	NLP-focus
learning	NLP-focus
in	O
which	O
several	O
tasks	O
are	O
jointly	O
learned	O
by	O
a	O
single	O
model	O
,	O
allows	O
NLP	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
share	O
information	O
from	O
multiple	O
annotations	O
and	O
may	O
facilitate	O
better	O
predictions	O
when	O
the	O
tasks	O
are	O
inter	O
-	O
related	O
.	O

Active	O
learning	O
(	O
AL	O
)	O
has	O
been	O
demonstrated	O
to	O
optimize	O
annotation	O
processes	O
by	O
iteratively	O
selecting	O
unlabeled	O
examples	O
whose	O
annotation	O
is	O
most	O
valuable	O
for	O
the	O
NLP	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Yet	O
,	O
multi	O
-	O
task	O
active	O
learning	O
(	O
MT	O
-	O
AL	O
)	O
has	O
not	O
been	O
applied	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
Transformer	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
NLP	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

Our	O
results	O
suggest	O
that	O
MT	O
-	O
AL	O
can	O
be	O
effectively	O
used	O
in	O
order	O
to	O
minimize	O
annotation	O
efforts	O
for	O
multi	NLP-algorithm/tool
-	NLP-algorithm/tool
task	NLP-algorithm/tool
NLP	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

We	O
introduce	O
the	O
task	O
of	O
microblog	NLP-focus
opinion	NLP-focus
summarization	NLP-focus
(	NLP-focus
MOS	NLP-focus
)	NLP-focus
and	O
share	O
a	O
dataset	O
of	O
3100	O
gold	O
-	O
standard	O
opinion	O
summaries	O
to	O
facilitate	O
research	O
in	O
this	O
domain	O
.	O

Our	O
method	O
differs	O
from	O
previous	O
work	O
on	O
generating	O
gold	O
-	O
standard	O
summaries	O
from	O
social	O
media	O
,	O
which	O
usually	O
involves	O
selecting	O
representative	O
posts	O
and	O
thus	O
favors	O
extractive	NLP-algorithm/tool
summarization	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

To	O
showcase	O
the	O
dataset	O
’	O
s	O
utility	O
and	O
challenges	O
,	O
we	O
benchmark	O
a	O
range	O
of	O
abstractive	O
and	O
extractive	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
summarization	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
achieve	O
good	O
performance	O
,	O
with	O
the	O
former	O
outperforming	O
the	O
latter	O
.	O

Large	O
pretrained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
PLMs	NLP-algorithm/tool
)	NLP-algorithm/tool
are	O
often	O
domain	O
-	O
or	O
task	O
-	O
adapted	O
via	O
finetuning	O
or	O
prompting	O
.	O

Instead	O
,	O
we	O
prepare	O
PLMs	NLP-algorithm/tool
for	O
data	O
-	O
and	O
parameter	O
-	O
efficient	O
adaptation	O
PLMs	NLP-algorithm/tool
arning	O
to	O
learn	O
the	O
difference	O
between	O
general	O
and	O
adapted	O
PLMs	O
.	O

Experiments	O
on	O
few	NLP-focus
-	NLP-focus
shot	NLP-focus
dialogue	NLP-focus
completion	NLP-focus
low	NLP-focus
-	NLP-focus
resource	NLP-focus
abstractive	NLP-focus
summarization	NLP-focus
and	O
multi	NLP-focus
-	NLP-focus
domain	NLP-focus
language	NLP-focus
modeling	NLP-focus
show	O
improvements	O
in	O
adaptation	O
time	O
and	O
performance	O
over	O
direct	O
finetuning	O
or	O
preparation	O
via	O
domain	O
-	O
adaptive	O
pretraining	O
.	O

Natural	NLP-focus
Language	NLP-focus
Inference	NLP-focus
(	NLP-focus
NLI	NLP-focus
)	NLP-focus
and	O
Semantic	NLP-focus
Textual	NLP-focus
Similarity	NLP-focus
(	NLP-focus
STS	NLP-focus
)	NLP-focus
are	O
widely	O
used	O
benchmark	O
tasks	O
for	O
compositional	O
evaluation	O
of	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

Natural	NLP-focus
Language	NLP-focus
Inference	NLP-focus
(	NLP-focus
NLI	NLP-focus
)	NLP-focus
and	O
Semantic	NLP-focus
Textual	NLP-focus
Similarity	NLP-focus
(	NLP-focus
STS	NLP-focus
)	NLP-focus
are	O
widely	O
used	O
benchmark	O
tasks	O
for	O
compositional	O
evaluation	O
of	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

Despite	O
growing	O
interest	O
in	O
linguistic	O
universals	O
,	O
most	O
NLI	NLP-focus
STS	NLP-focus
studies	O
have	O
focused	O
almost	O
exclusively	O
on	O
English	O
.	O

In	O
particular	O
,	O
there	O
are	O
no	O
available	O
multilingual	O
NLI	NLP-focus
STS	NLP-focus
datasets	O
in	O
Japanese	O
,	O
which	O
is	O
typologically	O
different	O
from	O
English	O
and	O
can	O
shed	O
light	O
on	O
the	O
currently	O
controversial	O
behavior	O
of	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
in	O
matters	O
such	O
as	O
sensitivity	O
to	O
word	O
order	O
and	O
case	O
particles	O
.	O

In	O
particular	O
,	O
there	O
are	O
no	O
available	O
multilingual	O
NLI	NLP-focus
STS	NLP-focus
datasets	O
in	O
Japanese	O
,	O
which	O
is	O
typologically	O
different	O
from	O
English	O
and	O
can	O
shed	O
light	O
on	O
the	O
currently	O
controversial	O
behavior	O
of	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
in	O
matters	O
such	O
as	O
sensitivity	O
to	O
word	O
order	O
and	O
case	O
particles	O
.	O

Against	O
this	O
background	O
,	O
we	O
introduce	O
JSICK	O
a	O
Japanese	O
NLI	NLP-focus
STS	NLP-focus
dataset	O
that	O
was	O
manually	O
translated	O
from	O
the	O
English	O
dataset	O
SICK	O
.	O

We	O
also	O
present	O
a	O
stress	O
-	O
test	O
dataset	O
for	O
compositional	O
inference	O
,	O
created	O
by	O
transforming	O
syntactic	O
structures	O
of	O
sentences	O
in	O
JSICK	O
to	O
investigate	O
whether	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
sensitive	O
to	O
word	O
order	O
and	O
case	O
particles	O
.	O

We	O
conduct	O
baseline	O
experiments	O
on	O
different	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
compare	O
the	O
performance	O
of	O
multilingual	NLP-algorithm/tool
models	NLP-algorithm/tool
when	O
applied	O
to	O
Japanese	O
and	O
other	O
languages	O
.	O

The	O
results	O
of	O
the	O
stress	O
-	O
test	O
experiments	O
suggest	O
that	O
the	O
current	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
insensitive	O
to	O
word	O
order	O
and	O
case	O
marking	O
.	O

Cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
summarization	NLP-focus
is	O
the	O
task	O
of	O
generating	O
a	O
summary	O
in	O
one	O
language	O
(	O
e	O
.	O
g	O
.,	O
English	O
)	O
for	O
the	O
given	O
document	O
(	O
s	O
)	O
in	O
a	O
different	O
language	O
(	O
e	O
.	O
g	O
.,	O
Chinese	O
).	O

This	O
survey	O
is	O
for	O
both	O
beginners	O
and	O
experts	O
in	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
summarization	NLP-focus
and	O
we	O
hope	O
it	O
will	O
serve	O
as	O
a	O
starting	O
point	O
as	O
well	O
as	O
a	O
source	O
of	O
new	O
ideas	O
for	O
researchers	O
and	O
engineers	O
interested	O
in	O
this	O
area	O
.	O

Self	O
-	O
supervised	O
learning	O
(	O
SSL	O
)	O
methods	O
such	O
as	O
Word2vec	NLP-algorithm/tool
BERT	NLP-algorithm/tool
and	O
GPT	NLP-algorithm/tool
have	O
shown	O
great	O
effectiveness	O
in	O
language	NLP-algorithm/tool
understanding	NLP-algorithm/tool
.	O

Previous	O
contrastive	O
learning	O
methods	O
perform	O
data	NLP-algorithm/tool
augmentation	NLP-algorithm/tool
contrastive	O
learning	O
ing	O
separately	O
.	O

To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
a	O
four	O
-	O
level	O
optimization	O
framework	O
that	O
performs	O
data	NLP-algorithm/tool
augmentation	NLP-algorithm/tool
and	O
contrastive	O
learning	O
end	O
-	O
to	O
-	O
end	O
,	O
to	O
enable	O
the	O
augmented	O
data	O
contrastive	O
learning	O
contrastive	O
learning	O
task	O
.	O

This	O
framework	O
consists	O
of	O
four	O
learning	O
stages	O
,	O
including	O
training	O
machine	NLP-algorithm/tool
translation	NLP-algorithm/tool
models	NLP-algorithm/tool
for	O
sentence	NLP-focus
augmentation	NLP-focus
pretraining	O
a	O
text	NLP-algorithm/tool
encoder	NLP-algorithm/tool
using	O
contrastive	O
learning	O
finetuning	O
a	O
text	NLP-algorithm/tool
classification	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
updating	O
weights	O
of	O
translation	O
data	O
by	O
minimizing	O
the	O
validation	O
loss	O
classification	O
model	O
model	O
,	O
which	O
are	O
performed	O
in	O
a	O
unified	O
way	O
.	O

This	O
framework	O
consists	O
of	O
four	O
learning	O
stages	O
,	O
including	O
training	O
machine	NLP-algorithm/tool
translation	NLP-algorithm/tool
models	NLP-algorithm/tool
for	O
sentence	NLP-focus
augmentation	NLP-focus
pretraining	O
a	O
text	NLP-algorithm/tool
encoder	NLP-algorithm/tool
using	O
contrastive	O
learning	O
finetuning	O
a	O
text	NLP-algorithm/tool
classification	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
updating	O
weights	O
of	O
translation	O
data	O
by	O
minimizing	O
the	O
validation	O
loss	O
classification	O
model	O
model	O
,	O
which	O
are	O
performed	O
in	O
a	O
unified	O
way	O
.	O

Abstraction	NLP-focus
is	O
a	O
core	O
tenet	O
of	O
human	O
cognition	O
and	O
communication	O
.	O

Yet	O
,	O
interpreting	O
and	O
grounding	O
abstraction	NLP-focus
expressed	O
in	O
NL	O
has	O
not	O
yet	O
been	O
systematically	O
studied	O
in	O
NLP	O
with	O
no	O
accepted	O
benchmarks	O
specifically	O
eliciting	O
abstraction	O
in	O
NL	O
.	O

In	O
this	O
work	O
,	O
we	O
set	O
the	O
foundation	O
for	O
a	O
systematic	O
study	O
of	O
processing	O
and	O
grounding	O
abstraction	NLP-focus
in	O
NLP	O
.	O

First	O
,	O
we	O
deliver	O
a	O
novel	O
abstraction	NLP-focus
elicitation	NLP-focus
method	O
and	O
present	O
Hexagons	O
a	O
2D	O
instruction	O
-	O
following	O
game	O
.	O

Our	O
results	O
show	O
that	O
contemporary	O
models	O
and	O
modeling	O
practices	O
are	O
substantially	O
inferior	O
to	O
human	O
performance	O
,	O
and	O
that	O
model	O
performance	O
is	O
inversely	O
correlated	O
with	O
the	O
level	O
of	O
abstraction	NLP-focus
abstraction	NLP-focus
s	O
satisfying	O
performance	O
on	O
higher	O
levels	O
of	O
abstraction	O
.	O

These	O
findings	O
are	O
consistent	O
across	O
models	O
and	O
setups	O
,	O
confirming	O
that	O
abstraction	O
is	O
a	O
challenging	O
phenomenon	O
deserving	O
further	O
attention	O
and	O
study	O
in	O
NLP	O
AI	AI/ML/DL-domain
research	O
.	O

We	O
investigate	O
how	O
disagreement	O
in	O
natural	NLP-focus
language	NLP-focus
inference	NLP-focus
(	NLP-focus
NLI	NLP-focus
)	NLP-focus
annotation	O
arises	O
.	O

We	O
explore	O
two	O
modeling	O
approaches	O
for	O
detecting	O
items	O
with	O
potential	O
disagreement	O
:	O
a	O
4	NLP-algorithm/tool
-	NLP-algorithm/tool
way	NLP-algorithm/tool
classification	NLP-algorithm/tool
with	O
a	O
“	O
Complicated	O
”	O
label	O
in	O
addition	O
to	O
the	O
three	O
standard	O
NLI	O
labels	O
and	O
a	O
multilabel	O
classification	O
approach	O
.	O

We	O
also	O
measure	O
transitivity	NLP-metrics
which	O
quantifies	O
the	O
importance	O
of	O
word	O
order	O
.	O

Our	O
results	O
indicate	O
that	O
the	O
awareness	O
of	O
object	O
structure	O
yields	O
a	O
more	O
natural	NLP-focus
sentence	NLP-focus
organization	NLP-focus
.	O

Despite	O
extensive	O
research	O
efforts	O
in	O
recent	O
years	O
,	O
computational	NLP-focus
argumentation	NLP-focus
(	NLP-focus
CA	NLP-focus
)	NLP-focus
remains	O
one	O
of	O
the	O
most	O
challenging	O
areas	O
of	O
natural	O
language	O
processing	O
.	O

The	O
integration	O
of	O
knowledge	O
from	O
such	O
a	O
wide	O
range	O
in	O
CA	NLP-focus
requires	O
modeling	O
capabilities	O
far	O
beyond	O
many	O
other	O
natural	O
language	O
understanding	O
tasks	O
.	O

However	O
,	O
a	O
systematic	O
overview	O
of	O
the	O
types	O
of	O
knowledge	O
introduced	O
in	O
existing	O
CA	NLP-focus
models	O
is	O
missing	O
,	O
hindering	O
targeted	O
progress	O
in	O
the	O
field	O
.	O

Adopting	O
the	O
operational	O
definition	O
of	O
knowledge	O
as	O
any	O
task	O
-	O
relevant	O
normative	O
information	O
not	O
provided	O
as	O
input	O
,	O
the	O
survey	O
paper	O
at	O
hand	O
fills	O
this	O
gap	O
by	O
(	O
1	O
)	O
proposing	O
a	O
taxonomy	O
of	O
types	O
of	O
knowledge	O
required	O
in	O
CA	NLP-focus
CA	NLP-focus
CA	NLP-focus
CA	NLP-focus
)	O
systematizing	O
the	O
large	O
body	O
of	O
CA	O
work	O
according	O
to	O
the	O
reliance	O
on	O
and	O
exploitation	O
of	O
these	O
knowledge	O
types	O
for	O
the	O
four	O
main	O
research	O
areas	O
in	O
CA	O
,	O
and	O
(	O
3	O
)	O
outlining	O
and	O
discussing	O
directions	O
for	O
future	O
research	O
efforts	O
in	O
CA	O
.	O

We	O
introduce	O
Transformer	O
Grammars	O
(	O
TGs	O
)	O
a	O
novel	O
class	O
of	O
Transformer	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
that	O
combine	O
(	O
i	O
)	O
the	O
expressive	O
power	O
,	O
scalability	O
,	O
and	O
strong	O
performance	O
of	O
Transformers	O
and	O
(	O
ii	O
)	O
recursive	O
syntactic	O
compositions	O
,	O
which	O
here	O
are	O
implemented	O
through	O
a	O
special	O
attention	O
mask	O
and	O
deterministic	O
transformation	O
of	O
the	O
linearized	O
tree	O
.	O

We	O
find	O
that	O
TGs	O
outperform	O
various	O
strong	O
baselines	O
on	O
sentence	NLP-focus
-	NLP-focus
level	NLP-focus
language	NLP-focus
modeling	NLP-focus
perplexity	NLP-metrics
as	O
well	O
as	O
on	O
multiple	O
syntax	O
-	O
sensitive	O
language	O
modeling	O
evaluation	O
metrics	O
.	O

We	O
find	O
that	O
TGs	O
outperform	O
various	O
strong	O
baselines	O
on	O
sentence	NLP-focus
-	NLP-focus
level	NLP-focus
language	NLP-focus
modeling	NLP-focus
perplexity	NLP-metrics
as	O
well	O
as	O
on	O
multiple	O
syntax	O
-	O
sensitive	O
language	O
modeling	O
evaluation	O
metrics	O
.	O

Additionally	O
,	O
we	O
find	O
that	O
the	O
recursive	O
syntactic	O
composition	O
bottleneck	O
which	O
represents	O
each	O
sentence	O
as	O
a	O
single	O
vector	O
harms	O
perplexity	O
on	O
document	NLP-focus
-	NLP-focus
level	NLP-focus
language	NLP-focus
modeling	NLP-focus
providing	O
evidence	O
that	O
a	O
different	O
kind	O
of	O
memory	O
mechanism	O
—	O
one	O
that	O
is	O
independent	O
of	O
composed	O
syntactic	O
representations	O
plays	O
an	O
important	O
role	O
in	O
current	O
successful	O
models	O
of	O
long	O
text	O
.	O

In	O
this	O
work	O
we	O
introduce	O
the	O
concept	O
of	O
policy	NLP-focus
-	NLP-focus
aware	NLP-focus
abuse	NLP-focus
detection	NLP-focus
abandoning	O
the	O
unrealistic	O
expectation	O
that	O
systems	O
can	O
reliably	O
learn	O
which	O
phenomena	O
constitute	O
abuse	O
from	O
inspecting	O
the	O
data	O
alone	O
.	O

We	O
collect	O
and	O
annotate	O
a	O
dataset	O
of	O
3	O
,	O
535	O
English	O
posts	O
with	O
such	O
slots	O
,	O
and	O
show	O
how	O
architectures	O
for	O
intent	NLP-focus
classification	NLP-focus
and	O
slot	NLP-focus
filling	NLP-focus
can	O
be	O
used	O
for	O
abuse	O
detection	O
,	O
while	O
providing	O
a	O
rationale	O
for	O
model	O
decisions	O
.	O
1	O
.	O

Morphological	NLP-focus
tasks	NLP-focus
use	O
large	O
multi	O
-	O
lingual	O
datasets	O
that	O
organize	O
words	O
into	O
inflection	O
tables	O
,	O
which	O
then	O
serve	O
as	O
training	O
and	O
evaluation	O
data	O
for	O
various	O
tasks	O
.	O

To	O
overcome	O
this	O
deficiency	O
,	O
we	O
propose	O
to	O
view	O
morphology	NLP-focus
as	O
a	O
clause	O
-	O
level	O
phenomenon	O
,	O
rather	O
than	O
word	O
-	O
level	O
.	O

We	O
use	O
this	O
dataset	O
to	O
derive	O
3	O
clause	NLP-focus
-	NLP-focus
level	NLP-focus
morphological	NLP-focus
tasks	NLP-focus
inflection	NLP-focus
reinflection	NLP-focus
and	O
analysis	O
.	O

Furthermore	O
,	O
redefining	O
morphology	O
to	O
the	O
clause	O
-	O
level	O
provides	O
a	O
neat	O
interface	O
with	O
contextualized	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
and	O
allows	O
assessing	O
the	O
morphological	O
knowledge	O
encoded	O
in	O
these	O
models	O
and	O
their	O
usability	O
for	O
morphological	O
tasks	O
.	O

Taken	O
together	O
,	O
this	O
work	O
opens	O
up	O
new	O
horizons	O
in	O
the	O
study	O
of	O
computational	O
morphology	O
,	O
leaving	O
ample	O
space	O
for	O
studying	O
neural	NLP-focus
morphology	NLP-focus
cross	O
-	O
linguistically	O
.	O

We	O
show	O
that	O
FaithDial	O
can	O
serve	O
as	O
training	O
signal	O
for	O
:	O
i	O
)	O
a	O
hallucination	O
critic	O
,	O
which	O
discriminates	O
whether	O
an	O
utterance	O
is	O
faithful	O
or	O
not	O
,	O
and	O
boosts	O
the	O
performance	O
by	O
12	Numerical-result
.	Numerical-result
8	Numerical-result
F1	Classification-metrics
score	O
on	O
the	O
BEGIN	O
benchmark	O
compared	O
to	O
existing	O
datasets	O
for	O
dialogue	O
coherence	O
;	O
ii	O
)	O
high	O
-	O
quality	O
dialogue	O
generation	O
.	O

We	O
show	O
that	O
FaithDial	O
can	O
serve	O
as	O
training	O
signal	O
for	O
:	O
i	O
)	O
a	O
hallucination	O
critic	O
,	O
which	O
discriminates	O
whether	O
an	O
utterance	O
is	O
faithful	O
or	O
not	O
,	O
and	O
boosts	O
the	O
performance	O
by	O
12	Numerical-result
.	Numerical-result
8	Numerical-result
F1	Classification-metrics
score	O
on	O
the	O
BEGIN	O
benchmark	O
compared	O
to	O
existing	O
datasets	O
for	O
dialogue	O
coherence	O
;	O
ii	O
)	O
high	O
-	O
quality	O
dialogue	O
generation	O
.	O

This	O
is	O
now	O
an	O
essential	O
tool	O
for	O
building	O
low	NLP-algorithm/tool
-	NLP-algorithm/tool
resource	NLP-algorithm/tool
syntactic	NLP-algorithm/tool
analyzers	NLP-algorithm/tool
such	O
as	O
part	NLP-algorithm/tool
-	NLP-algorithm/tool
of	NLP-algorithm/tool
-	NLP-algorithm/tool
speech	NLP-algorithm/tool
(	NLP-algorithm/tool
POS	NLP-algorithm/tool
)	NLP-algorithm/tool
taggers	NLP-algorithm/tool
.	O

When	O
building	O
machine	NLP-focus
translation	NLP-focus
systems	O
,	O
one	O
often	O
needs	O
to	O
make	O
the	O
best	O
out	O
of	O
heterogeneous	O
sets	O
of	O
parallel	O
data	O
in	O
training	O
and	O
to	O
robustly	O
handle	O
inputs	O
from	O
unexpected	O
domains	O
in	O
testing	O
.	O

In	O
this	O
study	O
,	O
we	O
revisit	O
multi	NLP-focus
-	NLP-focus
domain	NLP-focus
machine	NLP-focus
translation	NLP-focus
with	O
the	O
aim	O
to	O
formulate	O
the	O
motivations	O
for	O
developing	O
such	O
systems	O
and	O
the	O
associated	O
expectations	O
with	O
respect	O
to	O
performance	O
.	O

Task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialogue	NLP-focus
systems	O
typically	O
rely	O
on	O
large	O
amounts	O
of	O
high	O
-	O
quality	O
training	O
data	O
or	O
require	O
complex	O
handcrafted	O
rules	O
.	O

We	O
propose	O
the	O
Conversation	O
Graph	O
(	O
ConvGraph	O
)	O
a	O
graph	O
-	O
based	O
representation	O
of	O
dialogues	O
that	O
can	O
be	O
exploited	O
for	O
data	NLP-focus
augmentation	NLP-focus
multi	O
-	O
reference	O
training	O
and	O
evaluation	O
of	O
non	O
-	O
deterministic	O
agents	O
.	O

Intrinsic	O
and	O
extrinsic	O
evaluation	O
across	O
three	O
datasets	O
shows	O
that	O
data	NLP-focus
augmentation	NLP-focus
and	O
/	O
or	O
multi	O
-	O
reference	O
training	O
with	O
ConvGraph	O
can	O
improve	O
dialogue	O
success	O
rates	O
by	O
up	O
to	O
6	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
.	O

Intrinsic	O
and	O
extrinsic	O
evaluation	O
across	O
three	O
datasets	O
shows	O
that	O
data	NLP-focus
augmentation	NLP-focus
and	O
/	O
or	O
multi	O
-	O
reference	O
training	O
with	O
ConvGraph	O
can	O
improve	O
dialogue	O
success	O
rates	O
by	O
up	O
to	O
6	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
.	O

Additionally	O
,	O
we	O
set	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
newly	O
released	O
PG	O
-	O
19	O
data	O
-	O
set	O
,	O
obtaining	O
a	O
test	O
perplexity	NLP-metrics
of	O
33	Numerical-result
.	Numerical-result
2	Numerical-result
with	O
a	O
22	O
layer	O
Routing	O
Transformer	O
model	O
trained	O
on	O
sequences	O
of	O
length	O
8192	O
.	O

Additionally	O
,	O
we	O
set	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
newly	O
released	O
PG	O
-	O
19	O
data	O
-	O
set	O
,	O
obtaining	O
a	O
test	O
perplexity	NLP-metrics
of	O
33	Numerical-result
.	Numerical-result
2	Numerical-result
with	O
a	O
22	O
layer	O
Routing	O
Transformer	O
model	O
trained	O
on	O
sequences	O
of	O
length	O
8192	O
.	O

We	O
propose	O
a	O
decipherment	NLP-algorithm/tool
model	NLP-algorithm/tool
that	O
handles	O
both	O
of	O
these	O
challenges	O
by	O
building	O
on	O
rich	O
linguistic	O
constraints	O
reflecting	O
consistent	O
patterns	O
in	O
historical	O
sound	O
change	O
.	O

The	O
resulting	O
generative	O
framework	O
jointly	O
models	O
word	NLP-focus
segmentation	NLP-focus
and	O
cognate	NLP-focus
alignment	NLP-focus
informed	O
by	O
phonological	O
constraints	O
.	O

We	O
propose	O
the	O
Recursive	O
Non	O
-	O
autoregressive	O
Graph	O
-	O
to	O
-	O
Graph	O
Transformer	O
architecture	O
(	O
RNGTr	O
)	O
for	O
the	O
iterative	O
refinement	O
of	O
arbitrary	O
graphs	O
through	O
the	O
recursive	O
application	O
of	O
a	O
non	O
-	O
autoregressive	O
Graph	O
-	O
to	O
-	O
Graph	O
Transformer	O
and	O
apply	O
it	O
to	O
syntactic	NLP-focus
dependency	NLP-focus
parsing	NLP-focus
.	O

We	O
demonstrate	O
the	O
power	O
and	O
effectiveness	O
of	O
RNGTr	O
on	O
several	O
dependency	O
corpora	O
,	O
using	O
a	O
refinement	O
model	O
pre	O
-	O
trained	O
with	O
BERT	NLP-algorithm/tool
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
DRaiL	O
an	O
open	O
-	O
source	O
declarative	O
framework	O
for	O
specifying	O
deep	NLP-algorithm/tool
relational	NLP-algorithm/tool
models	NLP-algorithm/tool
designed	O
to	O
support	O
a	O
variety	O
of	O
NLP	O
scenarios	O
.	O

We	O
use	O
large	O
-	O
scale	O
corpora	O
in	O
six	O
different	O
gendered	O
languages	O
,	O
along	O
with	O
tools	O
from	O
NLP	O
and	O
information	NLP-focus
theory	NLP-focus
to	O
test	O
whether	O
there	O
is	O
a	O
relationship	O
between	O
the	O
grammatical	O
genders	O
of	O
inanimate	O
nouns	O
and	O
the	O
adjectives	O
used	O
to	O
describe	O
those	O
nouns	O
.	O

Equipped	O
with	O
this	O
new	O
analysis	O
tool	O
,	O
we	O
can	O
ask	O
questions	O
that	O
were	O
not	O
possible	O
before	O
,	O
for	O
example	O
,	O
is	O
part	O
-	O
of	O
-	O
speech	O
information	O
important	O
for	O
word	NLP-focus
prediction	NLP-focus
We	O
perform	O
a	O
series	O
of	O
analyses	O
on	O
BERT	NLP-algorithm/tool
to	O
answer	O
these	O
types	O
of	O
questions	O
.	O

Equipped	O
with	O
this	O
new	O
analysis	O
tool	O
,	O
we	O
can	O
ask	O
questions	O
that	O
were	O
not	O
possible	O
before	O
,	O
for	O
example	O
,	O
is	O
part	O
-	O
of	O
-	O
speech	O
information	O
important	O
for	O
word	NLP-focus
prediction	NLP-focus
We	O
perform	O
a	O
series	O
of	O
analyses	O
on	O
BERT	NLP-algorithm/tool
to	O
answer	O
these	O
types	O
of	O
questions	O
.	O

Pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
representation	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
PLMs	NLP-algorithm/tool
)	NLP-algorithm/tool
cannot	O
well	O
capture	O
factual	O
knowledge	O
from	O
text	O
.	O

In	O
contrast	O
,	O
knowledge	NLP-algorithm/tool
embedding	NLP-algorithm/tool
(	NLP-algorithm/tool
KE	NLP-algorithm/tool
)	NLP-algorithm/tool
methods	NLP-algorithm/tool
can	O
effectively	O
represent	O
the	O
relational	O
facts	O
in	O
knowledge	O
graphs	O
(	O
KGs	O
)	O
with	O
informative	O
entity	O
embeddings	O
but	O
conventional	O
KE	NLP-algorithm/tool
models	NLP-algorithm/tool
cannot	O
take	O
full	O
advantage	O
of	O
the	O
abundant	O
textual	O
information	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
unified	O
model	O
for	O
Knowledge	O
Embedding	O
and	O
Pre	O
-	O
trained	O
LanguagERepresentation	O
(	O
KEPLER	O
)	O
which	O
can	O
not	O
only	O
better	O
integrate	O
factual	O
knowledge	O
into	O
PLMs	NLP-algorithm/tool
PLMs	NLP-algorithm/tool
lso	O
produce	O
effective	O
text	O
-	O
enhanced	O
KE	O
with	O
the	O
strong	O
PLMs	O
.	O

In	O
KEPLER	O
we	O
encode	O
textual	O
entity	O
descriptions	O
with	O
a	O
PLM	NLP-algorithm/tool
as	O
their	O
embeddings	O
KE	O
d	O
then	O
jointly	O
optimize	O
the	O
KE	O
and	O
language	O
modeling	O
objectives	O
.	O

Experimental	O
results	O
show	O
that	O
KEPLER	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
on	O
various	O
NLP	O
tasks	O
,	O
and	O
also	O
works	O
remarkably	O
well	O
as	O
an	O
inductive	NLP-algorithm/tool
KE	NLP-algorithm/tool
model	NLP-algorithm/tool
on	O
KG	NLP-focus
link	NLP-focus
prediction	NLP-focus
.	O

Experimental	O
results	O
show	O
that	O
KEPLER	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
on	O
various	O
NLP	O
tasks	O
,	O
and	O
also	O
works	O
remarkably	O
well	O
as	O
an	O
inductive	NLP-algorithm/tool
KE	NLP-algorithm/tool
model	NLP-algorithm/tool
on	O
KG	NLP-focus
link	NLP-focus
prediction	NLP-focus
.	O

Answering	O
questions	O
that	O
involve	O
multi	NLP-focus
-	NLP-focus
step	NLP-focus
reasoning	NLP-focus
requires	O
decomposing	O
them	O
and	O
using	O
the	O
answers	O
of	O
intermediate	O
steps	O
to	O
reach	O
the	O
final	O
answer	O
.	O

However	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
in	O
grounded	O
question	NLP-focus
answering	NLP-focus
often	O
do	O
not	O
explicitly	O
perform	O
decomposition	O
,	O
leading	O
to	O
difficulties	O
in	O
generalization	O
to	O
out	O
-	O
of	O
-	O
distribution	O
examples	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
model	O
that	O
computes	O
a	O
representation	O
and	O
denotation	O
for	O
all	O
question	O
spans	O
in	O
a	O
bottom	O
-	O
up	O
,	O
compositional	O
manner	O
using	O
a	O
CKY	NLP-algorithm/tool
-	NLP-algorithm/tool
style	NLP-algorithm/tool
parser	NLP-algorithm/tool
.	O

Our	O
model	O
induces	O
latent	NLP-algorithm/tool
trees	NLP-algorithm/tool
driven	O
by	O
end	O
-	O
to	O
-	O
end	O
(	O
the	O
answer	O
)	O
supervision	O
only	O
.	O

We	O
show	O
that	O
this	O
inductive	O
bias	O
towards	O
tree	O
structures	O
dramatically	O
improves	O
systematic	O
generalization	O
to	O
out	O
-	O
of	O
-	O
distribution	O
examples	O
,	O
compared	O
to	O
strong	O
baselines	O
on	O
an	O
arithmetic	O
expressions	O
benchmark	O
as	O
well	O
as	O
on	O
C	O
losure	O
a	O
dataset	O
that	O
focuses	O
on	O
systematic	O
generalization	O
for	O
grounded	O
question	NLP-focus
answering	NLP-focus
.	O

On	O
this	O
challenging	O
dataset	O
,	O
our	O
model	O
reaches	O
an	O
accuracy	Classification-metrics
of	O
96	Numerical-result
.	Numerical-result
1	Numerical-result
\\%	Numerical-result
significantly	O
higher	O
than	O
prior	O
models	O
that	O
almost	O
perfectly	O
solve	O
the	O
task	O
on	O
a	O
random	O
,	O
in	O
-	O
distribution	O
split	O
.	O

On	O
this	O
challenging	O
dataset	O
,	O
our	O
model	O
reaches	O
an	O
accuracy	Classification-metrics
of	O
96	Numerical-result
.	Numerical-result
1	Numerical-result
\\%	Numerical-result
significantly	O
higher	O
than	O
prior	O
models	O
that	O
almost	O
perfectly	O
solve	O
the	O
task	O
on	O
a	O
random	O
,	O
in	O
-	O
distribution	O
split	O
.	O

Aspect	NLP-focus
-	NLP-focus
based	NLP-focus
summarization	NLP-focus
is	O
the	O
task	O
of	O
generating	O
focused	O
summaries	O
based	O
on	O
specific	O
points	O
of	O
interest	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
WikiAsp	O
1	O
a	O
large	O
-	O
scale	O
dataset	O
for	O
multi	NLP-focus
-	NLP-focus
domain	NLP-focus
aspect	NLP-focus
-	NLP-focus
based	NLP-focus
summarization	NLP-focus
that	O
attempts	O
to	O
spur	O
research	O
in	O
the	O
direction	O
of	O
open	NLP-focus
-	NLP-focus
domain	NLP-focus
aspect	NLP-focus
-	NLP-focus
based	NLP-focus
summarization	NLP-focus
.	O

Specifically	O
,	O
we	O
build	O
the	O
dataset	O
using	O
Wikipedia	O
articles	O
from	O
20	O
different	O
domains	O
,	O
using	O
the	O
section	O
titles	O
and	O
boundaries	O
of	O
each	O
article	O
as	O
a	O
proxy	O
for	O
aspect	NLP-focus
annotation	NLP-focus
.	O

Results	O
highlight	O
key	O
challenges	O
that	O
existing	O
summarization	NLP-algorithm/tool
models	NLP-algorithm/tool
face	O
in	O
this	O
setting	O
,	O
such	O
as	O
proper	O
pronoun	O
handling	O
of	O
quoted	O
sources	O
and	O
consistent	O
explanation	O
of	O
time	O
-	O
sensitive	O
events	O
.	O

For	O
natural	O
language	O
processing	O
systems	O
,	O
two	O
kinds	O
of	O
evidence	O
support	O
the	O
use	O
of	O
text	O
representations	O
from	O
neural	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
“	O
pretrained	O
”	O
on	O
large	O
unannotated	O
corpora	O
:	O
performance	O
on	O
application	O
-	O
inspired	O
benchmarks	O
(	O
Peters	O
et	O
al	O
.,	O
2018	O
,	O
inter	O
alia	O
),	O
and	O
the	O
emergence	O
of	O
syntactic	O
abstractions	O
in	O
those	O
representations	O
(	O
Tenney	O
et	O
al	O
.,	O
2019	O
,	O
inter	O
alia	O
).	O

We	O
apply	O
novel	O
probes	O
to	O
recent	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
specifically	O
focusing	O
on	O
predicate	O
-	O
argument	O
structure	O
as	O
operationalized	O
by	O
semantic	O
dependencies	O
(	O
Ivanova	O
et	O
al	O
.,	O
2012	O
)—	O
and	O
find	O
that	O
,	O
unlike	O
syntax	O
,	O
semantics	O
is	O
not	O
brought	O
to	O
the	O
surface	O
.	O

Although	O
current	O
CCG	NLP-algorithm/tool
supertaggers	NLP-algorithm/tool
achieve	O
high	O
accuracy	O
on	O
the	O
standard	O
WSJ	O
test	O
set	O
,	O
few	O
systems	O
make	O
use	O
of	O
the	O
categories	O
’	O
internal	O
structure	O
that	O
will	O
drive	O
the	O
syntactic	O
derivation	O
during	O
parsing	O
.	O

Our	O
best	O
tagger	O
is	O
capable	O
of	O
recovering	O
a	O
sizeable	O
fraction	O
of	O
the	O
long	O
-	O
tail	O
supertags	O
and	O
even	O
generates	O
CCG	NLP-focus
categories	O
that	O
have	O
never	O
been	O
seen	O
in	O
training	O
while	O
approximating	O
the	O
prior	O
state	O
of	O
the	O
art	O
in	O
overall	O
tag	O
accuracy	Classification-metrics
with	O
fewer	O
parameters	O
.	O

Our	O
best	O
tagger	O
is	O
capable	O
of	O
recovering	O
a	O
sizeable	O
fraction	O
of	O
the	O
long	O
-	O
tail	O
supertags	O
and	O
even	O
generates	O
CCG	NLP-focus
categories	O
that	O
have	O
never	O
been	O
seen	O
in	O
training	O
while	O
approximating	O
the	O
prior	O
state	O
of	O
the	O
art	O
in	O
overall	O
tag	O
accuracy	Classification-metrics
with	O
fewer	O
parameters	O
.	O

Prior	O
studies	O
in	O
multilingual	NLP-focus
language	NLP-focus
modeling	NLP-focus
(	O
e	O
.	O
g	O
.,	O
Cotterell	O
et	O
al	O
.,	O
2018	O
;	O
Mielke	O
et	O
al	O
.,	O
2019	O
)	O
disagree	O
on	O
whether	O
or	O
not	O
inflectional	NLP-focus
morphology	NLP-focus
makes	O
languages	O
harder	O
to	O
model	O
.	O

We	O
also	O
investigate	O
linguistically	O
motivated	O
subword	NLP-focus
segmentation	NLP-focus
strategies	O
like	O
Morfessor	NLP-algorithm/tool
and	O
Finite	NLP-algorithm/tool
-	NLP-algorithm/tool
State	NLP-algorithm/tool
Transducers	NLP-algorithm/tool
(	NLP-algorithm/tool
FSTs	NLP-algorithm/tool
)	NLP-algorithm/tool
segmentation	NLP-focus
these	O
segmentation	O
strategies	O
yield	O
better	O
performance	O
and	O
reduce	O
the	O
impact	O
of	O
a	O
language	O
’	O
s	O
morphology	O
on	O
language	NLP-focus
modeling	NLP-focus
.	O

We	O
also	O
investigate	O
linguistically	O
motivated	O
subword	NLP-focus
segmentation	NLP-focus
strategies	O
like	O
Morfessor	NLP-algorithm/tool
and	O
Finite	NLP-algorithm/tool
-	NLP-algorithm/tool
State	NLP-algorithm/tool
Transducers	NLP-algorithm/tool
(	NLP-algorithm/tool
FSTs	NLP-algorithm/tool
)	NLP-algorithm/tool
segmentation	NLP-focus
these	O
segmentation	O
strategies	O
yield	O
better	O
performance	O
and	O
reduce	O
the	O
impact	O
of	O
a	O
language	O
’	O
s	O
morphology	O
on	O
language	NLP-focus
modeling	NLP-focus
.	O

We	O
present	O
the	O
Quantized	O
Transformer	O
(	O
QT	O
)	O
an	O
unsupervised	O
system	O
for	O
extractive	NLP-focus
opinion	NLP-focus
summarization	NLP-focus
QT	O

QT	O
is	O
inspired	O
by	O
Vector	O
-	O
Quantized	O
Variational	O
Autoencoders	O
which	O
we	O
repurpose	O
for	O
popularity	O
-	O
driven	O
summarization	NLP-focus
.	O

It	O
uses	O
a	O
clustering	O
interpretation	O
of	O
the	O
quantized	O
space	O
and	O
a	O
novel	O
extraction	O
algorithm	O
to	O
discover	O
popular	O
opinions	O
among	O
hundreds	O
of	O
reviews	O
,	O
a	O
significant	O
step	O
towards	O
opinion	NLP-focus
summarization	NLP-focus
of	O
practical	O
scope	O
.	O

We	O
also	O
make	O
publicly	O
available	O
Space	O
a	O
large	O
-	O
scale	O
evaluation	O
benchmark	O
for	O
opinion	NLP-focus
summarizers	NLP-focus
comprising	O
general	O
and	O
aspect	O
-	O
specific	O
summaries	O
for	O
50	O
hotels	O
.	O

We	O
reformulate	O
faithfulness	O
as	O
an	O
accurate	O
attribution	O
of	O
causality	O
to	O
the	O
model	O
,	O
and	O
introduce	O
the	O
concept	O
of	O
aligned	NLP-focus
faithfulness	NLP-focus
faithful	O
causal	O
chains	O
that	O
are	O
aligned	O
with	O
their	O
expected	O
social	O
behavior	O
.	O

We	O
introduce	O
an	O
Edit	O
-	O
Based	O
TransfOrmer	O
with	O
Repositioning	O
(	O
EDITOR	O
)	O
which	O
makes	O
sequence	NLP-focus
generation	NLP-focus
flexible	O
by	O
seamlessly	O
allowing	O
users	O
to	O
specify	O
preferences	O
in	O
output	O
lexical	O
choice	O
.	O

Building	O
on	O
recent	O
models	O
for	O
non	NLP-focus
-	NLP-focus
autoregressive	NLP-focus
sequence	NLP-focus
generation	NLP-focus
(	O
Gu	O
et	O
al	O
.,	O
2019	O
),	O
EDITOR	O
generates	O
new	O
sequences	O
by	O
iteratively	O
editing	O
hypotheses	O
.	O

EDITOR	O
also	O
achieves	O
comparable	O
or	O
better	O
translation	O
quality	O
with	O
faster	O
decoding	O
speed	O
than	O
the	O
Levenshtein	O
Transformer	O
on	O
standard	O
Romanian	O
-	O
English	O
English	O
-	O
German	O
and	O
English	O
-	O
Japanese	O
machine	NLP-focus
translation	NLP-focus
tasks	O
.	O

We	O
investigate	O
the	O
capacity	O
of	O
this	O
architecture	O
relative	O
to	O
sparse	NLP-algorithm/tool
bag	NLP-algorithm/tool
-	NLP-algorithm/tool
of	NLP-algorithm/tool
-	NLP-algorithm/tool
words	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
attentional	O
neural	O
networks	O
.	O

Building	O
on	O
these	O
insights	O
,	O
we	O
propose	O
a	O
simple	O
neural	O
model	O
that	O
combines	O
the	O
efficiency	O
of	O
dual	O
encoders	O
with	O
some	O
of	O
the	O
expressiveness	O
of	O
more	O
costly	O
attentional	O
architectures	O
,	O
and	O
explore	O
sparse	O
-	O
dense	O
hybrids	O
to	O
capitalize	O
on	O
the	O
precision	O
of	O
sparse	NLP-focus
retrieval	NLP-focus
.	O

A	O
key	O
limitation	O
in	O
current	O
datasets	O
for	O
multi	NLP-focus
-	NLP-focus
hop	NLP-focus
reasoning	NLP-focus
is	O
that	O
the	O
required	O
steps	O
for	O
answering	O
the	O
question	O
are	O
mentioned	O
in	O
it	O
explicitly	O
.	O

In	O
this	O
work	O
,	O
we	O
introduce	O
StrategyQA	O
a	O
question	NLP-focus
answering	NLP-focus
(	NLP-focus
QA	NLP-focus
)	NLP-focus
benchmark	O
where	O
the	O
required	O
reasoning	O
steps	O
are	O
implicit	O
in	O
the	O
question	O
,	O
and	O
should	O
be	O
inferred	O
using	O
a	O
strategy	O
.	O

Empirically	O
,	O
we	O
show	O
that	O
humans	O
perform	O
well	O
(	O
87	Numerical-result
\\%	Numerical-result
on	O
this	O
task	O
,	O
while	O
our	O
best	O
baseline	O
reaches	O
an	O
accuracy	O
of	O
∼	Numerical-result
66	Numerical-result
\\%	Numerical-result
.	O

We	O
present	O
a	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
that	O
combines	O
a	O
large	O
parametric	O
neural	O
network	O
(	O
i	O
.	O
e	O
.,	O
a	O
transformer	O
with	O
a	O
non	O
-	O
parametric	O
episodic	O
memory	O
component	O
in	O
an	O
integrated	O
architecture	O
.	O

Experiments	O
on	O
word	O
-	O
based	O
and	O
character	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
language	NLP-algorithm/tool
modeling	NLP-algorithm/tool
datasets	O
demonstrate	O
the	O
efficacy	O
of	O
our	O
proposed	O
method	O
compared	O
to	O
strong	O
baselines	O
.	O

Task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialog	NLP-focus
(	NLP-focus
TOD	NLP-focus
)	NLP-focus
systems	O
often	O
need	O
to	O
formulate	O
knowledge	O
base	O
(	O
KB	O
)	O
queries	O
corresponding	O
to	O
the	O
user	O
intent	O
and	O
use	O
the	O
query	O
results	O
to	O
generate	O
system	O
responses	O
.	O

For	O
query	NLP-focus
prediction	NLP-focus
we	O
propose	O
a	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	AI/ML/DL-domain
RL	AI/ML/DL-domain
)	AI/ML/DL-domain
baseline	O
,	O
which	O
rewards	O
the	O
generation	O
of	O
those	O
queries	O
whose	O
KB	O
results	O
cover	O
the	O
entities	O
mentioned	O
in	O
subsequent	O
dialog	O
.	O

For	O
query	NLP-focus
prediction	NLP-focus
we	O
propose	O
a	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	AI/ML/DL-domain
RL	AI/ML/DL-domain
)	AI/ML/DL-domain
baseline	O
,	O
which	O
rewards	O
the	O
generation	O
of	O
those	O
queries	O
whose	O
KB	O
results	O
cover	O
the	O
entities	O
mentioned	O
in	O
subsequent	O
dialog	O
.	O

Further	O
analysis	O
reveals	O
that	O
correlation	O
among	O
query	O
attributes	O
in	O
KB	O
can	O
significantly	O
confuse	O
memory	O
augmented	O
policy	O
optimization	O
(	O
MAPO	O
)	O
an	O
existing	O
state	O
of	O
the	O
art	O
RL	AI/ML/DL-domain
MAPO	O
.	O
To	O
address	O
this	O
,	O
we	O
improve	O
the	O
MAPO	O
baseline	O
with	O
simple	O
but	O
important	O
modifications	O
suited	O
to	O
our	O
task	O
.	O

To	O
train	O
the	O
full	O
TOD	NLP-focus
system	O
for	O
our	O
setting	O
,	O
we	O
propose	O
a	O
pipelined	O
approach	O
:	O
it	O
independently	O
predicts	O
when	O
to	O
make	O
a	O
KB	O
query	O
(	O
query	NLP-algorithm/tool
position	NLP-algorithm/tool
predictor	NLP-algorithm/tool
KB	O
query	O
edicts	O
a	O
KB	O
query	O
at	O
the	O
predicted	O
position	O
(	O
query	O
predictor	O
),	O
and	O
uses	O
the	O
results	O
of	O
predicted	O
query	O
in	O
subsequent	O
dialog	O
(	O
next	O
response	O
predictor	O
).	O

To	O
train	O
the	O
full	O
TOD	NLP-focus
system	O
for	O
our	O
setting	O
,	O
we	O
propose	O
a	O
pipelined	O
approach	O
:	O
it	O
independently	O
predicts	O
when	O
to	O
make	O
a	O
KB	O
query	O
(	O
query	NLP-algorithm/tool
position	NLP-algorithm/tool
predictor	NLP-algorithm/tool
KB	O
query	O
edicts	O
a	O
KB	O
query	O
at	O
the	O
predicted	O
position	O
(	O
query	O
predictor	O
),	O
and	O
uses	O
the	O
results	O
of	O
predicted	O
query	O
in	O
subsequent	O
dialog	O
(	O
next	O
response	O
predictor	O
).	O

The	O
scarcity	O
of	O
comprehensive	O
up	O
-	O
to	O
-	O
date	O
studies	O
on	O
evaluation	O
metrics	O
for	O
text	NLP-focus
summarization	NLP-focus
and	O
the	O
lack	O
of	O
consensus	O
regarding	O
evaluation	O
protocols	O
continue	O
to	O
inhibit	O
progress	O
.	O

We	O
address	O
the	O
existing	O
shortcomings	O
of	O
summarization	O
evaluation	O
methods	O
along	O
five	O
dimensions	O
:	O
1	O
)	O
we	O
re	O
-	O
evaluate	O
14	O
automatic	O
evaluation	O
metrics	O
in	O
a	O
comprehensive	O
and	O
consistent	O
fashion	O
using	O
neural	NLP-algorithm/tool
summarization	NLP-algorithm/tool
model	NLP-algorithm/tool
outputs	O
along	O
with	O
expert	O
and	O
crowd	O
-	O
sourced	O
human	O
annotations	O
;	O
2	O
)	O
we	O
consistently	O
benchmark	O
23	O
recent	O
summarization	NLP-algorithm/tool
models	NLP-algorithm/tool
using	O
the	O
aforementioned	O
automatic	O
evaluation	O
metrics	O
;	O
3	O
)	O
we	O
assemble	O
the	O
largest	O
collection	O
of	O
summaries	O
generated	O
by	O
models	O
trained	O
on	O
the	O
CNN	O
/	O
DailyMail	O
news	O
summarization	NLP-algorithm/tool
models	NLP-algorithm/tool
in	O
a	O
unified	O
format	O
;	O
4	O
)	O
we	O
implement	O
and	O
share	O
a	O
toolkit	O
that	O
provides	O
an	O
extensible	O
and	O
unified	O
API	O
for	O
evaluating	O
summarization	O
models	O
across	O
a	O
broad	O
range	O
of	O
automatic	O
metrics	O
;	O
and	O
5	O
)	O
we	O
assemble	O
and	O
share	O
the	O
largest	O
and	O
most	O
diverse	O
,	O
in	O
terms	O
of	O
model	O
types	O
,	O
collection	O
of	O
human	O
judgments	O
of	O
model	O
-	O
generated	O
summaries	O
on	O
the	O
CNN	O
/	O
Daily	O
Mail	O
dataset	O
annotated	O
by	O
both	O
expert	O
judges	O
and	O
crowd	O
-	O
source	O
workers	O
.	O

We	O
hope	O
that	O
this	O
work	O
will	O
help	O
promote	O
a	O
more	O
complete	O
evaluation	O
protocol	O
for	O
text	NLP-focus
summarization	NLP-focus
as	O
well	O
as	O
advance	O
research	O
in	O
developing	O
evaluation	O
metrics	O
that	O
better	O
correlate	O
with	O
human	O
judgments	O
.	O

For	O
instance	O
,	O
given	O
training	O
data	O
for	O
named	NLP-focus
entity	NLP-focus
recognition	NLP-focus
(	NLP-focus
NER	NLP-focus
)	NLP-focus
in	O
Vietnamese	O
and	O
for	O
part	NLP-focus
-	NLP-focus
of	NLP-focus
-	NLP-focus
speech	NLP-focus
(	NLP-focus
POS	NLP-focus
)	NLP-focus
tagging	NLP-focus
in	O
Wolof	O
NER	NLP-focus
Wolof	O
can	O
perform	O
accurate	O
predictions	O
for	O
NER	O
in	O
Wolof	O
.	O

The	O
metrics	O
standardly	O
used	O
to	O
evaluate	O
Natural	NLP-algorithm/tool
Language	NLP-algorithm/tool
Generation	NLP-algorithm/tool
(	NLP-algorithm/tool
NLG	NLP-algorithm/tool
)	NLP-algorithm/tool
models	NLP-algorithm/tool
such	O
as	O
BLEU	NLP-metrics
or	O
METEOR	NLP-metrics
fail	O
to	O
provide	O
information	O
on	O
which	O
linguistic	O
factors	O
impact	O
performance	O
.	O

The	O
metrics	O
standardly	O
used	O
to	O
evaluate	O
Natural	NLP-algorithm/tool
Language	NLP-algorithm/tool
Generation	NLP-algorithm/tool
(	NLP-algorithm/tool
NLG	NLP-algorithm/tool
)	NLP-algorithm/tool
models	NLP-algorithm/tool
such	O
as	O
BLEU	NLP-metrics
or	O
METEOR	NLP-metrics
fail	O
to	O
provide	O
information	O
on	O
which	O
linguistic	O
factors	O
impact	O
performance	O
.	O

Focusing	O
on	O
Surface	NLP-focus
Realization	NLP-focus
(	NLP-focus
SR	NLP-focus
)	NLP-focus
the	O
task	O
of	O
converting	O
an	O
unordered	O
dependency	O
tree	O
into	O
a	O
well	O
-	O
formed	O
sentence	O
,	O
we	O
propose	O
a	O
framework	O
for	O
error	O
analysis	O
which	O
permits	O
identifying	O
which	O
features	O
of	O
the	O
input	O
affect	O
the	O
models	O
’	O
results	O
.	O

We	O
demonstrate	O
the	O
advantages	O
of	O
our	O
framework	O
by	O
performing	O
error	O
analysis	O
on	O
the	O
results	O
of	O
174	O
system	O
runs	O
submitted	O
to	O
the	O
Multilingual	NLP-focus
SR	NLP-focus
shared	O
tasks	O
;	O
we	O
show	O
that	O
dependency	O
edge	O
accuracy	Classification-metrics
correlate	O
with	O
automatic	O
metrics	O
thereby	O
providing	O
a	O
more	O
interpretable	O
basis	O
for	O
evaluation	O
;	O
and	O
we	O
suggest	O
ways	O
in	O
which	O
our	O
framework	O
could	O
be	O
used	O
to	O
improve	O
models	O
and	O
data	O
.	O

We	O
demonstrate	O
the	O
advantages	O
of	O
our	O
framework	O
by	O
performing	O
error	O
analysis	O
on	O
the	O
results	O
of	O
174	O
system	O
runs	O
submitted	O
to	O
the	O
Multilingual	NLP-focus
SR	NLP-focus
shared	O
tasks	O
;	O
we	O
show	O
that	O
dependency	O
edge	O
accuracy	Classification-metrics
correlate	O
with	O
automatic	O
metrics	O
thereby	O
providing	O
a	O
more	O
interpretable	O
basis	O
for	O
evaluation	O
;	O
and	O
we	O
suggest	O
ways	O
in	O
which	O
our	O
framework	O
could	O
be	O
used	O
to	O
improve	O
models	O
and	O
data	O
.	O

The	O
framework	O
is	O
available	O
in	O
the	O
form	O
of	O
a	O
toolkit	O
which	O
can	O
be	O
used	O
both	O
by	O
campaign	O
organizers	O
to	O
provide	O
detailed	O
,	O
linguistically	O
interpretable	O
feedback	O
on	O
the	O
state	O
of	O
the	O
art	O
in	O
multilingual	NLP-focus
SR	NLP-focus
and	O
by	O
individual	O
researchers	O
to	O
improve	O
models	O
and	O
datasets	O
1	O
.	O

Models	O
for	O
question	NLP-focus
answering	NLP-focus
dialogue	NLP-focus
agents	NLP-focus
and	O
summarization	NLP-focus
often	O
interpret	O
the	O
meaning	O
of	O
a	O
sentence	O
in	O
a	O
rich	O
context	O
and	O
use	O
that	O
meaning	O
in	O
a	O
new	O
context	O
.	O

We	O
isolate	O
and	O
define	O
the	O
problem	O
of	O
sentence	NLP-focus
decontextualization	NLP-focus
taking	O
a	O
sentence	O
together	O
with	O
its	O
context	O
and	O
rewriting	O
it	O
to	O
be	O
interpretable	O
out	O
of	O
context	O
,	O
while	O
preserving	O
its	O
meaning	O
.	O

We	O
present	O
preliminary	O
studies	O
that	O
show	O
the	O
value	O
of	O
sentence	NLP-focus
decontextualization	NLP-focus
in	O
a	O
user	O
-	O
facing	O
task	O
,	O
and	O
as	O
preprocessing	O
for	O
systems	O
that	O
perform	O
document	NLP-focus
understanding	NLP-focus
.	O

We	O
argue	O
that	O
decontextualization	NLP-focus
is	O
an	O
important	O
subtask	O
in	O
many	O
downstream	O
applications	O
,	O
and	O
that	O
the	O
definitions	O
and	O
resources	O
provided	O
can	O
benefit	O
tasks	O
that	O
operate	O
on	O
sentences	O
that	O
occur	O
in	O
a	O
richer	O
context	O
.	O

Slang	O
is	O
a	O
common	O
type	O
of	O
informal	O
language	O
,	O
but	O
its	O
flexible	O
nature	O
and	O
paucity	O
of	O
data	O
resources	O
present	O
challenges	O
for	O
existing	O
natural	NLP-algorithm/tool
language	NLP-algorithm/tool
systems	NLP-algorithm/tool
.	O

We	O
take	O
an	O
initial	O
step	O
toward	O
machine	NLP-focus
generation	NLP-focus
of	O
slang	O
by	O
developing	O
a	O
framework	O
that	O
models	O
the	O
speaker	O
’	O
s	O
word	O
choice	O
in	O
slang	O
context	O
.	O

We	O
perform	O
rigorous	O
evaluations	O
on	O
three	O
slang	O
dictionaries	O
and	O
show	O
that	O
our	O
approach	O
not	O
only	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
but	O
also	O
better	O
predicts	O
the	O
historical	O
emergence	O
of	O
slang	O
word	O
usages	O
from	O
1960s	O
to	O
2000s	O
.	O

For	O
digital	O
corpora	O
of	O
historical	O
prints	O
,	O
the	O
errors	O
are	O
further	O
exacerbated	O
due	O
to	O
low	O
scan	O
quality	O
and	O
lack	O
of	O
language	O
standardization	O
.	O
For	O
the	O
task	O
of	O
OCR	NLP-focus
post	NLP-focus
-	NLP-focus
hoc	NLP-focus
correction	NLP-focus
we	O
propose	O
a	O
neural	O
approach	O
based	O
on	O
a	O
combination	O
of	O
recurrent	O
(	O
RNN	O
)	O
and	O
deep	O
convolutional	O
network	O
(	O
ConvNet	O
)	O
to	O
correct	O
OCR	O
transcription	O
errors	O
.	O

Accounting	O
for	O
the	O
input	O
and	O
output	O
similarity	O
,	O
we	O
propose	O
a	O
new	O
loss	O
function	O
that	O
rewards	O
the	O
model	O
’	O
s	O
correcting	O
behavior	O
.	O
Evaluation	O
on	O
a	O
historical	O
book	O
corpus	O
in	O
German	O
language	O
shows	O
that	O
our	O
models	O
are	O
robust	O
in	O
capturing	O
diverse	O
OCR	NLP-focus
transcription	NLP-focus
errors	O
and	O
reduce	O
the	O
word	O
error	O
rate	O
of	O
32	O
.	O
3	O
\\%	O
by	O
more	O
than	O
89	O
\\%.	O

We	O
introduce	O
a	O
novel	O
paraphrastic	NLP-focus
augmentation	NLP-focus
strategy	O
based	O
on	O
sentence	O
-	O
level	O
lexically	O
constrained	O
paraphrasing	O
and	O
discriminative	O
span	O
alignment	O
.	O

With	O
four	O
days	O
of	O
training	O
data	O
collection	O
for	O
a	O
span	NLP-algorithm/tool
alignment	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
one	O
day	O
of	O
parallel	O
compute	O
,	O
we	O
automatically	O
generate	O
and	O
release	O
to	O
the	O
community	O
495	O
,	O
300	O
unique	O
(	O
Frame	O
,	O
Trigger	O
)	O
pairs	O
in	O
diverse	O
sentential	O
contexts	O
,	O
a	O
roughly	O
50	O
-	O
fold	O
expansion	O
atop	O
FrameNet	O
v1	O
.	O
7	O
.	O

Recent	O
approaches	O
to	O
data	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
text	NLP-focus
generation	NLP-focus
have	O
adopted	O
the	O
very	O
successful	O
encoder	O
-	O
decoder	O
architecture	O
or	O
variants	O
thereof	O
.	O

Extensive	O
experiments	O
on	O
two	O
data	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
text	NLP-focus
benchmarks	O
(	O
RotoWire	O
and	O
MLB	O
show	O
that	O
our	O
approach	O
outperforms	O
competitive	O
baselines	O
in	O
terms	O
of	O
automatic	O
and	O
human	O
evaluation	O
.	O

Phonological	NLP-focus
generalizations	NLP-focus
are	O
finite	O
-	O
state	O
.	O

Tracking	NLP-focus
dialogue	NLP-focus
states	NLP-focus
to	O
better	O
interpret	O
user	O
goals	O
and	O
feed	O
downstream	O
policy	O
learning	O
is	O
a	O
bottleneck	O
in	O
dialogue	O
management	O
.	O

In	O
this	O
work	O
,	O
we	O
examine	O
the	O
ability	O
of	O
NER	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
use	O
contextual	O
information	O
when	O
predicting	O
the	O
type	O
of	O
an	O
ambiguous	O
entity	O
.	O

We	O
introduce	O
NRB	O
,	O
a	O
new	O
testbed	O
carefully	O
designed	O
to	O
diagnose	O
Name	NLP-focus
Regularity	NLP-focus
Bias	NLP-focus
of	O
NER	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

We	O
introduce	O
NRB	O
,	O
a	O
new	O
testbed	O
carefully	O
designed	O
to	O
diagnose	O
Name	NLP-focus
Regularity	NLP-focus
Bias	NLP-focus
of	O
NER	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

Our	O
results	O
indicate	O
that	O
all	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
we	O
tested	O
show	O
such	O
a	O
bias	O
;	O
BERT	NLP-algorithm/tool
fine	NLP-algorithm/tool
-	NLP-algorithm/tool
tuned	NLP-algorithm/tool
models	NLP-algorithm/tool
significantly	O
outperforming	O
feature	O
-	O
based	O
(	O
LSTM	O
-	O
CRF	O
ones	O
on	O
NRB	NLP-focus
despite	O
having	O
comparable	O
(	O
sometimes	O
lower	O
)	O
performance	O
on	O
standard	O
benchmarks	O
.	O
To	O
mitigate	O
this	O
bias	O
,	O
we	O
propose	O
a	O
novel	O
model	O
-	O
agnostic	O
training	O
method	O
that	O
adds	O
learnable	O
adversarial	O
noise	O
NRB	NLP-focus
ome	O
entity	O
mentions	O
,	O
thus	O
enforcing	O
models	O
to	O
focus	O
more	O
strongly	O
on	O
the	O
contextual	O
signal	O
,	O
leading	O
to	O
significant	O
gains	O
on	O
NRB	O
.	O

Our	O
results	O
indicate	O
that	O
all	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
we	O
tested	O
show	O
such	O
a	O
bias	O
;	O
BERT	NLP-algorithm/tool
fine	NLP-algorithm/tool
-	NLP-algorithm/tool
tuned	NLP-algorithm/tool
models	NLP-algorithm/tool
significantly	O
outperforming	O
feature	O
-	O
based	O
(	O
LSTM	O
-	O
CRF	O
ones	O
on	O
NRB	NLP-focus
despite	O
having	O
comparable	O
(	O
sometimes	O
lower	O
)	O
performance	O
on	O
standard	O
benchmarks	O
.	O
To	O
mitigate	O
this	O
bias	O
,	O
we	O
propose	O
a	O
novel	O
model	O
-	O
agnostic	O
training	O
method	O
that	O
adds	O
learnable	O
adversarial	O
noise	O
NRB	NLP-focus
ome	O
entity	O
mentions	O
,	O
thus	O
enforcing	O
models	O
to	O
focus	O
more	O
strongly	O
on	O
the	O
contextual	O
signal	O
,	O
leading	O
to	O
significant	O
gains	O
on	O
NRB	O
.	O

Limerick	NLP-focus
generation	NLP-focus
exemplifies	O
some	O
of	O
the	O
most	O
difficult	O
challenges	O
faced	O
in	O
poetry	O
generation	O
,	O
as	O
the	O
poems	O
must	O
tell	O
a	O
story	O
in	O
only	O
five	O
lines	O
,	O
with	O
constraints	O
on	O
rhyme	O
,	O
stress	O
,	O
and	O
meter	O
.	O

To	O
address	O
these	O
challenges	O
,	O
we	O
introduce	O
LimGen	O
a	O
novel	O
and	O
fully	O
automated	O
system	O
for	O
limerick	NLP-focus
generation	NLP-focus
that	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
LimGen	O
network	O
-	O
based	O
poetry	O
models	O
,	O
as	O
well	O
as	O
prior	O
rule	O
-	O
based	O
poetry	O
models	O
.	O

LimGen	O
consists	O
of	O
three	O
important	O
pieces	O
:	O
the	O
Adaptive	NLP-algorithm/tool
Multi	NLP-algorithm/tool
-	NLP-algorithm/tool
Templated	NLP-algorithm/tool
Constraint	NLP-algorithm/tool
algorithm	O
that	O
constrains	O
our	O
search	O
to	O
the	O
space	O
of	O
realistic	O
poems	O
,	O
the	O
Multi	NLP-algorithm/tool
-	NLP-algorithm/tool
Templated	NLP-algorithm/tool
Beam	NLP-algorithm/tool
Search	NLP-algorithm/tool
algorithm	O
which	O
searches	O
efficiently	O
through	O
the	O
space	O
,	O
and	O
the	O
probabilistic	NLP-algorithm/tool
Storyline	NLP-algorithm/tool
algorithm	O
that	O
provides	O
coherent	O
storylines	O
related	O
to	O
a	O
user	O
-	O
provided	O
prompt	O
word	O
.	O

While	O
pretrained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
have	O
driven	O
impressive	O
gains	O
over	O
morpho	O
-	O
syntactic	O
and	O
semantic	O
tasks	O
,	O
their	O
ability	O
to	O
model	O
discourse	O
and	O
pragmatic	O
phenomena	O
is	O
less	O
clear	O
.	O

As	O
a	O
step	O
towards	O
a	O
better	O
understanding	O
of	O
their	O
discourse	NLP-focus
modeling	NLP-focus
capabilities	O
,	O
we	O
propose	O
a	O
sentence	NLP-focus
intrusion	NLP-focus
detection	NLP-focus
task	O
.	O

We	O
examine	O
the	O
performance	O
of	O
a	O
broad	O
range	O
of	O
pretrained	NLP-algorithm/tool
LMs	NLP-algorithm/tool
on	O
this	O
detection	O
task	O
for	O
English	O
.	O

Lacking	O
a	O
dataset	O
for	O
the	O
task	O
,	O
we	O
introduce	O
INSteD	O
a	O
novel	O
intruder	NLP-focus
sentence	NLP-focus
detection	NLP-focus
dataset	O
,	O
containing	O
170	O
,	O
000	O
+	O
documents	O
constructed	O
from	O
English	O
Wikipedia	O
and	O
CNN	O
news	O
articles	O
.	O

Our	O
experiments	O
show	O
that	O
pretrained	NLP-algorithm/tool
LMs	NLP-algorithm/tool
perform	O
impressively	O
in	O
in	O
-	O
domain	O
evaluation	O
,	O
but	O
experience	O
a	O
substantial	O
drop	O
in	O
the	O
cross	O
-	O
domain	O
setting	O
,	O
indicating	O
limited	O
generalization	O
capacity	O
.	O

We	O
extend	O
this	O
type	O
of	O
study	O
by	O
employing	O
BERT	NLP-algorithm/tool
to	O
characterize	O
variation	O
in	O
the	O
senses	O
of	O
words	O
as	O
well	O
,	O
analyzing	O
two	O
months	O
of	O
English	O
comments	O
in	O
474	O
Reddit	O
communities	O
.	O

Text	NLP-focus
classification	NLP-focus
is	O
a	O
widely	O
studied	O
problem	O
and	O
has	O
broad	O
applications	O
.	O

Experiments	O
on	O
17	O
text	NLP-focus
classification	NLP-focus
datasets	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
method	O
.	O

Direct	O
decoding	O
for	O
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialogue	NLP-focus
is	O
known	O
to	O
suffer	O
from	O
the	O
explaining	O
-	O
away	O
effect	O
,	O
manifested	O
in	O
models	O
that	O
prefer	O
short	O
and	O
generic	O
responses	O
.	O

We	O
explore	O
few	AI/ML/DL-domain
-	AI/ML/DL-domain
shot	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	AI/ML/DL-domain
FSL	AI/ML/DL-domain
)	AI/ML/DL-domain
for	O
relation	NLP-focus
classification	NLP-focus
(	NLP-focus
RC	NLP-focus
)	NLP-focus
.	O

We	O
explore	O
few	AI/ML/DL-domain
-	AI/ML/DL-domain
shot	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	AI/ML/DL-domain
FSL	AI/ML/DL-domain
)	AI/ML/DL-domain
for	O
relation	NLP-focus
classification	NLP-focus
(	NLP-focus
RC	NLP-focus
)	NLP-focus
.	O

Focusing	O
on	O
the	O
realistic	O
scenario	O
of	O
FSL	AI/ML/DL-domain
FSL	AI/ML/DL-domain
which	O
a	O
test	O
instance	O
might	O
not	O
belong	O
to	O
any	O
of	O
the	O
target	O
categories	O
(	O
none	O
-	O
of	O
-	O
the	O
-	O
above	O
,	O
[	O
NOTA	O
]),	O
we	O
first	O
revisit	O
the	O
recent	O
popular	O
dataset	O
structure	O
for	O
FSL	O
,	O
pointing	O
out	O
its	O
unrealistic	O
data	O
distribution	O
.	O

To	O
remedy	O
this	O
,	O
we	O
propose	O
a	O
novel	O
methodology	O
for	O
deriving	O
more	O
realistic	O
few	O
-	O
shot	O
test	O
data	O
from	O
available	O
datasets	O
for	O
supervised	NLP-focus
RC	NLP-focus
and	O
apply	O
it	O
to	O
the	O
TACRED	O
dataset	O
.	O

Next	O
,	O
we	O
analyze	O
classification	O
schemes	O
within	O
the	O
popular	O
embedding	O
-	O
based	O
nearest	O
-	O
neighbor	O
approach	O
for	O
FSL	AI/ML/DL-domain
with	O
respect	O
to	O
constraints	O
they	O
impose	O
on	O
the	O
embedding	O
space	O
.	O

Triggered	O
by	O
this	O
analysis	O
,	O
we	O
propose	O
a	O
novel	O
classification	O
scheme	O
in	O
which	O
the	O
NOTA	O
category	O
is	O
represented	O
as	O
learned	O
vectors	O
,	O
shown	O
empirically	O
to	O
be	O
an	O
appealing	O
option	O
for	O
FSL	AI/ML/DL-domain
.	O

While	O
argument	NLP-focus
mining	NLP-focus
has	O
achieved	O
significant	O
success	O
in	O
classifying	O
argumentative	O
relations	O
between	O
statements	O
(	O
support	O
,	O
attack	O
,	O
and	O
neutral	O
),	O
we	O
have	O
a	O
limited	O
computational	O
understanding	O
of	O
logical	O
mechanisms	O
that	O
constitute	O
those	O
relations	O
.	O

In	O
this	O
paper	O
,	O
we	O
evaluate	O
the	O
translation	NLP-focus
of	O
negation	O
both	O
automatically	O
and	O
manually	O
,	O
in	O
English	NLP-focus
–	NLP-focus
German	NLP-focus
(	NLP-focus
EN	NLP-focus
–	NLP-focus
DE	NLP-focus
)	NLP-focus
and	O
English	NLP-focus
–	NLP-focus
Chinese	NLP-focus
(	NLP-focus
EN	NLP-focus
–	NLP-focus
ZH	NLP-focus
)	NLP-focus
.	O

We	O
show	O
that	O
the	O
ability	O
of	O
neural	NLP-focus
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
NMT	NLP-focus
)	NLP-focus
models	O
to	O
translate	O
negation	O
has	O
improved	O
with	O
deeper	O
and	O
more	O
advanced	O
networks	O
,	O
although	O
the	O
performance	O
varies	O
between	O
language	O
pairs	O
and	O
translation	O
directions	O
.	O

The	O
accuracy	O
of	O
manual	O
evaluation	O
in	O
EN	NLP-focus
→	NLP-focus
DE	NLP-focus
DE	NLP-focus
→	NLP-focus
EN	NLP-focus
EN	NLP-focus
→	NLP-focus
ZH	NLP-focus
and	O
ZH	NLP-focus
→	NLP-focus
EN	NLP-focus
is	O
95	Numerical-result
.	Numerical-result
7	Numerical-result
\\%	Numerical-result
94	Numerical-result
.	Numerical-result
8	Numerical-result
\\%	Numerical-result
93	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
and	O
91	Numerical-result
.	Numerical-result
7	Numerical-result
\\%	Numerical-result
respectively	O
.	O

The	O
accuracy	O
of	O
manual	O
evaluation	O
in	O
EN	NLP-focus
→	NLP-focus
DE	NLP-focus
DE	NLP-focus
→	NLP-focus
EN	NLP-focus
EN	NLP-focus
→	NLP-focus
ZH	NLP-focus
and	O
ZH	NLP-focus
→	NLP-focus
EN	NLP-focus
is	O
95	Numerical-result
.	Numerical-result
7	Numerical-result
\\%	Numerical-result
94	Numerical-result
.	Numerical-result
8	Numerical-result
\\%	Numerical-result
93	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
and	O
91	Numerical-result
.	Numerical-result
7	Numerical-result
\\%	Numerical-result
respectively	O
.	O

In	O
addition	O
,	O
we	O
show	O
that	O
under	O
-	O
translation	O
is	O
the	O
most	O
significant	O
error	O
type	O
in	O
NMT	NLP-focus
which	O
contrasts	O
with	O
the	O
more	O
diverse	O
error	O
profile	O
previously	O
observed	O
for	O
statistical	NLP-focus
machine	NLP-focus
translation	NLP-focus
.	O

While	O
our	O
information	O
flow	O
analysis	O
does	O
not	O
reveal	O
any	O
deficiencies	O
that	O
could	O
be	O
used	O
to	O
detect	O
or	O
fix	O
the	O
under	NLP-focus
-	NLP-focus
translation	NLP-focus
of	O
negation	O
,	O
we	O
find	O
that	O
negation	O
is	O
often	O
rephrased	O
during	O
training	O
,	O
which	O
could	O
make	O
it	O
more	O
difficult	O
for	O
the	O
model	O
to	O
learn	O
a	O
reliable	O
link	O
between	O
source	O
and	O
target	O
negation	O
.	O

We	O
finally	O
conduct	O
intrinsic	O
analysis	O
and	O
extrinsic	O
probing	O
tasks	O
on	O
negation	O
,	O
showing	O
that	O
NMT	NLP-algorithm/tool
models	NLP-algorithm/tool
can	O
distinguish	O
negation	O
and	O
non	O
-	O
negation	O
tokens	O
very	O
well	O
and	O
encode	O
a	O
lot	O
of	O
information	O
about	O
negation	O
in	O
hidden	O
states	O
but	O
nevertheless	O
leave	O
room	O
for	O
improvement	O
.	O

Traditional	O
text	O
overlap	O
based	O
metrics	O
such	O
as	O
ROUGE	NLP-metrics
fail	O
to	O
achieve	O
this	O
because	O
they	O
are	O
limited	O
to	O
matching	O
tokens	O
,	O
either	O
lexically	O
or	O
via	O
embeddings	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
metric	O
to	O
evaluate	O
the	O
content	O
quality	O
of	O
a	O
summary	O
using	O
question	NLP-focus
-	NLP-focus
answering	NLP-focus
(	NLP-focus
QA	NLP-focus
)	NLP-focus
QA	NLP-focus
.	O

We	O
demonstrate	O
the	O
experimental	O
benefits	O
of	O
QA	O
-	O
based	O
metrics	O
through	O
an	O
analysis	O
of	O
our	O
proposed	O
metric	O
,	O
QAEval	NLP-metrics
QAEval	NLP-metrics
.	O

Through	O
a	O
careful	O
analysis	O
of	O
each	O
component	O
of	O
QAEval	NLP-metrics
we	O
identify	O
its	O
performance	O
bottlenecks	O
and	O
estimate	O
that	O
its	O
potential	O
upper	O
-	O
bound	O
performance	O
surpasses	O
all	O
other	O
automatic	O
metrics	O
,	O
approaching	O
that	O
of	O
the	O
gold	O
-	O
standard	O
Pyramid	O
Method	O
1	O
.	O

A	O
question	NLP-focus
answering	NLP-focus
system	O
that	O
in	O
addition	O
to	O
providing	O
an	O
answer	O
provides	O
an	O
explanation	O
of	O
the	O
reasoning	O
that	O
leads	O
to	O
that	O
answer	O
has	O
potential	O
advantages	O
in	O
terms	O
of	O
debuggability	O
,	O
extensibility	O
,	O
and	O
trust	O
.	O

To	O
this	O
end	O
,	O
we	O
propose	O
QED	O
a	O
linguistically	O
informed	O
extensible	O
framework	O
for	O
explanations	O
in	O
question	NLP-focus
answering	NLP-focus
QED	O

We	O
describe	O
and	O
publicly	O
release	O
an	O
expert	O
-	O
annotated	O
dataset	O
of	O
QED	O
explanations	O
built	O
upon	O
a	O
subset	O
of	O
the	O
Google	O
Natural	O
Questions	O
dataset	O
,	O
and	O
report	O
baseline	O
models	O
on	O
two	O
tasks	O
—	O
post	O
-	O
hoc	O
explanation	O
generation	O
given	O
an	O
answer	O
,	O
and	O
joint	O
question	NLP-focus
answering	NLP-focus
explanation	NLP-focus
generation	NLP-focus
ion	O
.	O

We	O
present	O
a	O
new	O
method	O
,	O
Soloist	O
1	O
that	O
uses	O
transfer	O
learning	O
and	O
machine	AI/ML/DL-domain
teaching	AI/ML/DL-domain
to	O
build	O
task	O
bots	O
at	O
scale	O
.	O

We	O
parameterize	O
classical	O
modular	O
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialog	NLP-focus
systems	NLP-focus
using	O
a	O
Transformer	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
auto	NLP-algorithm/tool
-	NLP-algorithm/tool
regressive	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
which	O
subsumes	O
different	O
dialog	O
modules	O
into	O
a	O
single	O
neural	O
model	O
.	O

We	O
parameterize	O
classical	O
modular	O
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialog	NLP-focus
systems	NLP-focus
using	O
a	O
Transformer	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
auto	NLP-algorithm/tool
-	NLP-algorithm/tool
regressive	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
which	O
subsumes	O
different	O
dialog	O
modules	O
into	O
a	O
single	O
neural	O
model	O
.	O

We	O
pre	O
-	O
train	O
,	O
on	O
heterogeneous	O
dialog	O
corpora	O
,	O
a	O
task	NLP-algorithm/tool
-	NLP-algorithm/tool
grounded	NLP-algorithm/tool
response	NLP-algorithm/tool
generation	NLP-algorithm/tool
model	NLP-algorithm/tool
which	O
can	O
generate	O
dialog	O
responses	O
grounded	O
in	O
user	O
goals	O
and	O
real	O
-	O
world	O
knowledge	O
for	O
task	O
completion	O
.	O

Pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
encode	O
rich	O
information	O
about	O
linguistic	O
structure	O
but	O
their	O
knowledge	O
about	O
lexical	NLP-focus
polysemy	NLP-focus
remains	O
unclear	O
.	O

Pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
encode	O
rich	O
information	O
about	O
linguistic	O
structure	O
but	O
their	O
knowledge	O
about	O
lexical	NLP-focus
polysemy	NLP-focus
remains	O
unclear	O
.	O

We	O
propose	O
a	O
novel	O
experimental	O
setup	O
for	O
analyzing	O
this	O
knowledge	O
in	O
LMs	NLP-algorithm/tool
specifically	O
trained	O
for	O
different	O
languages	O
(	O
English	O
,	O
French	O
,	O
Spanish	O
,	O
and	O
Greek	O
)	O
and	O
in	O
multilingual	NLP-algorithm/tool
BERT	NLP-algorithm/tool
.	O

Machine	NLP-focus
translation	NLP-focus
(	NLP-focus
MT	NLP-focus
)	NLP-focus
technology	O
has	O
facilitated	O
our	O
daily	O
tasks	O
by	O
providing	O
accessible	O
shortcuts	O
for	O
gathering	O
,	O
processing	O
,	O
and	O
communicating	O
information	O
.	O

As	O
a	O
relatively	O
new	O
field	O
of	O
inquiry	O
,	O
studies	O
of	O
gender	O
bias	O
in	O
MT	NLP-focus
still	O
lack	O
cohesion	O
.	O

To	O
this	O
end	O
,	O
we	O
:	O
i	O
)	O
critically	O
review	O
current	O
conceptualizations	O
of	O
bias	O
in	O
light	O
of	O
theoretical	O
insights	O
from	O
related	O
disciplines	O
,	O
ii	O
)	O
summarize	O
previous	O
analyses	O
aimed	O
at	O
assessing	O
gender	O
bias	O
in	O
MT	NLP-focus
iii	O
)	O
discuss	O
the	O
mitigating	O
strategies	O
proposed	O
so	O
far	O
,	O
and	O
iv	O
)	O
point	O
toward	O
potential	O
directions	O
for	O
future	O
work	O
.	O

We	O
present	O
a	O
new	O
conjunctivist	O
framework	O
,	O
neural	O
event	O
semantics	O
(	O
NES	O
)	O
for	O
compositional	NLP-focus
grounded	NLP-focus
language	NLP-focus
understanding	NLP-focus
.	O

We	O
introduce	O
a	O
theoretical	O
framework	O
for	O
understanding	O
and	O
predicting	O
the	O
complexity	O
of	O
sequence	NLP-focus
classification	NLP-focus
tasks	O
,	O
using	O
a	O
novel	O
extension	O
of	O
the	O
theory	O
of	O
Boolean	O
function	O
sensitivity	O
.	O

We	O
then	O
estimate	O
sensitivity	O
on	O
15	O
NLP	O
tasks	O
,	O
finding	O
that	O
sensitivity	O
is	O
higher	O
on	O
challenging	O
tasks	O
collected	O
in	O
GLUE	O
than	O
on	O
simple	O
text	NLP-focus
classification	NLP-focus
tasks	O
,	O
and	O
that	O
sensitivity	O
predicts	O
the	O
performance	O
both	O
of	O
simple	O
lexical	NLP-algorithm/tool
classifiers	NLP-algorithm/tool
and	O
of	O
vanilla	O
BiLSTMs	O
without	O
pretrained	O
contextualized	O
embeddings	O
.	O

We	O
then	O
estimate	O
sensitivity	O
on	O
15	O
NLP	O
tasks	O
,	O
finding	O
that	O
sensitivity	O
is	O
higher	O
on	O
challenging	O
tasks	O
collected	O
in	O
GLUE	O
than	O
on	O
simple	O
text	NLP-focus
classification	NLP-focus
tasks	O
,	O
and	O
that	O
sensitivity	O
predicts	O
the	O
performance	O
both	O
of	O
simple	O
lexical	NLP-algorithm/tool
classifiers	NLP-algorithm/tool
and	O
of	O
vanilla	O
BiLSTMs	O
without	O
pretrained	O
contextualized	O
embeddings	O
.	O

Named	NLP-focus
Entity	NLP-focus
Recognition	NLP-focus
(	NLP-focus
NER	NLP-focus
)	NLP-focus
is	O
a	O
fundamental	O
NLP	O
task	O
,	O
commonly	O
formulated	O
as	O
classification	O
over	O
a	O
sequence	O
of	O
tokens	O
.	O

To	O
address	O
NER	NLP-focus
in	O
MRLs	O
we	O
then	O
need	O
to	O
answer	O
two	O
fundamental	O
questions	O
,	O
namely	O
,	O
what	O
are	O
the	O
basic	O
units	O
to	O
be	O
labeled	O
,	O
and	O
how	O
can	O
these	O
units	O
be	O
detected	O
and	O
classified	O
in	O
realistic	O
settings	O
(	O
i	O
.	O
e	O
.,	O
where	O
no	O
gold	O
morphology	O
is	O
available	O
).	O

We	O
empirically	O
investigate	O
these	O
questions	O
on	O
a	O
novel	O
NER	NLP-focus
benchmark	O
,	O
with	O
parallel	O
token	O
-	O
level	O
and	O
morpheme	O
-	O
level	O
NER	O
annotations	O
,	O
which	O
we	O
develop	O
for	O
Modern	O
Hebrew	O
,	O
a	O
morphologically	O
rich	O
-	O
and	O
-	O
ambiguous	O
language	O
.	O

Our	O
results	O
show	O
that	O
explicitly	O
modeling	O
morphological	O
boundaries	O
leads	O
to	O
improved	O
NER	NLP-focus
performance	O
,	O
and	O
that	O
a	O
novel	O
hybrid	O
architecture	O
,	O
in	O
which	O
NER	O
precedes	O
and	O
prunes	O
morphological	O
decomposition	O
,	O
greatly	O
outperforms	O
the	O
standard	O
pipeline	O
,	O
where	O
morphological	O
decomposition	O
strictly	O
precedes	O
NER	O
,	O
setting	O
a	O
new	O
performance	O
bar	O
for	O
both	O
Hebrew	O
NER	O
and	O
Hebrew	O
morphological	O
decomposition	O
tasks	O
.	O

Systems	O
for	O
Open	NLP-focus
-	NLP-focus
Domain	NLP-focus
Question	NLP-focus
Answering	NLP-focus
(	NLP-focus
OpenQA	NLP-focus
)	NLP-focus
generally	O
depend	O
on	O
a	O
retriever	O
for	O
finding	O
candidate	O
passages	O
in	O
a	O
large	O
corpus	O
and	O
a	O
reader	O
for	O
extracting	O
answers	O
from	O
those	O
passages	O
.	O

To	O
address	O
this	O
,	O
we	O
define	O
ColBERT	O
-	O
QA	O
which	O
adapts	O
the	O
scalable	O
neural	O
retrieval	O
model	O
ColBERT	NLP-algorithm/tool
to	O
OpenQA	NLP-focus
ColBERT	NLP-algorithm/tool
.	O

To	O
address	O
this	O
,	O
we	O
define	O
ColBERT	O
-	O
QA	O
which	O
adapts	O
the	O
scalable	O
neural	O
retrieval	O
model	O
ColBERT	NLP-algorithm/tool
to	O
OpenQA	NLP-focus
ColBERT	NLP-algorithm/tool
.	O

We	O
propose	O
an	O
efficient	O
weak	O
supervision	O
strategy	O
that	O
iteratively	O
uses	O
ColBERT	NLP-algorithm/tool
to	O
create	O
its	O
own	O
training	O
data	O
.	O

This	O
greatly	O
improves	O
OpenQA	NLP-focus
retrieval	O
on	O
Natural	O
Questions	O
SQuAD	O
and	O
TriviaQA	O
and	O
the	O
resulting	O
system	O
attains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
OpenQA	NLP-focus
ive	O
OpenQA	O
performance	O
on	O
all	O
three	O
datasets	O
.	O

This	O
paper	O
presents	O
a	O
novel	O
unsupervised	NLP-focus
abstractive	NLP-focus
summarization	NLP-focus
method	O
for	O
opinionated	O
texts	O
.	O

Experimental	O
results	O
demonstrate	O
that	O
the	O
generated	O
topic	O
sentences	O
are	O
appropriate	O
as	O
a	O
summary	O
of	O
opinionated	O
texts	O
,	O
which	O
are	O
more	O
informative	O
and	O
cover	O
more	O
input	O
contents	O
than	O
those	O
generated	O
by	O
the	O
recent	O
unsupervised	NLP-algorithm/tool
summarization	NLP-algorithm/tool
model	NLP-algorithm/tool
(	O
Bražinskas	O
et	O
al	O
.,	O
2020	O
).	O

Furthermore	O
,	O
we	O
demonstrate	O
that	O
the	O
variance	O
of	O
latent	O
Gaussians	O
represents	O
the	O
granularity	O
of	O
sentences	O
,	O
analogous	O
to	O
Gaussian	NLP-algorithm/tool
word	NLP-algorithm/tool
embedding	NLP-algorithm/tool
(	O
Vilnis	O
and	O
McCallum	O
,	O
2015	O
).	O

Recent	O
works	O
have	O
shown	O
that	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LM	NLP-algorithm/tool
)	NLP-algorithm/tool
capture	O
different	O
types	O
of	O
knowledge	O
regarding	O
facts	O
or	O
common	O
sense	O
.	O

In	O
this	O
paper	O
,	O
we	O
ask	O
the	O
question	O
,	O
“	O
How	O
can	O
we	O
know	O
when	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
know	O
,	O
with	O
confidence	O
,	O
the	O
answer	O
to	O
a	O
particular	O
query	O
?”	O
We	O
examine	O
this	O
question	O
from	O
the	O
point	O
of	O
view	O
of	O
calibration	O
,	O
the	O
property	O
of	O
a	O
probabilistic	O
model	O
s	O
predicted	O
probabilities	O
actually	O
being	O
well	O
correlated	O
with	O
the	O
probabilities	O
of	O
correctness	O
.	O

We	O
examine	O
three	O
strong	O
generative	O
models	O
T5	NLP-algorithm/tool
BART	NLP-algorithm/tool
and	O
GPT	NLP-algorithm/tool
-	NLP-algorithm/tool
2	NLP-algorithm/tool
and	O
study	O
whether	O
their	O
probabilities	O
on	O
QA	NLP-focus
tasks	O
are	O
well	O
calibrated	O
,	O
finding	O
the	O
answer	O
is	O
a	O
relatively	O
emphatic	O
no	O
.	O

We	O
examine	O
three	O
strong	O
generative	O
models	O
T5	NLP-algorithm/tool
BART	NLP-algorithm/tool
and	O
GPT	NLP-algorithm/tool
-	NLP-algorithm/tool
2	NLP-algorithm/tool
and	O
study	O
whether	O
their	O
probabilities	O
on	O
QA	NLP-focus
tasks	O
are	O
well	O
calibrated	O
,	O
finding	O
the	O
answer	O
is	O
a	O
relatively	O
emphatic	O
no	O
.	O

Recently	O
,	O
a	O
multitude	O
of	O
methods	O
have	O
been	O
proposed	O
for	O
pretraining	O
vision	O
and	O
language	O
BERTs	NLP-algorithm/tool
to	O
tackle	O
challenges	O
at	O
the	O
intersection	O
of	O
these	O
two	O
key	O
areas	O
of	O
AI	AI/ML/DL-domain
.	O

Recently	O
,	O
a	O
multitude	O
of	O
methods	O
have	O
been	O
proposed	O
for	O
pretraining	O
vision	O
and	O
language	O
BERTs	NLP-algorithm/tool
to	O
tackle	O
challenges	O
at	O
the	O
intersection	O
of	O
these	O
two	O
key	O
areas	O
of	O
AI	AI/ML/DL-domain
.	O

We	O
then	O
conduct	O
controlled	O
experiments	O
to	O
discern	O
the	O
empirical	O
differences	O
between	O
five	O
vision	O
and	O
language	O
BERTs	NLP-algorithm/tool
.	O

Common	NLP-focus
grounding	NLP-focus
is	O
the	O
process	O
of	O
creating	O
and	O
maintaining	O
mutual	O
understandings	O
,	O
which	O
is	O
a	O
critical	O
aspect	O
of	O
sophisticated	O
human	O
communication	O
.	O

In	O
this	O
paper	O
we	O
study	O
the	O
question	O
:	O
Are	O
Pretrained	NLP-algorithm/tool
Language	NLP-algorithm/tool
Models	NLP-algorithm/tool
(	NLP-algorithm/tool
PLMs	NLP-algorithm/tool
)	NLP-algorithm/tool
consistent	O
with	O
respect	O
to	O
factual	O
knowledge	O
?	O
To	O
this	O
end	O
,	O
we	O
create	O
ParaRel	O
a	O
high	O
-	O
quality	O
resource	O
of	O
cloze	O
-	O
style	O
query	O
English	O
paraphrases	O
.	O

Using	O
ParaRel	O
we	O
show	O
that	O
the	O
consistency	O
of	O
all	O
PLMs	NLP-algorithm/tool
we	O
experiment	O
with	O
is	O
poor	O
—	O
though	O
with	O
high	O
variance	O
between	O
relations	O
.	O

Our	O
analysis	O
of	O
the	O
representational	O
spaces	O
of	O
PLMs	NLP-algorithm/tool
suggests	O
that	O
they	O
have	O
a	O
poor	O
structure	O
and	O
are	O
currently	O
not	O
suitable	O
for	O
representing	O
knowledge	O
robustly	O
.	O

Recent	O
advancements	O
in	O
open	NLP-focus
-	NLP-focus
domain	NLP-focus
question	NLP-focus
answering	NLP-focus
(	NLP-focus
ODQA	NLP-focus
)	NLP-focus
that	O
is	O
,	O
finding	O
answers	O
from	O
large	O
open	O
-	O
domain	O
corpus	O
like	O
Wikipedia	O
,	O
have	O
led	O
to	O
human	O
-	O
level	O
performance	O
on	O
many	O
datasets	O
.	O

However	O
,	O
progress	O
in	O
QA	NLP-focus
over	O
book	O
stories	O
(	O
Book	NLP-focus
QA	NLP-focus
lags	O
despite	O
its	O
similar	O
task	O
formulation	O
to	O
ODQA	NLP-focus
.	O

This	O
work	O
provides	O
a	O
comprehensive	O
and	O
quantitative	O
analysis	O
about	O
the	O
difficulty	O
of	O
Book	NLP-focus
QA	NLP-focus
(	O
1	O
)	O
We	O
benchmark	O
the	O
research	O
on	O
the	O
NarrativeQA	O
dataset	O
with	O
extensive	O
experiments	O
with	O
cutting	O
-	O
edge	O
ODQA	NLP-focus
techniques	O
.	O

This	O
quantifies	O
the	O
challenges	O
Book	NLP-focus
QA	NLP-focus
poses	O
,	O
as	O
well	O
as	O
advances	O
the	O
published	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
with	O
a	O
∼	O
7	O
\\%	O
absolute	O
improvement	O
on	O
ROUGE	O
-	O
L	O
.	O

(	O
2	O
)	O
We	O
further	O
analyze	O
the	O
detailed	O
challenges	O
in	O
Book	NLP-focus
QA	NLP-focus
through	O
human	O
studies	O
.	O
1	O
Our	O
findings	O
indicate	O
that	O
the	O
event	O
-	O
centric	O
questions	O
dominate	O
this	O
task	O
,	O
which	O
exemplifies	O
the	O
inability	O
of	O
existing	O
QA	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
handle	O
event	O
-	O
oriented	O
scenarios	O
.	O

(	O
2	O
)	O
We	O
further	O
analyze	O
the	O
detailed	O
challenges	O
in	O
Book	NLP-focus
QA	NLP-focus
through	O
human	O
studies	O
.	O
1	O
Our	O
findings	O
indicate	O
that	O
the	O
event	O
-	O
centric	O
questions	O
dominate	O
this	O
task	O
,	O
which	O
exemplifies	O
the	O
inability	O
of	O
existing	O
QA	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
handle	O
event	O
-	O
oriented	O
scenarios	O
.	O

Language	NLP-algorithm/tool
models	NLP-algorithm/tool
trained	O
on	O
billions	O
of	O
tokens	O
have	O
recently	O
led	O
to	O
unprecedented	O
results	O
on	O
many	O
NLP	O
tasks	O
.	O

We	O
find	O
that	O
assertions	O
enable	O
semantic	O
emulation	O
of	O
languages	O
that	O
satisfy	O
a	O
strong	O
notion	O
of	O
semantic	NLP-focus
transparency	NLP-focus
.	O

Here	O
,	O
we	O
summarize	O
the	O
research	O
in	O
compressing	O
Transformers	O
focusing	O
on	O
the	O
especially	O
popular	O
BERT	NLP-algorithm/tool
model	O
.	O

In	O
particular	O
,	O
we	O
survey	O
the	O
state	O
of	O
the	O
art	O
in	O
compression	O
for	O
BERT	NLP-algorithm/tool
we	O
clarify	O
the	O
current	O
best	O
practices	O
for	O
compressing	O
large	O
-	O
scale	O
Transformer	O
models	O
,	O
and	O
we	O
provide	O
insights	O
into	O
the	O
workings	O
of	O
various	O
methods	O
.	O

We	O
investigate	O
how	O
well	O
BERT	NLP-algorithm/tool
performs	O
on	O
predicting	O
factuality	O
in	O
several	O
existing	O
English	O
datasets	O
,	O
encompassing	O
various	O
linguistic	O
constructions	O
.	O

Although	O
BERT	NLP-algorithm/tool
obtains	O
a	O
strong	O
performance	O
on	O
most	O
datasets	O
,	O
it	O
does	O
so	O
by	O
exploiting	O
common	O
surface	O
patterns	O
that	O
correlate	O
with	O
certain	O
factuality	O
labels	O
,	O
and	O
it	O
fails	O
on	O
instances	O
where	O
pragmatic	O
reasoning	O
is	O
necessary	O
.	O

Open	NLP-focus
-	NLP-focus
domain	NLP-focus
Question	NLP-focus
Answering	NLP-focus
models	O
that	O
directly	O
leverage	O
question	O
-	O
answer	O
(	O
QA	O
)	O
pairs	O
such	O
as	O
closed	NLP-algorithm/tool
-	NLP-algorithm/tool
book	NLP-algorithm/tool
QA	NLP-algorithm/tool
(	NLP-algorithm/tool
CBQA	NLP-algorithm/tool
)	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
QA	NLP-algorithm/tool
-	NLP-algorithm/tool
pair	NLP-algorithm/tool
retrievers	NLP-algorithm/tool
show	O
promise	O
in	O
terms	O
of	O
speed	O
and	O
memory	O
compared	O
with	O
conventional	O
models	O
which	O
retrieve	O
and	O
read	O
from	O
text	O
corpora	O
.	O

Open	NLP-focus
-	NLP-focus
domain	NLP-focus
Question	NLP-focus
Answering	NLP-focus
models	O
that	O
directly	O
leverage	O
question	O
-	O
answer	O
(	O
QA	O
)	O
pairs	O
such	O
as	O
closed	NLP-algorithm/tool
-	NLP-algorithm/tool
book	NLP-algorithm/tool
QA	NLP-algorithm/tool
(	NLP-algorithm/tool
CBQA	NLP-algorithm/tool
)	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
QA	NLP-algorithm/tool
-	NLP-algorithm/tool
pair	NLP-algorithm/tool
retrievers	NLP-algorithm/tool
show	O
promise	O
in	O
terms	O
of	O
speed	O
and	O
memory	O
compared	O
with	O
conventional	O
models	O
which	O
retrieve	O
and	O
read	O
from	O
text	O
corpora	O
.	O

QA	NLP-algorithm/tool
-	NLP-algorithm/tool
pair	NLP-algorithm/tool
retrievers	NLP-algorithm/tool
also	O
offer	O
interpretable	O
answers	O
,	O
a	O
high	O
degree	O
of	O
control	O
,	O
and	O
are	O
trivial	O
to	O
update	O
at	O
test	O
time	O
with	O
new	O
knowledge	O
.	O

We	O
introduce	O
a	O
new	O
QA	NLP-algorithm/tool
-	NLP-algorithm/tool
pair	NLP-algorithm/tool
retriever	NLP-algorithm/tool
RePAQ	O
PAQ	O
complement	O
PAQ	O
.	O

Using	O
PAQ	O
we	O
train	O
CBQA	NLP-algorithm/tool
models	NLP-algorithm/tool
which	O
outperform	O
comparable	O
baselines	O
by	O
5	O
\\%,	O
but	O
trail	O
RePAQ	O
RePAQ	O
r	O
15	O
\\%,	O
indicating	O
the	O
effectiveness	O
of	O
explicit	O
retrieval	O
.	O

RePAQ	O
can	O
be	O
configured	O
for	O
size	O
(	O
under	O
500MB	O
)	O
or	O
speed	O
(	O
over	O
1K	O
questions	O
per	O
second	O
)	O
while	O
retaining	O
high	O
accuracy	Classification-metrics
.	O

We	O
take	O
a	O
step	O
towards	O
addressing	O
the	O
under	O
-	O
representation	O
of	O
the	O
African	O
continent	O
in	O
NLP	O
research	O
by	O
bringing	O
together	O
different	O
stakeholders	O
to	O
create	O
the	O
first	O
large	O
,	O
publicly	O
available	O
,	O
high	O
-	O
quality	O
dataset	O
for	O
named	NLP-focus
entity	NLP-focus
recognition	NLP-focus
(	NLP-focus
NER	NLP-focus
)	NLP-focus
in	O
ten	O
African	O
languages	O
.	O

We	O
detail	O
the	O
characteristics	O
of	O
these	O
languages	O
to	O
help	O
researchers	O
and	O
practitioners	O
better	O
understand	O
the	O
challenges	O
they	O
pose	O
for	O
NER	NLP-focus
tasks	O
.	O

The	O
quality	O
of	O
a	O
summarization	NLP-focus
evaluation	NLP-focus
metric	NLP-focus
is	O
quantified	O
by	O
calculating	O
the	O
correlation	O
between	O
its	O
scores	O
and	O
human	O
annotations	O
across	O
a	O
large	O
number	O
of	O
summaries	O
.	O

Further	O
,	O
although	O
many	O
metrics	O
fail	O
to	O
show	O
statistical	O
improvements	O
over	O
ROUGE	NLP-metrics
two	O
recent	O
works	O
,	O
QAEval	NLP-metrics
and	O
BERTScore	NLP-metrics
do	O
so	O
in	O
some	O
evaluation	O
settings	O
.	O
1	O
.	O

We	O
introduce	O
ParsiNLU	O
the	O
first	O
benchmark	O
in	O
Persian	O
language	O
that	O
includes	O
a	O
range	O
of	O
language	O
understanding	O
tasks	O
—	O
reading	NLP-focus
comprehension	NLP-focus
textual	NLP-focus
entailment	NLP-focus
and	O
so	O
on	O
.	O

Additionally	O
,	O
we	O
present	O
the	O
first	O
results	O
on	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
monolingual	O
and	O
multilingual	NLP-algorithm/tool
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
on	O
this	O
benchmark	O
and	O
compare	O
them	O
with	O
human	O
performance	O
,	O
which	O
provides	O
valuable	O
insights	O
into	O
our	O
ability	O
to	O
tackle	O
natural	O
language	O
understanding	O
challenges	O
in	O
Persian	O
.	O

We	O
hope	O
ParsiNLU	O
fosters	O
further	O
research	O
and	O
advances	O
in	O
Persian	NLP-focus
language	NLP-focus
understanding	NLP-focus
1	O
.	O

The	O
ability	O
to	O
structure	O
a	O
conversational	O
transcript	O
as	O
a	O
sequence	O
of	O
dialog	O
acts	O
dialog	NLP-focus
act	NLP-focus
recognition	NLP-focus
including	O
the	O
segmentation	NLP-focus
is	O
critical	O
for	O
understanding	O
dialog	O
.	O

We	O
apply	O
two	O
pre	O
-	O
trained	O
transformer	O
models	O
XLNet	NLP-algorithm/tool
and	O
Longformer	NLP-algorithm/tool
to	O
this	O
task	O
in	O
English	O
and	O
achieve	O
strong	O
results	O
on	O
Switchboard	O
Dialog	O
Act	O
and	O
Meeting	O
Recorder	O
Dialog	O
Act	O
corpora	O
with	O
dialog	NLP-metrics
act	NLP-metrics
segmentation	NLP-metrics
error	NLP-metrics
rates	NLP-metrics
(	NLP-metrics
DSER	NLP-metrics
)	NLP-metrics
of	O
8	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
and	O
14	Numerical-result
.	Numerical-result
2	Numerical-result
\\%	Numerical-result
.	O

We	O
apply	O
two	O
pre	O
-	O
trained	O
transformer	O
models	O
XLNet	NLP-algorithm/tool
and	O
Longformer	NLP-algorithm/tool
to	O
this	O
task	O
in	O
English	O
and	O
achieve	O
strong	O
results	O
on	O
Switchboard	O
Dialog	O
Act	O
and	O
Meeting	O
Recorder	O
Dialog	O
Act	O
corpora	O
with	O
dialog	NLP-metrics
act	NLP-metrics
segmentation	NLP-metrics
error	NLP-metrics
rates	NLP-metrics
(	NLP-metrics
DSER	NLP-metrics
)	NLP-metrics
of	O
8	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
and	O
14	Numerical-result
.	Numerical-result
2	Numerical-result
\\%	Numerical-result
.	O

We	O
apply	O
two	O
pre	O
-	O
trained	O
transformer	O
models	O
XLNet	NLP-algorithm/tool
and	O
Longformer	NLP-algorithm/tool
to	O
this	O
task	O
in	O
English	O
and	O
achieve	O
strong	O
results	O
on	O
Switchboard	O
Dialog	O
Act	O
and	O
Meeting	O
Recorder	O
Dialog	O
Act	O
corpora	O
with	O
dialog	NLP-metrics
act	NLP-metrics
segmentation	NLP-metrics
error	NLP-metrics
rates	NLP-metrics
(	NLP-metrics
DSER	NLP-metrics
)	NLP-metrics
of	O
8	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
and	O
14	Numerical-result
.	Numerical-result
2	Numerical-result
\\%	Numerical-result
.	O

To	O
understand	O
the	O
key	O
factors	O
affecting	O
dialog	NLP-focus
act	NLP-focus
recognition	NLP-focus
we	O
perform	O
a	O
comparative	O
analysis	O
of	O
models	O
trained	O
under	O
different	O
conditions	O
.	O

We	O
present	O
a	O
memory	O
-	O
based	O
model	O
for	O
context	NLP-focus
-	NLP-focus
dependent	NLP-focus
semantic	NLP-focus
parsing	NLP-focus
.	O

We	O
evaluate	O
our	O
approach	O
on	O
three	O
semantic	NLP-focus
parsing	NLP-focus
benchmarks	O
.	O

We	O
study	O
controllable	NLP-focus
text	NLP-focus
summarization	NLP-focus
which	O
allows	O
users	O
to	O
gain	O
control	O
on	O
a	O
particular	O
attribute	O
(	O
e	O
.	O
g	O
.,	O
length	O
limit	O
)	O
of	O
the	O
generated	O
summaries	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
novel	O
training	O
framework	O
based	O
on	O
Constrained	O
Markov	O
Decision	O
Process	O
(	O
CMDP	O
)	O
which	O
conveniently	O
includes	O
a	O
reward	O
function	O
along	O
with	O
a	O
set	O
of	O
constraints	O
,	O
to	O
facilitate	O
better	O
summarization	NLP-focus
control	O
.	O

Our	O
framework	O
can	O
be	O
applied	O
to	O
control	O
important	O
attributes	O
of	O
summarization	NLP-focus
including	O
length	O
,	O
covered	O
entities	O
and	O
abstractiveness	O
,	O
as	O
we	O
devise	O
specific	O
constraints	O
for	O
each	O
of	O
these	O
aspects	O
.	O

Free	O
-	O
order	O
case	O
-	O
marking	O
languages	O
such	O
as	O
Russian	O
,	O
Latin	O
,	O
or	O
Tamil	O
,	O
have	O
proved	O
more	O
challenging	O
than	O
fixed	O
-	O
order	O
languages	O
for	O
the	O
tasks	O
of	O
syntactic	NLP-algorithm/tool
parsing	NLP-algorithm/tool
and	O
subject	O
-	O
verb	O
agreement	O
prediction	O
.	O

In	O
this	O
work	O
,	O
we	O
investigate	O
whether	O
this	O
class	O
of	O
languages	O
is	O
also	O
more	O
difficult	O
to	O
translate	O
by	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Neural	NLP-algorithm/tool
Machine	NLP-algorithm/tool
Translation	NLP-algorithm/tool
(	NLP-algorithm/tool
NMT	NLP-algorithm/tool
)	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

Using	O
a	O
variety	O
of	O
synthetic	O
languages	O
and	O
a	O
newly	O
introduced	O
translation	O
challenge	O
set	O
,	O
we	O
find	O
that	O
word	O
order	O
flexibility	O
in	O
the	O
source	O
language	O
only	O
leads	O
to	O
a	O
very	O
small	O
loss	O
of	O
NMT	NLP-focus
quality	O
,	O
even	O
though	O
the	O
core	O
verb	O
arguments	O
become	O
impossible	O
to	O
disambiguate	O
in	O
sentences	O
without	O
semantic	O
cues	O
.	O

However	O
,	O
in	O
medium	O
-	O
and	O
low	O
-	O
resource	O
settings	O
,	O
the	O
overall	O
NMT	NLP-focus
quality	O
of	O
fixed	O
-	O
order	O
languages	O
remains	O
unmatched	O
.	O

To	O
develop	O
commonsense	O
-	O
grounded	O
NLP	O
applications	O
,	O
a	O
comprehensive	O
and	O
accurate	O
commonsense	NLP-algorithm/tool
knowledge	NLP-algorithm/tool
graph	NLP-algorithm/tool
(	NLP-algorithm/tool
CKG	NLP-algorithm/tool
)	NLP-algorithm/tool
is	O
needed	O
.	O

It	O
is	O
time	O
-	O
consuming	O
to	O
manually	O
construct	O
CKGs	NLP-algorithm/tool
CKGs	NLP-algorithm/tool
any	O
research	O
efforts	O
have	O
been	O
devoted	O
to	O
the	O
automatic	O
construction	O
of	O
CKGs	O
.	O

We	O
propose	O
a	O
general	O
graph	O
-	O
to	O
-	O
paths	O
pretraining	O
framework	O
that	O
leverages	O
high	O
-	O
order	O
structures	O
in	O
CKGs	NLP-algorithm/tool
to	O
capture	O
high	O
-	O
order	O
relationships	O
between	O
concepts	O
.	O

In	O
addition	O
,	O
to	O
enforce	O
consistency	O
in	O
the	O
recognized	O
vocabulary	O
,	O
we	O
introduce	O
a	O
lexically	O
aware	O
decoding	O
method	O
that	O
augments	O
the	O
neural	O
post	O
-	O
correction	O
model	O
with	O
a	O
count	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
constructed	O
from	O
the	O
recognized	O
texts	O
,	O
implemented	O
using	O
weighted	O
finite	O
-	O
state	O
automata	O
(	O
WFSA	O
)	O
decoding	O
ient	O
and	O
effective	O
decoding	O
.	O

Results	O
on	O
four	O
endangered	O
languages	O
demonstrate	O
the	O
utility	O
of	O
the	O
proposed	O
method	O
,	O
with	O
relative	O
error	O
reductions	O
of	O
15	Numerical-result
\\%	Numerical-result
29	Numerical-result
\\%	Numerical-result
where	O
we	O
find	O
the	O
combination	O
of	O
self	O
-	O
training	O
and	O
lexically	NLP-algorithm/tool
aware	NLP-algorithm/tool
decoding	NLP-algorithm/tool
essential	O
for	O
achieving	O
consistent	O
improvements	O
.	O
1	O
.	O

Results	O
on	O
four	O
endangered	O
languages	O
demonstrate	O
the	O
utility	O
of	O
the	O
proposed	O
method	O
,	O
with	O
relative	O
error	O
reductions	O
of	O
15	Numerical-result
\\%	Numerical-result
29	Numerical-result
\\%	Numerical-result
where	O
we	O
find	O
the	O
combination	O
of	O
self	O
-	O
training	O
and	O
lexically	NLP-algorithm/tool
aware	NLP-algorithm/tool
decoding	NLP-algorithm/tool
essential	O
for	O
achieving	O
consistent	O
improvements	O
.	O
1	O
.	O

We	O
study	O
continual	O
learning	O
for	O
natural	NLP-focus
language	NLP-focus
instruction	NLP-focus
generation	NLP-focus
by	O
observing	O
human	O
users	O
’	O
instruction	O
execution	O
.	O

We	O
study	O
learning	O
named	NLP-algorithm/tool
entity	NLP-algorithm/tool
recognizers	NLP-algorithm/tool
in	O
the	O
presence	O
of	O
missing	O
entity	O
annotations	O
.	O

(	O
2021	O
)	O
by	O
+	Numerical-result
12	Numerical-result
.	Numerical-result
7	Numerical-result
and	O
+	Numerical-result
2	Numerical-result
.	Numerical-result
3	Numerical-result
F1	Classification-metrics
score	O
in	O
a	O
challenging	O
setting	O
with	O
only	O
1	O
,	O
000	O
biased	O
annotations	O
,	O
averaged	O
across	O
7	O
datasets	O
.	O

(	O
2021	O
)	O
by	O
+	Numerical-result
12	Numerical-result
.	Numerical-result
7	Numerical-result
and	O
+	Numerical-result
2	Numerical-result
.	Numerical-result
3	Numerical-result
F1	Classification-metrics
score	O
in	O
a	O
challenging	O
setting	O
with	O
only	O
1	O
,	O
000	O
biased	O
annotations	O
,	O
averaged	O
across	O
7	O
datasets	O
.	O

We	O
introduce	O
Generative	NLP-focus
Spoken	NLP-focus
Language	NLP-focus
Modeling	NLP-focus
the	O
task	O
of	O
learning	O
the	O
acoustic	O
and	O
linguistic	O
characteristics	O
acoustic	O
uage	O
from	O
raw	O
audio	O
(	O
no	O
text	O
,	O
no	O
labels	O
),	O
and	O
a	O
set	O
of	O
metrics	O
to	O
automatically	O
evaluate	O
the	O
learned	O
representations	O
at	O
acoustic	O
and	O
linguistic	O
levels	O
for	O
both	O
encoding	O
and	O
generation	O
.	O

We	O
set	O
up	O
baseline	O
systems	O
consisting	O
of	O
a	O
discrete	O
speech	NLP-algorithm/tool
encoder	NLP-algorithm/tool
(	O
returning	O
pseudo	O
-	O
text	O
units	O
),	O
a	O
generative	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
(	O
trained	O
on	O
pseudo	O
-	O
text	O
,	O
and	O
a	O
speech	NLP-algorithm/tool
decoder	NLP-algorithm/tool
(	O
generating	O
a	O
waveform	O
pseudo	O
-	O
text	O
trained	O
l	O
trained	O
without	O
supervision	O
and	O
validate	O
the	O
proposed	O
metrics	O
with	O
human	O
evaluation	O
.	O

Across	O
3	O
speech	NLP-algorithm/tool
encoders	NLP-algorithm/tool
(	O
CPC	NLP-algorithm/tool
wav2vec	NLP-algorithm/tool
2	NLP-algorithm/tool
.	NLP-algorithm/tool
0	NLP-algorithm/tool
HuBERT	NLP-algorithm/tool
,	O
we	O
find	O
that	O
the	O
number	O
of	O
discrete	O
units	O
(	O
50	O
,	O
100	O
,	O
or	O
200	O
)	O
matters	O
in	O
a	O
task	O
-	O
dependent	O
and	O
encoder	O
-	O
dependent	O
way	O
,	O
and	O
that	O
some	O
combinations	O
approach	O
text	O
-	O
based	O
systems	O
.	O
1	O
.	O

To	O
understand	O
the	O
connection	O
between	O
model	O
compression	O
and	O
out	O
-	O
of	O
-	O
distribution	O
generalization	O
,	O
we	O
define	O
the	O
task	O
of	O
compressing	O
language	NLP-algorithm/tool
representation	NLP-algorithm/tool
models	NLP-algorithm/tool
such	O
that	O
they	O
perform	O
best	O
in	O
a	O
domain	O
adaptation	O
setting	O
.	O

AMoC	O
outperforms	O
strong	O
baselines	O
on	O
dozens	O
of	O
domain	O
pairs	O
across	O
three	O
text	NLP-focus
classification	NLP-focus
and	O
sequence	NLP-focus
tagging	NLP-focus
tasks	O
.	O
1	O
.	O

We	O
demonstrate	O
that	O
this	O
device	O
is	O
capable	O
of	O
correctly	O
parsing	O
reasonably	O
nontrivial	O
sentences	O
.	O
1	O
While	O
our	O
experiments	O
entail	O
rather	O
simple	O
sentences	O
in	O
English	O
,	O
our	O
results	O
suggest	O
that	O
the	O
parser	NLP-algorithm/tool
can	O
be	O
extended	O
beyond	O
what	O
we	O
have	O
implemented	O
,	O
to	O
several	O
directions	O
encompassing	O
much	O
of	O
language	O
.	O

Progress	O
in	O
cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
modeling	NLP-focus
depends	O
on	O
challenging	O
,	O
realistic	O
,	O
and	O
diverse	O
evaluation	O
sets	O
.	O

We	O
introduce	O
Multilingual	O
Knowledge	O
Questions	O
and	O
Answers	O
(	O
MKQA	O
)	O
an	O
open	NLP-focus
-	NLP-focus
domain	NLP-focus
question	NLP-focus
answering	NLP-focus
evaluation	O
set	O
comprising	O
10k	O
question	O
-	O
answer	O
pairs	O
aligned	O
across	O
26	O
typologically	O
diverse	O
languages	O
(	O
260k	O
question	O
-	O
answer	O
pairs	O
in	O
total	O
).	O

We	O
benchmark	O
a	O
variety	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
and	O
baselines	O
for	O
generative	O
and	O
extractive	NLP-focus
question	NLP-focus
answering	NLP-focus
trained	O
on	O
Natural	O
Questions	O
,	O
in	O
zero	O
shot	O
and	O
translation	O
settings	O
.	O

⚠	O
This	O
paper	O
contains	O
prompts	O
and	O
model	O
outputs	O
that	O
are	O
offensive	O
in	O
nature	O
.	O
When	O
trained	O
on	O
large	O
,	O
unfiltered	O
crawls	O
from	O
the	O
Internet	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
pick	O
up	O
and	O
reproduce	O
all	O
kinds	O
of	O
undesirable	O
biases	O
that	O
can	O
be	O
found	O
in	O
the	O
data	O
:	O
They	O
often	O
generate	O
racist	O
,	O
sexist	O
,	O
violent	O
,	O
or	O
otherwise	O
toxic	O
language	O
.	O

In	O
this	O
paper	O
,	O
we	O
first	O
demonstrate	O
a	O
surprising	O
finding	O
:	O
Pretrained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
recognize	O
,	O
to	O
a	O
considerable	O
degree	O
,	O
their	O
undesirable	O
biases	O
and	O
the	O
toxicity	O
of	O
the	O
content	O
they	O
produce	O
.	O

Based	O
on	O
this	O
finding	O
,	O
we	O
then	O
propose	O
a	O
decoding	O
algorithm	O
that	O
,	O
given	O
only	O
a	O
textual	O
description	O
of	O
the	O
undesired	O
behavior	O
,	O
reduces	O
the	O
probability	O
of	O
a	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
producing	O
problematic	O
text	O
.	O

While	O
we	O
by	O
no	O
means	O
eliminate	O
the	O
issue	O
of	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
generating	O
biased	O
text	O
,	O
we	O
believe	O
our	O
approach	O
to	O
be	O
an	O
important	O
step	O
in	O
this	O
direction	O
.	O
1	O
.	O

Several	O
metrics	O
have	O
been	O
proposed	O
for	O
assessing	O
the	O
similarity	O
of	O
(	NLP-focus
abstract	NLP-focus
)	NLP-focus
meaning	NLP-focus
representations	NLP-focus
(	NLP-focus
AMRs	NLP-focus
)	NLP-focus
but	O
little	O
is	O
known	O
about	O
how	O
they	O
relate	O
to	O
human	O
similarity	O
ratings	O
.	O

e	O
conduct	O
experiments	O
on	O
natural	NLP-focus
language	NLP-focus
inference	NLP-focus
and	O
machine	NLP-focus
translation	NLP-focus
we	O
show	O
that	O
differentiable	O
subset	O
pruning	O
performs	O
comparably	O
or	O
better	O
than	O
previous	O
works	O
while	O
offering	O
precise	O
control	O
of	O
the	O
sparsity	O
level	O
.	O

Human	O
evaluation	O
of	O
modern	O
high	O
-	O
quality	O
machine	NLP-focus
translation	NLP-focus
systems	O
is	O
a	O
difficult	O
problem	O
,	O
and	O
there	O
is	O
increasing	O
evidence	O
that	O
inadequate	O
evaluation	O
procedures	O
can	O
lead	O
to	O
erroneous	O
conclusions	O
.	O

As	O
a	O
step	O
toward	O
this	O
goal	O
,	O
we	O
propose	O
an	O
evaluation	O
methodology	O
grounded	O
in	O
explicit	O
error	O
analysis	O
,	O
based	O
on	O
the	O
Multidimensional	NLP-algorithm/tool
Quality	NLP-algorithm/tool
Metrics	NLP-algorithm/tool
(	NLP-algorithm/tool
MQM	NLP-algorithm/tool
)	NLP-algorithm/tool
framework	O
.	O

We	O
carry	O
out	O
the	O
largest	O
MQM	NLP-algorithm/tool
research	O
study	O
to	O
date	O
,	O
scoring	O
the	O
outputs	O
of	O
top	O
systems	O
from	O
the	O
WMT	O
2020	O
shared	O
task	O
in	O
two	O
language	O
pairs	O
using	O
annotations	O
provided	O
by	O
professional	O
translators	O
with	O
access	O
to	O
full	O
document	O
context	O
.	O

We	O
develop	O
neural	O
models	O
that	O
possess	O
an	O
interpretable	O
inference	O
process	O
for	O
dependency	NLP-focus
parsing	NLP-focus
.	O

Debugging	O
a	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
model	O
is	O
hard	O
since	O
the	O
bug	O
usually	O
involves	O
the	O
training	O
data	O
and	O
the	O
learning	O
process	O
.	O

We	O
call	O
this	O
problem	O
explanation	NLP-focus
-	NLP-focus
based	NLP-focus
human	NLP-focus
debugging	NLP-focus
(	NLP-focus
EBHD	NLP-focus
)	NLP-focus
.	O

In	O
particular	O
,	O
we	O
categorize	O
and	O
discuss	O
existing	O
work	O
along	O
three	O
dimensions	O
of	O
EBHD	NLP-focus
EBHD	NLP-focus
bug	O
context	O
,	O
the	O
workflow	O
,	O
and	O
the	O
experimental	O
setting	O
),	O
compile	O
findings	O
on	O
how	O
EBHD	O
components	O
affect	O
the	O
feedback	O
providers	O
,	O
and	O
highlight	O
open	O
problems	O
that	O
could	O
be	O
future	O
research	O
directions	O
.	O

Further	O
diachronic	NLP-algorithm/tool
analysis	NLP-algorithm/tool
reveals	O
that	O
declining	O
words	O
tend	O
to	O
decrease	O
in	O
the	O
diversity	O
of	O
their	O
lexical	O
contexts	O
over	O
time	O
,	O
gradually	O
narrowing	O
their	O
‘	O
ecological	O
niches	O
’.	O

A	O
salient	O
feature	O
of	O
the	O
model	O
is	O
its	O
ability	O
to	O
identify	O
idioms	O
unseen	O
during	O
training	O
with	O
gains	O
from	O
1	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
to	O
30	Numerical-result
.	Numerical-result
8	Numerical-result
\\%	Numerical-result
over	O
competitive	O
baselines	O
on	O
the	O
largest	O
dataset	O
.	O

We	O
present	O
methods	O
for	O
calculating	O
a	O
measure	O
of	O
phonotactic	NLP-focus
complexity	NLP-focus
bits	O
per	O
phoneme	O
that	O
permits	O
a	O
straightforward	O
cross	O
-	O
linguistic	O
comparison	O
.	O

Abstract	NLP-focus
meaning	NLP-focus
representation	NLP-focus
(	NLP-focus
AMR	NLP-focus
)-	NLP-focus
to	NLP-focus
-	NLP-focus
text	NLP-focus
generation	NLP-focus
is	O
the	O
challenging	O
task	O
of	O
generating	O
natural	O
language	O
texts	O
from	O
AMR	NLP-algorithm/tool
graphs	NLP-algorithm/tool
where	O
nodes	O
represent	O
concepts	O
and	O
edges	O
denote	O
relations	O
.	O

Abstract	NLP-focus
meaning	NLP-focus
representation	NLP-focus
(	NLP-focus
AMR	NLP-focus
)-	NLP-focus
to	NLP-focus
-	NLP-focus
text	NLP-focus
generation	NLP-focus
is	O
the	O
challenging	O
task	O
of	O
generating	O
natural	O
language	O
texts	O
from	O
AMR	NLP-algorithm/tool
graphs	NLP-algorithm/tool
where	O
nodes	O
represent	O
concepts	O
and	O
edges	O
denote	O
relations	O
.	O

The	O
model	O
directly	O
encodes	O
the	O
AMR	NLP-algorithm/tool
graphs	NLP-algorithm/tool
and	O
learns	O
the	O
node	O
representations	O
.	O

Our	O
model	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
approach	O
by	O
1	Numerical-result
.	Numerical-result
5	Numerical-result
BLEU	NLP-metrics
points	O
on	O
LDC2015E86	O
and	O
4	Numerical-result
.	Numerical-result
8	Numerical-result
BLEU	NLP-metrics
points	O
on	O
LDC2017T10	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
.	O

Our	O
model	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
approach	O
by	O
1	Numerical-result
.	Numerical-result
5	Numerical-result
BLEU	NLP-metrics
points	O
on	O
LDC2015E86	O
and	O
4	Numerical-result
.	Numerical-result
8	Numerical-result
BLEU	NLP-metrics
points	O
on	O
LDC2017T10	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
.	O

Pre	O
-	O
training	O
by	O
language	NLP-focus
modeling	NLP-focus
has	O
become	O
a	O
popular	O
and	O
successful	O
approach	O
to	O
NLP	O
tasks	O
,	O
but	O
we	O
have	O
yet	O
to	O
understand	O
exactly	O
what	O
linguistic	O
capacities	O
these	O
pre	O
-	O
training	O
processes	O
confer	O
upon	O
models	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
a	O
suite	O
of	O
diagnostics	O
drawn	O
from	O
human	O
language	O
experiments	O
,	O
which	O
allow	O
us	O
to	O
ask	O
targeted	O
questions	O
about	O
information	O
used	O
by	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
for	O
generating	O
predictions	O
in	O
context	O
.	O

As	O
a	O
case	O
study	O
,	O
we	O
apply	O
these	O
diagnostics	O
to	O
the	O
popular	O
BERT	NLP-algorithm/tool
model	NLP-algorithm/tool
finding	O
that	O
it	O
can	O
generally	O
distinguish	O
good	O
from	O
bad	O
completions	O
involving	O
shared	O
category	O
or	O
role	O
reversal	O
,	O
albeit	O
with	O
less	O
sensitivity	O
than	O
humans	O
,	O
and	O
it	O
robustly	O
retrieves	O
noun	O
hypernyms	O
but	O
it	O
struggles	O
with	O
challenging	O
inference	O
and	O
role	O
-	O
based	O
event	O
prediction	O
—	O
and	O
,	O
in	O
particular	O
,	O
it	O
shows	O
clear	O
insensitivity	O
to	O
the	O
contextual	O
impacts	O
of	O
negation	O
.	O

Data	O
privacy	O
is	O
an	O
important	O
issue	O
for	O
“	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
as	O
a	O
service	O
”	O
providers	O
.	O

Our	O
contribution	O
is	O
an	O
investigation	O
of	O
this	O
problem	O
in	O
the	O
context	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
which	O
are	O
important	O
in	O
applications	O
such	O
as	O
machine	NLP-focus
translation	NLP-focus
and	O
video	O
captioning	O
.	O

We	O
define	O
the	O
membership	O
inference	O
problem	O
for	O
sequence	O
generation	O
provide	O
an	O
open	O
dataset	O
based	O
on	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
machine	NLP-algorithm/tool
translation	NLP-algorithm/tool
models	NLP-algorithm/tool
and	O
report	O
initial	O
results	O
on	O
whether	O
these	O
models	O
leak	O
private	O
information	O
against	O
several	O
kinds	O
of	O
membership	O
inference	O
attacks	O
.	O

Our	O
approach	O
extends	O
BERT	NLP-algorithm/tool
by	O
(	O
1	O
)	O
masking	O
contiguous	O
random	O
spans	O
,	O
rather	O
than	O
random	O
tokens	O
,	O
and	O
(	O
2	O
)	O
training	O
the	O
span	O
boundary	O
representations	O
to	O
predict	O
the	O
entire	O
content	O
of	O
the	O
masked	O
span	O
,	O
without	O
relying	O
on	O
the	O
individual	O
token	O
representations	O
within	O
it	O
.	O

SpanBERT	O
BERT	NLP-algorithm/tool
stently	O
outperforms	O
BERT	O
and	O
our	O
better	O
-	O
tuned	O
baselines	O
,	O
with	O
substantial	O
gains	O
on	O
span	NLP-focus
selection	NLP-focus
tasks	O
such	O
as	O
question	NLP-focus
answering	NLP-focus
and	O
coreference	NLP-focus
resolution	NLP-focus
.	O

SpanBERT	O
BERT	NLP-algorithm/tool
stently	O
outperforms	O
BERT	O
and	O
our	O
better	O
-	O
tuned	O
baselines	O
,	O
with	O
substantial	O
gains	O
on	O
span	NLP-focus
selection	NLP-focus
tasks	O
such	O
as	O
question	NLP-focus
answering	NLP-focus
and	O
coreference	NLP-focus
resolution	NLP-focus
.	O

In	O
particular	O
,	O
with	O
the	O
same	O
training	O
data	O
and	O
model	O
size	O
as	O
BERTlarge	NLP-algorithm/tool
our	O
single	O
model	O
obtains	O
94	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
and	O
88	Numerical-result
.	Numerical-result
7	Numerical-result
\\%	Numerical-result
F1	Classification-metrics
on	O
SQuAD	O
1	O
.	O
1	O
and	O
2	O
.	O
0	O
respectively	O
.	O

In	O
particular	O
,	O
with	O
the	O
same	O
training	O
data	O
and	O
model	O
size	O
as	O
BERTlarge	NLP-algorithm/tool
our	O
single	O
model	O
obtains	O
94	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
and	O
88	Numerical-result
.	Numerical-result
7	Numerical-result
\\%	Numerical-result
F1	Classification-metrics
on	O
SQuAD	O
1	O
.	O
1	O
and	O
2	O
.	O
0	O
respectively	O
.	O

In	O
particular	O
,	O
with	O
the	O
same	O
training	O
data	O
and	O
model	O
size	O
as	O
BERTlarge	NLP-algorithm/tool
our	O
single	O
model	O
obtains	O
94	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
and	O
88	Numerical-result
.	Numerical-result
7	Numerical-result
\\%	Numerical-result
F1	Classification-metrics
on	O
SQuAD	O
1	O
.	O
1	O
and	O
2	O
.	O
0	O
respectively	O
.	O

We	O
also	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art	O
on	O
the	O
OntoNotes	O
coreference	NLP-focus
resolution	NLP-focus
task	O
(	O
79	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
F1	Classification-metrics
,	O
strong	O
performance	O
on	O
the	O
TACRED	O
relation	NLP-focus
extraction	NLP-focus
benchmark	O
,	O
and	O
even	O
gains	O
on	O
GLUE	O
.	O

We	O
also	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art	O
on	O
the	O
OntoNotes	O
coreference	NLP-focus
resolution	NLP-focus
task	O
(	O
79	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
F1	Classification-metrics
,	O
strong	O
performance	O
on	O
the	O
TACRED	O
relation	NLP-focus
extraction	NLP-focus
benchmark	O
,	O
and	O
even	O
gains	O
on	O
GLUE	O
.	O

We	O
also	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art	O
on	O
the	O
OntoNotes	O
coreference	NLP-focus
resolution	NLP-focus
task	O
(	O
79	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
F1	Classification-metrics
,	O
strong	O
performance	O
on	O
the	O
TACRED	O
relation	NLP-focus
extraction	NLP-focus
benchmark	O
,	O
and	O
even	O
gains	O
on	O
GLUE	O
.	O

Chinese	NLP-focus
word	NLP-focus
segmentation	NLP-focus
and	O
dependency	NLP-focus
parsing	NLP-focus
are	O
two	O
fundamental	O
tasks	O
for	O
Chinese	O
natural	O
language	O
processing	O
.	O

The	O
dependency	NLP-focus
parsing	NLP-focus
is	O
defined	O
at	O
the	O
word	O
-	O
level	O
.	O

Therefore	O
word	NLP-algorithm/tool
segmentation	NLP-algorithm/tool
is	O
the	O
precondition	O
of	O
dependency	NLP-focus
parsing	NLP-focus
dependency	NLP-focus
parsing	NLP-focus
ency	O
parsing	O
suffer	O
from	O
error	O
propagation	O
and	O
unable	O
to	O
directly	O
make	O
use	O
of	O
character	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	O
such	O
as	O
BERT	NLP-algorithm/tool
.	O

Therefore	O
word	NLP-algorithm/tool
segmentation	NLP-algorithm/tool
is	O
the	O
precondition	O
of	O
dependency	NLP-focus
parsing	NLP-focus
dependency	NLP-focus
parsing	NLP-focus
ency	O
parsing	O
suffer	O
from	O
error	O
propagation	O
and	O
unable	O
to	O
directly	O
make	O
use	O
of	O
character	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	O
such	O
as	O
BERT	NLP-algorithm/tool
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
graph	O
-	O
based	O
model	O
to	O
integrate	O
Chinese	NLP-focus
word	NLP-focus
segmentation	NLP-focus
and	O
dependency	NLP-focus
parsing	NLP-focus
.	O

Different	O
from	O
previous	O
transition	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
joint	NLP-algorithm/tool
models	NLP-algorithm/tool
our	O
proposed	O
model	O
is	O
more	O
concise	O
,	O
which	O
results	O
in	O
fewer	O
efforts	O
of	O
feature	O
engineering	O
.	O

Our	O
graph	O
-	O
based	O
joint	O
model	O
achieves	O
better	O
performance	O
than	O
previous	O
joint	O
models	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
both	O
Chinese	NLP-focus
word	NLP-focus
segmentation	NLP-focus
and	O
dependency	NLP-focus
parsing	NLP-focus
.	O

Additionally	O
,	O
when	O
BERT	NLP-algorithm/tool
is	O
combined	O
,	O
our	O
model	O
can	O
substantially	O
reduce	O
the	O
performance	O
gap	O
of	O
dependency	NLP-focus
parsing	NLP-focus
between	O
joint	O
models	O
and	O
gold	NLP-algorithm/tool
-	NLP-algorithm/tool
segmented	NLP-algorithm/tool
word	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
models	NLP-algorithm/tool
code	O
.	O

Additionally	O
,	O
when	O
BERT	NLP-algorithm/tool
is	O
combined	O
,	O
our	O
model	O
can	O
substantially	O
reduce	O
the	O
performance	O
gap	O
of	O
dependency	NLP-focus
parsing	NLP-focus
between	O
joint	O
models	O
and	O
gold	NLP-algorithm/tool
-	NLP-algorithm/tool
segmented	NLP-algorithm/tool
word	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
models	NLP-algorithm/tool
code	O
.	O

Story	NLP-focus
generation	NLP-focus
namely	O
,	O
generating	O
a	O
reasonable	O
story	O
from	O
a	O
leading	O
context	O
is	O
an	O
important	O
but	O
challenging	O
task	O
.	O

In	O
spite	O
of	O
the	O
success	O
in	O
modeling	O
fluency	O
and	O
local	O
coherence	O
,	O
existing	O
neural	NLP-algorithm/tool
language	NLP-algorithm/tool
generation	NLP-algorithm/tool
models	NLP-algorithm/tool
(	O
e	O
.	O
g	O
.,	O
GPT	NLP-algorithm/tool
-	NLP-algorithm/tool
2	NLP-algorithm/tool
still	O
suffer	O
from	O
repetition	O
,	O
logic	O
conflicts	O
,	O
and	O
lack	O
of	O
long	O
-	O
range	O
coherence	O
in	O
generated	O
stories	O
.	O

In	O
this	O
paper	O
,	O
we	O
devise	O
a	O
knowledge	O
-	O
enhanced	O
pretraining	O
model	O
for	O
commonsense	NLP-focus
story	NLP-focus
generation	NLP-focus
.	O

Cross	NLP-focus
-	NLP-focus
lingual	NLP-focus
entity	NLP-focus
linking	NLP-focus
(	NLP-focus
XEL	NLP-focus
)	NLP-focus
is	O
the	O
task	O
of	O
finding	O
referents	O
in	O
a	O
target	O
-	O
language	O
knowledge	O
base	O
(	O
KB	O
)	O
for	O
mentions	O
extracted	O
from	O
source	O
-	O
language	O
texts	O
.	O

The	O
first	O
step	O
of	O
(	NLP-focus
X	NLP-focus
)	NLP-focus
EL	NLP-focus
is	O
candidate	O
generation	O
,	O
which	O
retrieves	O
a	O
list	O
of	O
plausible	O
candidate	O
entities	O
from	O
the	O
target	O
-	O
language	O
KB	O
for	O
each	O
mention	O
.	O

In	O
this	O
paper	O
,	O
we	O
first	O
assess	O
the	O
problems	O
faced	O
by	O
current	O
entity	NLP-focus
candidate	NLP-focus
generation	NLP-focus
methods	O
for	O
low	NLP-focus
-	NLP-focus
resource	NLP-focus
XEL	NLP-focus
then	O
propose	O
three	O
improvements	O
that	O
(	O
1	O
)	O
reduce	O
the	O
disconnect	O
between	O
entity	O
mentions	O
and	O
KB	O
entries	O
and	O
(	O
2	O
)	O
improve	O
the	O
robustness	O
of	O
the	O
model	O
to	O
low	O
-	O
resource	O
scenarios	O
.	O

The	O
methods	O
are	O
simple	O
,	O
but	O
effective	O
:	O
We	O
experiment	O
with	O
our	O
approach	O
on	O
seven	O
XEL	NLP-focus
datasets	O
and	O
find	O
that	O
they	O
yield	O
an	O
average	O
gain	O
of	O
16	Numerical-result
.	Numerical-result
9	Numerical-result
\\%	Numerical-result
in	O
Top	O
-	O
30	O
gold	O
candidate	O
recall	Classification-metrics
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
.	O

The	O
methods	O
are	O
simple	O
,	O
but	O
effective	O
:	O
We	O
experiment	O
with	O
our	O
approach	O
on	O
seven	O
XEL	NLP-focus
datasets	O
and	O
find	O
that	O
they	O
yield	O
an	O
average	O
gain	O
of	O
16	Numerical-result
.	Numerical-result
9	Numerical-result
\\%	Numerical-result
in	O
Top	O
-	O
30	O
gold	O
candidate	O
recall	Classification-metrics
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
.	O

The	O
methods	O
are	O
simple	O
,	O
but	O
effective	O
:	O
We	O
experiment	O
with	O
our	O
approach	O
on	O
seven	O
XEL	NLP-focus
datasets	O
and	O
find	O
that	O
they	O
yield	O
an	O
average	O
gain	O
of	O
16	Numerical-result
.	Numerical-result
9	Numerical-result
\\%	Numerical-result
in	O
Top	O
-	O
30	O
gold	O
candidate	O
recall	Classification-metrics
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
.	O

Our	O
improved	O
model	O
also	O
yields	O
an	O
average	O
gain	O
of	O
7	Numerical-result
.	Numerical-result
9	Numerical-result
\\%	Numerical-result
in	O
in	Classification-metrics
-	Classification-metrics
KB	Classification-metrics
accuracy	Classification-metrics
of	O
end	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
end	NLP-focus
XEL	NLP-focus
1	O
.	O

Our	O
improved	O
model	O
also	O
yields	O
an	O
average	O
gain	O
of	O
7	Numerical-result
.	Numerical-result
9	Numerical-result
\\%	Numerical-result
in	O
in	Classification-metrics
-	Classification-metrics
KB	Classification-metrics
accuracy	Classification-metrics
of	O
end	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
end	NLP-focus
XEL	NLP-focus
1	O
.	O

Our	O
improved	O
model	O
also	O
yields	O
an	O
average	O
gain	O
of	O
7	Numerical-result
.	Numerical-result
9	Numerical-result
\\%	Numerical-result
in	O
in	Classification-metrics
-	Classification-metrics
KB	Classification-metrics
accuracy	Classification-metrics
of	O
end	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
end	NLP-focus
XEL	NLP-focus
1	O
.	O

We	O
investigate	O
which	O
architectural	O
factors	O
affect	O
the	O
generalization	O
behavior	O
of	O
neural	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
trained	O
on	O
two	O
syntactic	O
tasks	O
,	O
English	NLP-focus
question	NLP-focus
formation	NLP-focus
and	O
English	NLP-focus
tense	NLP-focus
reinflection	NLP-focus
.	O

However	O
,	O
the	O
only	O
factor	O
that	O
consistently	O
contributed	O
a	O
hierarchical	O
bias	O
across	O
tasks	O
was	O
the	O
use	O
of	O
a	O
tree	O
-	O
structured	O
model	O
rather	O
than	O
a	O
model	O
with	O
sequential	O
recurrence	O
suggesting	O
that	O
human	O
-	O
like	O
syntactic	NLP-focus
generalization	NLP-focus
requires	O
architectural	O
syntactic	O
structure	O
.	O

Machine	NLP-focus
reading	NLP-focus
comprehension	NLP-focus
tasks	O
require	O
a	O
machine	O
reader	O
to	O
answer	O
questions	O
relevant	O
to	O
the	O
given	O
document	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
the	O
first	O
free	O
-	O
form	O
multiple	NLP-focus
-	NLP-focus
Choice	NLP-focus
Chinese	NLP-focus
machine	NLP-focus
reading	NLP-focus
Comprehension	NLP-focus
dataset	O
(	O
C3	O
,	O
containing	O
13	O
,	O
369	O
documents	O
(	O
dialogues	O
or	O
more	O
formally	O
written	O
mixed	O
-	O
genre	O
texts	O
)	O
and	O
their	O
associated	O
19	O
,	O
577	O
multiple	O
-	O
choice	O
free	O
-	O
form	O
questions	O
collected	O
from	O
Chinese	O
-	O
as	O
-	O
a	O
-	O
second	O
-	O
language	O
examinations	O
.	O

We	O
implement	O
rule	O
-	O
based	O
and	O
popular	O
neural	O
methods	O
and	O
find	O
that	O
there	O
is	O
still	O
a	O
significant	O
performance	O
gap	O
between	O
the	O
best	O
performing	O
model	O
(	O
68	Numerical-result
.	Numerical-result
5	Numerical-result
\\%	Numerical-result
and	O
human	O
readers	O
(	O
96	Numerical-result
.	Numerical-result
0	Numerical-result
\\%	Numerical-result
,	O
especiallyon	O
problems	O
that	O
require	O
prior	O
knowledge	O
.	O

We	O
further	O
study	O
the	O
effects	O
of	O
distractor	O
plausibility	O
and	O
data	NLP-algorithm/tool
augmentation	NLP-algorithm/tool
based	O
on	O
translated	O
relevant	O
datasets	O
for	O
English	O
on	O
model	O
performance	O
.	O

Target	NLP-focus
-	NLP-focus
dependent	NLP-focus
sentiment	NLP-focus
analysis	NLP-focus
(	NLP-focus
TDSA	NLP-focus
)	NLP-focus
aims	O
to	O
classify	O
the	O
sentiment	O
of	O
a	O
text	O
towards	O
a	O
given	O
target	O
.	O

This	O
paper	O
proposes	O
a	O
novel	O
Target	O
-	O
Guided	O
Structured	O
Attention	O
Network	O
(	O
TG	O
-	O
SAN	O
)	O
which	O
captures	O
target	O
-	O
related	O
contexts	O
for	O
TDSA	NLP-focus
in	O
a	O
fine	O
-	O
to	O
-	O
coarse	O
manner	O
.	O

It	O
then	O
fuses	O
the	O
extracted	O
segments	O
based	O
on	O
their	O
relatedness	O
with	O
the	O
target	O
for	O
sentiment	NLP-focus
classification	NLP-focus
.	O

First	O
,	O
TG	O
-	O
SAN	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
up	O
to	O
1	Numerical-result
.	Numerical-result
61	Numerical-result
\\%	Numerical-result
and	O
3	Numerical-result
.	Numerical-result
58	Numerical-result
\\%	Numerical-result
in	O
terms	O
of	O
accuracy	O
and	O
Marco	Classification-metrics
-	Classification-metrics
F1	Classification-metrics
respectively	O
.	O

First	O
,	O
TG	O
-	O
SAN	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
up	O
to	O
1	Numerical-result
.	Numerical-result
61	Numerical-result
\\%	Numerical-result
and	O
3	Numerical-result
.	Numerical-result
58	Numerical-result
\\%	Numerical-result
in	O
terms	O
of	O
accuracy	O
and	O
Marco	Classification-metrics
-	Classification-metrics
F1	Classification-metrics
respectively	O
.	O

We	O
demonstrate	O
the	O
utility	O
of	O
QDMR	O
by	O
showing	O
that	O
(	O
a	O
)	O
it	O
can	O
be	O
used	O
to	O
improve	O
open	NLP-focus
-	NLP-focus
domain	NLP-focus
question	NLP-focus
answering	NLP-focus
on	O
the	O
HotpotQA	O
dataset	O
,	O
(	O
b	O
)	O
it	O
can	O
be	O
deterministically	O
converted	O
to	O
a	O
pseudo	O
-	O
SQL	O
formal	O
language	O
which	O
can	O
alleviate	O
annotation	O
in	O
semantic	NLP-focus
parsing	NLP-focus
applications	O
.	O

With	O
this	O
data	O
we	O
built	O
classifiers	O
to	O
automatically	O
distinguish	O
trusted	O
from	O
mistrusted	O
speech	O
,	O
achieving	O
an	O
F1	Classification-metrics
of	O
66	Numerical-result
.	Numerical-result
1	Numerical-result
\\%	Numerical-result
.	O

With	O
this	O
data	O
we	O
built	O
classifiers	O
to	O
automatically	O
distinguish	O
trusted	O
from	O
mistrusted	O
speech	O
,	O
achieving	O
an	O
F1	Classification-metrics
of	O
66	Numerical-result
.	Numerical-result
1	Numerical-result
\\%	Numerical-result
.	O

Our	O
work	O
sheds	O
light	O
on	O
the	O
nature	O
of	O
trusted	O
language	O
and	O
provides	O
insight	O
into	O
the	O
challenging	O
problem	O
of	O
human	NLP-focus
deception	NLP-focus
detection	NLP-focus
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
an	O
unsupervised	NLP-algorithm/tool
discourse	NLP-algorithm/tool
constituency	NLP-algorithm/tool
parsing	NLP-algorithm/tool
algorithm	O
.	O

We	O
use	O
Viterbi	O
EM	O
with	O
a	O
margin	O
-	O
based	O
criterion	O
to	O
train	O
a	O
span	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
discourse	NLP-algorithm/tool
parser	NLP-algorithm/tool
in	O
an	O
unsupervised	O
manner	O
.	O

Experimental	O
results	O
demonstrate	O
that	O
our	O
unsupervised	NLP-algorithm/tool
parser	NLP-algorithm/tool
achieves	O
comparable	O
or	O
even	O
superior	O
performance	O
to	O
fully	O
supervised	O
parsers	O
.	O

Recent	O
years	O
have	O
seen	O
a	O
growing	O
interest	O
within	O
the	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
community	O
in	O
evaluating	O
the	O
ability	O
of	O
semantic	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
capture	O
human	O
meaning	O
representation	O
in	O
the	O
brain	O
.	O

Existing	O
research	O
has	O
mainly	O
focused	O
on	O
applying	O
semantic	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
decode	O
brain	O
activity	O
patterns	O
associated	O
with	O
the	O
meaning	O
of	O
individual	O
words	O
,	O
and	O
,	O
more	O
recently	O
,	O
this	O
approach	O
has	O
been	O
extended	O
to	O
sentences	O
and	O
larger	O
text	O
fragments	O
.	O

We	O
evaluate	O
a	O
range	O
of	O
semantic	NLP-algorithm/tool
models	NLP-algorithm/tool
(	O
word	O
embeddings	O
compositional	O
,	O
and	O
visual	O
models	O
in	O
their	O
ability	O
to	O
decode	O
brain	O
activity	O
associated	O
with	O
reading	O
of	O
both	O
literal	O
and	O
metaphoric	O
sentences	O
.	O

Our	O
results	O
suggest	O
that	O
compositional	O
models	O
and	O
word	O
embedding	O
are	O
able	O
to	O
capture	O
differences	O
in	O
the	O
processing	O
of	O
literal	O
and	O
metaphoric	O
sentences	O
,	O
providing	O
support	O
for	O
the	O
idea	O
that	O
the	O
literal	O
meaning	O
is	O
not	O
fully	O
accessible	O
during	O
familiar	O
metaphor	NLP-focus
comprehension	NLP-focus
.	O

We	O
describe	O
a	O
method	O
for	O
rapidly	O
creating	O
language	NLP-focus
proficiency	NLP-focus
assessments	NLP-focus
and	O
provide	O
experimental	O
evidence	O
that	O
such	O
tests	O
can	O
be	O
valid	O
,	O
reliable	O
,	O
and	O
secure	O
.	O

Our	O
approach	O
is	O
the	O
first	O
to	O
use	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
and	O
natural	O
language	O
processing	O
to	O
induce	O
proficiency	O
scales	O
based	O
on	O
a	O
given	O
standard	O
,	O
and	O
then	O
use	O
linguistic	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
estimate	O
item	O
difficulty	O
directly	O
for	O
computer	O
-	O
adaptive	O
testing	O
.	O

Our	O
approach	O
is	O
the	O
first	O
to	O
use	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
and	O
natural	O
language	O
processing	O
to	O
induce	O
proficiency	O
scales	O
based	O
on	O
a	O
given	O
standard	O
,	O
and	O
then	O
use	O
linguistic	NLP-algorithm/tool
models	NLP-algorithm/tool
to	O
estimate	O
item	O
difficulty	O
directly	O
for	O
computer	O
-	O
adaptive	O
testing	O
.	O

We	O
developed	O
a	O
Transformer	O
based	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
that	O
is	O
compatible	O
with	O
publicly	O
available	O
pre	O
-	O
trained	O
BERT	NLP-algorithm/tool
GPT	NLP-algorithm/tool
-	NLP-algorithm/tool
2	NLP-algorithm/tool
and	O
RoBERTa	NLP-algorithm/tool
checkpoints	O
and	O
conducted	O
an	O
extensive	O
empirical	O
study	O
on	O
the	O
utility	O
of	O
initializing	O
our	O
model	O
,	O
both	O
encoder	O
and	O
decoder	O
with	O
these	O
checkpoints	O
.	O

Our	O
models	O
result	O
in	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
Machine	NLP-focus
Translation	NLP-focus
Text	NLP-focus
Summarization	NLP-focus
Sentence	NLP-focus
Splitting	NLP-focus
and	O
Sentence	NLP-focus
Fusion	NLP-focus
.	O

To	O
advance	O
multi	NLP-focus
-	NLP-focus
domain	NLP-focus
(	NLP-focus
cross	NLP-focus
-	NLP-focus
domain	NLP-focus
)	NLP-focus
dialogue	NLP-focus
modeling	NLP-focus
as	O
well	O
as	O
alleviate	O
the	O
shortage	O
of	O
Chinese	O
task	O
-	O
oriented	O
datasets	O
we	O
propose	O
CrossWOZ	O
the	O
first	O
large	O
-	O
scale	O
Chinese	O
Cross	O
-	O
Domain	O
Wizard	O
-	O
of	O
-	O
Oz	O
task	O
-	O
oriented	O
dataset	O
.	O

We	O
also	O
provide	O
a	O
user	O
simulator	O
and	O
several	O
benchmark	O
models	O
for	O
pipelined	O
task	NLP-algorithm/tool
-	NLP-algorithm/tool
oriented	NLP-algorithm/tool
dialogue	NLP-algorithm/tool
systems	NLP-algorithm/tool
which	O
will	O
facilitate	O
researchers	O
to	O
compare	O
and	O
evaluate	O
their	O
models	O
on	O
this	O
corpus	O
.	O

The	O
large	O
size	O
and	O
rich	O
annotation	O
of	O
CrossWOZ	O
make	O
it	O
suitable	O
to	O
investigate	O
a	O
variety	O
of	O
tasks	O
in	O
cross	NLP-focus
-	NLP-focus
domain	NLP-focus
dialogue	NLP-focus
modeling	NLP-focus
such	O
as	O
dialogue	O
state	O
tracking	O
,	O
policy	O
learning	O
,	O
user	O
simulation	O
,	O
etc	O
.	O

We	O
study	O
the	O
influence	O
of	O
context	O
on	O
sentence	NLP-focus
acceptability	NLP-focus
.	O

Next	O
,	O
we	O
test	O
unidirectional	O
and	O
bidirectional	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
in	O
their	O
ability	O
to	O
predict	O
acceptability	O
ratings	O
.	O

In	O
a	O
suite	O
of	O
intrinsic	O
benchmarks	O
,	O
we	O
show	O
that	O
our	O
model	O
outperforms	O
previous	O
approaches	O
on	O
relatedness	O
tasks	O
and	O
on	O
hypernymy	NLP-focus
classification	NLP-focus
and	NLP-focus
detection	NLP-focus
while	O
being	O
competitive	O
on	O
word	NLP-focus
similarity	NLP-focus
tasks	O
.	O

Going	O
beyond	O
such	O
simple	O
constraints	O
,	O
recent	O
work	O
has	O
started	O
exploring	O
the	O
incorporation	O
of	O
complex	O
syntactic	O
-	O
guidance	O
as	O
constraints	O
in	O
the	O
task	O
of	O
controlled	NLP-focus
paraphrase	NLP-focus
generation	NLP-focus
.	O

However	O
,	O
these	O
prior	O
works	O
have	O
only	O
utilized	O
limited	O
syntactic	O
information	O
available	O
in	O
the	O
parse	NLP-algorithm/tool
tree	NLP-algorithm/tool
of	O
the	O
exemplar	O
sentence	O
.	O

We	O
address	O
this	O
limitation	O
in	O
the	O
paper	O
and	O
propose	O
Syntax	O
Guided	O
Controlled	O
Paraphraser	O
(	O
SGCP	O
,	O
an	O
end	O
-	O
to	O
-	O
end	O
framework	O
for	O
syntactic	NLP-focus
paraphrase	NLP-focus
generation	NLP-focus
.	O

We	O
show	O
that	O
Bayes	O
’	O
rule	O
provides	O
an	O
effective	O
mechanism	O
for	O
creating	O
document	NLP-algorithm/tool
translation	NLP-algorithm/tool
models	NLP-algorithm/tool
that	O
can	O
be	O
learned	O
from	O
only	O
parallel	O
sentences	O
and	O
monolingual	O
documents	O
a	O
compelling	O
benefit	O
because	O
parallel	O
documents	O
are	O
not	O
always	O
available	O
.	O

Our	O
proposed	O
model	O
uses	O
a	O
powerful	O
autoregressive	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
as	O
the	O
prior	O
on	O
target	O
language	O
documents	O
,	O
but	O
it	O
assumes	O
that	O
each	O
sentence	O
is	O
translated	O
independently	O
from	O
the	O
target	O
to	O
the	O
source	O
language	O
.	O

Experiments	O
show	O
that	O
our	O
model	O
benefits	O
from	O
using	O
cross	O
-	O
sentence	O
context	O
in	O
the	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
and	O
it	O
outperforms	O
existing	O
document	O
translation	O
approaches	O
.	O

We	O
introduce	O
The	O
Benchmark	O
of	O
Linguistic	O
Minimal	O
Pairs	O
(	O
BLiMP	O
)	O
1	O
a	O
challenge	O
set	O
for	O
evaluating	O
the	O
linguistic	O
knowledge	O
of	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
BLiMP	O
or	O
grammatical	O
phenomena	O
in	O
English	O
.	O

We	O
generate	O
the	O
data	O
according	O
to	O
linguist	O
-	O
crafted	O
grammar	O
templates	O
,	O
and	O
human	O
aggregate	O
agreement	O
with	O
the	O
labels	O
is	O
96	Numerical-result
.	Numerical-result
4	Numerical-result
\\%	Numerical-result
.	O

We	O
evaluate	O
n	O
-	O
gram	O
LSTM	O
and	O
Transformer	O
(	O
GPT	O
-	O
2	O
and	O
Transformer	O
-	O
XL	O
LMs	NLP-algorithm/tool
by	O
observing	O
whether	O
they	O
assign	O
a	O
higher	O
probability	O
to	O
the	O
acceptable	O
sentence	O
in	O
each	O
minimal	O
pair	O
.	O

Hyperparameter	O
selection	O
is	O
a	O
crucial	O
part	O
of	O
building	O
neural	NLP-focus
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
NMT	NLP-focus
)	NLP-focus
systems	O
across	O
both	O
academia	O
and	O
industry	O
.	O

While	O
recent	O
literature	O
has	O
proposed	O
methods	O
for	O
automatic	O
hyperparameter	O
optimization	O
(	O
HPO	O
)	O
there	O
has	O
been	O
limited	O
work	O
on	O
applying	O
these	O
methods	O
to	O
neural	NLP-focus
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
NMT	NLP-focus
)	NLP-focus
due	O
in	O
part	O
to	O
the	O
high	O
costs	O
associated	O
with	O
experiments	O
that	O
train	O
large	O
numbers	O
of	O
model	O
variants	O
.	O

Our	O
contributions	O
include	O
(	O
1	O
)	O
the	O
release	O
of	O
a	O
large	O
collection	O
of	O
trained	O
NMT	NLP-algorithm/tool
models	NLP-algorithm/tool
covering	O
a	O
wide	O
range	O
of	O
hyperparameters	O
(	O
2	O
)	O
the	O
proposal	O
of	O
targeted	O
metrics	O
for	O
evaluating	O
HPO	O
NMT	NLP-focus
HPO	O
on	O
NMT	O
,	O
and	O
(	O
3	O
)	O
a	O
reproducible	O
benchmark	O
of	O
several	O
HPO	O
methods	O
against	O
our	O
model	O
library	O
,	O
including	O
novel	O
graph	O
-	O
based	O
and	O
multiobjective	O
methods	O
.	O

Our	O
contributions	O
include	O
(	O
1	O
)	O
the	O
release	O
of	O
a	O
large	O
collection	O
of	O
trained	O
NMT	NLP-algorithm/tool
models	NLP-algorithm/tool
covering	O
a	O
wide	O
range	O
of	O
hyperparameters	O
(	O
2	O
)	O
the	O
proposal	O
of	O
targeted	O
metrics	O
for	O
evaluating	O
HPO	O
NMT	NLP-focus
HPO	O
on	O
NMT	O
,	O
and	O
(	O
3	O
)	O
a	O
reproducible	O
benchmark	O
of	O
several	O
HPO	O
methods	O
against	O
our	O
model	O
library	O
,	O
including	O
novel	O
graph	O
-	O
based	O
and	O
multiobjective	O
methods	O
.	O

Recent	O
work	O
has	O
presented	O
intriguing	O
results	O
examining	O
the	O
knowledge	O
contained	O
in	O
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
LM	NLP-algorithm/tool
having	O
the	O
LM	O
fill	O
in	O
the	O
blanks	O
of	O
prompts	O
such	O
as	O
“	O
Obama	O
is	O
a	O
\	O
_	O
\	O
_	O
by	O
profession	O
”.	O

Because	O
of	O
this	O
,	O
given	O
an	O
inappropriate	O
prompt	O
we	O
might	O
fail	O
to	O
retrieve	O
facts	O
that	O
the	O
LM	NLP-algorithm/tool
prompt	O
ow	O
,	O
and	O
thus	O
any	O
given	O
prompt	O
only	O
provides	O
a	O
lower	O
bound	O
estimate	O
LM	NLP-algorithm/tool
the	O
knowledge	O
contained	O
in	O
an	O
LM	O
.	O

In	O
this	O
paper	O
,	O
we	O
attempt	O
to	O
more	O
accurately	O
estimate	O
the	O
knowledge	O
contained	O
in	O
LMs	NLP-algorithm/tool
by	O
automatically	O
discovering	O
better	O
prompts	O
to	O
use	O
in	O
this	O
querying	O
process	O
.	O

Extensive	O
experiments	O
on	O
the	O
LAMA	O
benchmark	O
for	O
extracting	O
relational	O
knowledge	O
from	O
LMs	O
demonstrate	O
that	O
our	O
methods	O
can	O
improve	O
accuracy	O
from	O
31	Numerical-result
.	Numerical-result
1	Numerical-result
\\%	Numerical-result
to	O
39	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
providing	O
a	O
tighter	O
lower	O
bound	O
LMs	NLP-algorithm/tool
hat	O
LMs	O
know	O
.	O

Extensive	O
experiments	O
on	O
the	O
LAMA	O
benchmark	O
for	O
extracting	O
relational	O
knowledge	O
from	O
LMs	O
demonstrate	O
that	O
our	O
methods	O
can	O
improve	O
accuracy	O
from	O
31	Numerical-result
.	Numerical-result
1	Numerical-result
\\%	Numerical-result
to	O
39	Numerical-result
.	Numerical-result
6	Numerical-result
\\%	Numerical-result
providing	O
a	O
tighter	O
lower	O
bound	O
LMs	NLP-algorithm/tool
hat	O
LMs	O
know	O
.	O

Topic	NLP-focus
modeling	NLP-focus
analyzes	O
documents	O
to	O
learn	O
meaningful	O
patterns	O
of	O
words	O
.	O

However	O
,	O
existing	O
topic	NLP-algorithm/tool
models	NLP-algorithm/tool
fail	O
to	O
learn	O
interpretable	O
topics	O
when	O
working	O
with	O
large	O
and	O
heavy	O
-	O
tailed	O
vocabularies	O
.	O

To	O
this	O
end	O
,	O
we	O
develop	O
the	O
embedded	O
topic	O
model	O
(	O
etm	O
)	O
a	O
generative	O
model	O
of	O
documents	O
that	O
marries	O
traditional	NLP-algorithm/tool
topic	NLP-algorithm/tool
models	NLP-algorithm/tool
with	O
word	O
embeddings	O
.	O

It	O
outperforms	O
existing	O
document	NLP-algorithm/tool
models	NLP-algorithm/tool
such	O
as	O
latent	O
Dirichlet	O
allocation	O
in	O
terms	O
of	O
both	O
topic	NLP-metrics
quality	NLP-metrics
and	O
predictive	O
performance	O
.	O

It	O
outperforms	O
existing	O
document	NLP-algorithm/tool
models	NLP-algorithm/tool
such	O
as	O
latent	O
Dirichlet	O
allocation	O
in	O
terms	O
of	O
both	O
topic	NLP-metrics
quality	NLP-metrics
and	O
predictive	O
performance	O
.	O

Confidently	O
making	O
progress	O
on	O
multilingual	NLP-algorithm/tool
modeling	NLP-algorithm/tool
requires	O
challenging	O
,	O
trustworthy	O
evaluations	O
.	O

We	O
present	O
TyDi	O
QA	O
a	O
question	NLP-focus
answering	NLP-focus
dataset	O
covering	O
11	O
typologically	O
diverse	O
languages	O
with	O
204K	O
question	O
-	O
answer	O
pairs	O
.	O

The	O
trained	O
model	O
maps	O
words	O
to	O
topic	O
-	O
dependent	O
embeddings	O
which	O
naturally	O
addresses	O
the	O
issue	O
of	O
word	NLP-focus
polysemy	NLP-focus
.	O

Experimental	O
results	O
show	O
that	O
the	O
proposed	O
model	O
outperforms	O
the	O
word	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
embedding	NLP-algorithm/tool
methods	NLP-algorithm/tool
in	O
both	O
word	NLP-focus
similarity	NLP-focus
evaluation	NLP-focus
and	O
word	NLP-focus
sense	NLP-focus
disambiguation	NLP-focus
.	O

Experimental	O
results	O
show	O
that	O
the	O
proposed	O
model	O
outperforms	O
the	O
word	NLP-algorithm/tool
-	NLP-algorithm/tool
level	NLP-algorithm/tool
embedding	NLP-algorithm/tool
methods	NLP-algorithm/tool
in	O
both	O
word	NLP-focus
similarity	NLP-focus
evaluation	NLP-focus
and	O
word	NLP-focus
sense	NLP-focus
disambiguation	NLP-focus
.	O

Finally	O
,	O
the	O
model	O
can	O
be	O
easily	O
integrated	O
with	O
existing	O
deep	NLP-algorithm/tool
contextualized	NLP-algorithm/tool
word	NLP-algorithm/tool
embedding	NLP-algorithm/tool
learning	NLP-algorithm/tool
methods	NLP-algorithm/tool
to	O
further	O
improve	O
the	O
performance	O
of	O
downstream	O
tasks	O
such	O
as	O
sentiment	NLP-focus
classification	NLP-focus
.	O

Finally	O
,	O
the	O
model	O
can	O
be	O
easily	O
integrated	O
with	O
existing	O
deep	NLP-algorithm/tool
contextualized	NLP-algorithm/tool
word	NLP-algorithm/tool
embedding	NLP-algorithm/tool
learning	NLP-algorithm/tool
methods	NLP-algorithm/tool
to	O
further	O
improve	O
the	O
performance	O
of	O
downstream	O
tasks	O
such	O
as	O
sentiment	NLP-focus
classification	NLP-focus
.	O

We	O
also	O
propose	O
a	O
new	O
bias	O
evaluation	O
metric	O
,	O
Gender	NLP-metrics
-	NLP-metrics
based	NLP-metrics
Illicit	NLP-metrics
Proximity	NLP-metrics
Estimate	NLP-metrics
(	NLP-metrics
GIPE	NLP-metrics
)	NLP-metrics
which	O
measures	O
the	O
extent	O
of	O
undue	O
proximity	O
in	O
word	O
vectors	O
resulting	O
from	O
the	O
presence	O
of	O
gender	O
-	O
based	O
predilections	O
.	O

Experiments	O
based	O
on	O
a	O
suite	O
of	O
evaluation	O
metrics	O
show	O
that	O
RAN	O
-	O
Debias	O
significantly	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
reducing	O
proximity	O
bias	O
(	O
GIPE	NLP-metrics
by	O
at	O
least	O
42	Numerical-result
.	Numerical-result
02	Numerical-result
\\%	Numerical-result
.	O

Experiments	O
based	O
on	O
a	O
suite	O
of	O
evaluation	O
metrics	O
show	O
that	O
RAN	O
-	O
Debias	O
significantly	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
reducing	O
proximity	O
bias	O
(	O
GIPE	NLP-metrics
by	O
at	O
least	O
42	Numerical-result
.	Numerical-result
02	Numerical-result
\\%	Numerical-result
.	O

It	O
also	O
reduces	O
direct	O
bias	O
adding	O
minimal	O
semantic	O
disturbance	O
,	O
and	O
achieves	O
the	O
best	O
performance	O
in	O
a	O
downstream	O
application	O
task	O
(	O
coreference	NLP-focus
resolution	NLP-focus
.	O

To	O
alleviate	O
this	O
,	O
we	O
propose	O
PERL	O
A	O
representation	O
learning	O
model	O
that	O
extends	O
contextualized	O
word	O
embedding	O
models	O
such	O
as	O
BERT	NLP-algorithm/tool
PERL	O
in	O
et	O
al	O
.,	O
2019	O
)	O
with	O
pivot	O
-	O
based	O
fine	O
-	O
tuning	O
.	O

Different	O
metrics	O
have	O
been	O
proposed	O
to	O
compare	O
Abstract	NLP-algorithm/tool
Meaning	NLP-algorithm/tool
Representation	NLP-algorithm/tool
(	NLP-algorithm/tool
AMR	NLP-algorithm/tool
)	NLP-algorithm/tool
graphs	NLP-algorithm/tool
.	O

The	O
canonical	O
Smatch	NLP-metrics
metric	O
(	O
Cai	O
and	O
Knight	O
,	O
2013	O
)	O
aligns	O
the	O
variables	O
of	O
two	O
graphs	O
and	O
assesses	O
triple	O
matches	O
.	O

The	O
recent	O
SemBleu	NLP-metrics
metric	O
(	O
Song	O
and	O
Gildea	O
,	O
2019	O
)	O
is	O
based	O
on	O
the	O
machine	NLP-focus
-	NLP-focus
translation	NLP-focus
Bleu	O
c	O
Bleu	O
(	O
Papineni	O
et	O
al	O
.,	O
2002	O
)	O
and	O
increases	O
computational	O
efficiency	O
by	O
ablating	O
the	O
variable	O
-	O
alignment	O
.	O

The	O
recent	O
SemBleu	NLP-metrics
metric	O
(	O
Song	O
and	O
Gildea	O
,	O
2019	O
)	O
is	O
based	O
on	O
the	O
machine	NLP-focus
-	NLP-focus
translation	NLP-focus
Bleu	O
c	O
Bleu	O
(	O
Papineni	O
et	O
al	O
.,	O
2002	O
)	O
and	O
increases	O
computational	O
efficiency	O
by	O
ablating	O
the	O
variable	O
-	O
alignment	O
.	O

In	O
this	O
paper	O
,	O
i	O
)	O
we	O
establish	O
criteria	O
that	O
enable	O
researchers	O
to	O
perform	O
a	O
principled	O
assessment	O
of	O
metrics	O
comparing	O
meaning	O
representations	O
like	O
AMR	NLP-focus
ii	O
)	O
we	O
undertake	O
a	O
thorough	O
analysis	O
of	O
Smatch	NLP-metrics
and	O
SemBleu	NLP-metrics
where	O
we	O
show	O
that	O
the	O
latter	O
exhibits	O
some	O
undesirable	O
properties	O
.	O

In	O
this	O
paper	O
,	O
i	O
)	O
we	O
establish	O
criteria	O
that	O
enable	O
researchers	O
to	O
perform	O
a	O
principled	O
assessment	O
of	O
metrics	O
comparing	O
meaning	O
representations	O
like	O
AMR	NLP-focus
ii	O
)	O
we	O
undertake	O
a	O
thorough	O
analysis	O
of	O
Smatch	NLP-metrics
and	O
SemBleu	NLP-metrics
where	O
we	O
show	O
that	O
the	O
latter	O
exhibits	O
some	O
undesirable	O
properties	O
.	O

For	O
example	O
,	O
it	O
does	O
not	O
conform	O
to	O
the	O
identity	O
of	O
indiscernibles	O
rule	O
and	O
introduces	O
biases	O
that	O
are	O
hard	O
to	O
control	O
;	O
and	O
iii	O
)	O
we	O
propose	O
a	O
novel	O
metric	O
S2match	NLP-metrics
that	O
is	O
more	O
benevolent	O
to	O
only	O
very	O
slight	O
meaning	O
deviations	O
and	O
targets	O
the	O
fulfilment	O
of	O
all	O
established	O
criteria	O
.	O

We	O
assess	O
its	O
suitability	O
and	O
show	O
its	O
advantages	O
over	O
Smatch	NLP-metrics
and	O
SemBleu	NLP-metrics
.	O

Quality	NLP-focus
Estimation	NLP-focus
(	NLP-focus
QE	NLP-focus
)	NLP-focus
is	O
an	O
important	O
component	O
in	O
making	O
Machine	NLP-focus
Translation	NLP-focus
(	NLP-focus
MT	NLP-focus
)	NLP-focus
MT	NLP-focus
ful	O
in	O
real	O
-	O
world	O
applications	O
,	O
as	O
it	O
is	O
aimed	O
to	O
inform	O
the	O
user	O
on	O
the	O
quality	O
of	O
the	O
MT	O
output	O
at	O
test	O
time	O
.	O

As	O
an	O
alternative	O
,	O
we	O
devise	O
an	O
unsupervised	O
approach	O
to	O
QE	NLP-focus
where	O
no	O
training	O
or	O
access	O
to	O
additional	O
resources	O
besides	O
the	O
MT	NLP-focus
system	O
itself	O
is	O
required	O
.	O

Different	O
from	O
most	O
of	O
the	O
current	O
work	O
that	O
treats	O
the	O
MT	NLP-focus
MT	NLP-focus
tem	O
as	O
a	O
black	O
box	O
,	O
we	O
explore	O
useful	O
information	O
that	O
can	O
be	O
extracted	O
from	O
the	O
MT	O
system	O
as	O
a	O
by	O
-	O
product	O
of	O
translation	O
.	O

By	O
utilizing	O
methods	O
for	O
uncertainty	O
quantification	O
,	O
we	O
achieve	O
very	O
good	O
correlation	O
with	O
human	O
judgments	O
of	O
quality	O
,	O
rivaling	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
supervised	NLP-algorithm/tool
QE	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

To	O
evaluate	O
our	O
approach	O
we	O
collect	O
the	O
first	O
dataset	O
that	O
enables	O
work	O
on	O
both	O
black	O
-	O
box	O
and	O
glass	O
-	O
box	O
approaches	O
to	O
QE	NLP-focus
.	O

We	O
describe	O
an	O
approach	O
to	O
task	NLP-focus
-	NLP-focus
oriented	NLP-focus
dialogue	NLP-focus
in	O
which	O
dialogue	O
state	O
is	O
represented	O
as	O
a	O
dataflow	O
graph	O
.	O

Open	NLP-focus
-	NLP-focus
domain	NLP-focus
question	NLP-focus
answering	NLP-focus
(	NLP-focus
QA	NLP-focus
)	NLP-focus
involves	O
many	O
knowledge	O
and	O
reasoning	O
challenges	O
,	O
but	O
are	O
successful	O
QA	NLP-algorithm/tool
models	NLP-algorithm/tool
actually	O
learning	O
such	O
knowledge	O
when	O
trained	O
on	O
benchmark	O
QA	O
tasks	O
?	O
We	O
investigate	O
this	O
via	O
several	O
new	O
diagnostic	O
tasks	O
probing	O
whether	O
multiple	NLP-algorithm/tool
-	NLP-algorithm/tool
choice	NLP-algorithm/tool
QA	NLP-algorithm/tool
models	NLP-algorithm/tool
know	O
definitions	O
and	O
taxonomic	O
reasoning	O
—	O
two	O
skills	O
widespread	O
in	O
existing	O
benchmarks	O
and	O
fundamental	O
to	O
more	O
complex	O
reasoning	O
.	O

Open	NLP-focus
-	NLP-focus
domain	NLP-focus
question	NLP-focus
answering	NLP-focus
(	NLP-focus
QA	NLP-focus
)	NLP-focus
involves	O
many	O
knowledge	O
and	O
reasoning	O
challenges	O
,	O
but	O
are	O
successful	O
QA	NLP-algorithm/tool
models	NLP-algorithm/tool
actually	O
learning	O
such	O
knowledge	O
when	O
trained	O
on	O
benchmark	O
QA	O
tasks	O
?	O
We	O
investigate	O
this	O
via	O
several	O
new	O
diagnostic	O
tasks	O
probing	O
whether	O
multiple	NLP-algorithm/tool
-	NLP-algorithm/tool
choice	NLP-algorithm/tool
QA	NLP-algorithm/tool
models	NLP-algorithm/tool
know	O
definitions	O
and	O
taxonomic	O
reasoning	O
—	O
two	O
skills	O
widespread	O
in	O
existing	O
benchmarks	O
and	O
fundamental	O
to	O
more	O
complex	O
reasoning	O
.	O

Our	O
evaluati	O
o	O
confirms	O
that	O
transformer	O
-	O
based	O
multiple	NLP-algorithm/tool
-	NLP-algorithm/tool
choice	NLP-algorithm/tool
QA	NLP-algorithm/tool
models	NLP-algorithm/tool
are	O
already	O
predisposed	O
to	O
recognize	O
certain	O
types	O
of	O
structural	O
linguistic	O
knowledge	O
.	O

Recent	O
graph	NLP-algorithm/tool
-	NLP-algorithm/tool
to	NLP-algorithm/tool
-	NLP-algorithm/tool
text	NLP-algorithm/tool
models	NLP-algorithm/tool
generate	O
text	O
from	O
graph	O
-	O
based	O
data	O
using	O
either	O
global	O
or	O
local	O
aggregation	O
to	O
learn	O
node	O
representations	O
.	O

In	O
our	O
experiments	O
,	O
we	O
demonstrate	O
that	O
our	O
approaches	O
lead	O
to	O
significant	O
improvements	O
on	O
two	O
graph	O
-	O
to	O
-	O
text	O
datasets	O
achieving	O
BLEU	NLP-metrics
scores	O
of	O
18	Numerical-result
.	Numerical-result
01	Numerical-result
on	O
the	O
AGENDA	O
dataset	O
,	O
and	O
63	Numerical-result
.	Numerical-result
69	Numerical-result
on	O
the	O
WebNLG	O
dataset	O
for	O
seen	O
categories	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
by	O
3	Numerical-result
.	Numerical-result
7	Numerical-result
and	O
3	Numerical-result
.	Numerical-result
1	Numerical-result
points	O
,	O
respectively	O
.	O
1	O
.	O

In	O
our	O
experiments	O
,	O
we	O
demonstrate	O
that	O
our	O
approaches	O
lead	O
to	O
significant	O
improvements	O
on	O
two	O
graph	O
-	O
to	O
-	O
text	O
datasets	O
achieving	O
BLEU	NLP-metrics
scores	O
of	O
18	Numerical-result
.	Numerical-result
01	Numerical-result
on	O
the	O
AGENDA	O
dataset	O
,	O
and	O
63	Numerical-result
.	Numerical-result
69	Numerical-result
on	O
the	O
WebNLG	O
dataset	O
for	O
seen	O
categories	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
by	O
3	Numerical-result
.	Numerical-result
7	Numerical-result
and	O
3	Numerical-result
.	Numerical-result
1	Numerical-result
points	O
,	O
respectively	O
.	O
1	O
.	O

Our	O
method	O
has	O
no	O
additional	O
hyperparameters	O
to	O
the	O
conditional	O
random	O
field	O
based	O
model	O
widely	O
used	O
for	O
flat	O
named	NLP-focus
entity	NLP-focus
recognition	NLP-focus
tasks	O
.	O

Experiments	O
demonstrate	O
that	O
our	O
method	O
performs	O
better	O
than	O
or	O
at	O
least	O
as	O
well	O
as	O
existing	O
methods	O
capable	O
of	O
handling	O
nested	O
entities	O
achieving	O
F1	Classification-metrics
scores	O
of	O
85	Numerical-result
.	Numerical-result
82	Numerical-result
\\%	Numerical-result
84	Numerical-result
.	Numerical-result
34	Numerical-result
\\%	Numerical-result
and	O
77	Numerical-result
.	Numerical-result
36	Numerical-result
\\%	Numerical-result
on	O
ACE	O
-	O
2004	O
ACE	O
-	O
2005	O
and	O
GENIA	O
datasets	O
,	O
respectively	O
.	O

Experiments	O
demonstrate	O
that	O
our	O
method	O
performs	O
better	O
than	O
or	O
at	O
least	O
as	O
well	O
as	O
existing	O
methods	O
capable	O
of	O
handling	O
nested	O
entities	O
achieving	O
F1	Classification-metrics
scores	O
of	O
85	Numerical-result
.	Numerical-result
82	Numerical-result
\\%	Numerical-result
84	Numerical-result
.	Numerical-result
34	Numerical-result
\\%	Numerical-result
and	O
77	Numerical-result
.	Numerical-result
36	Numerical-result
\\%	Numerical-result
on	O
ACE	O
-	O
2004	O
ACE	O
-	O
2005	O
and	O
GENIA	O
datasets	O
,	O
respectively	O
.	O

Recent	O
work	O
has	O
shown	O
that	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
such	O
as	O
BERT	NLP-algorithm/tool
improve	O
robustness	O
to	O
spurious	O
correlations	O
in	O
the	O
dataset	O
.	O

Our	O
experiments	O
on	O
natural	NLP-focus
language	NLP-focus
inference	NLP-focus
and	O
paraphrase	O
identification	O
show	O
that	O
MTL	O
with	O
the	O
right	O
auxiliary	O
tasks	O
significantly	O
improves	O
performance	O
on	O
challenging	O
examples	O
without	O
hurting	O
the	O
in	O
-	O
distribution	O
performance	O
.	O

Recent	O
progress	O
in	O
the	O
task	O
of	O
Grammatical	NLP-focus
Error	NLP-focus
Correction	NLP-focus
(	NLP-focus
GEC	NLP-focus
)	NLP-focus
has	O
been	O
driven	O
by	O
addressing	O
data	O
sparsity	O
,	O
both	O
through	O
new	O
methods	O
for	O
generating	O
large	O
and	O
noisy	O
pretraining	O
data	O
and	O
through	O
the	O
publication	O
of	O
small	O
and	O
higher	O
-	O
quality	O
finetuning	O
data	O
in	O
the	O
BEA	O
-	O
2019	O
shared	O
task	O
.	O

Building	O
upon	O
recent	O
work	O
in	O
Neural	NLP-focus
Machine	NLP-focus
Translation	NLP-focus
(	NLP-focus
NMT	NLP-focus
)	NLP-focus
we	O
make	O
use	O
of	O
both	O
kinds	O
of	O
data	O
by	O
deriving	O
example	O
-	O
level	O
scores	O
on	O
our	O
large	O
pretraining	O
data	O
based	O
on	O
a	O
smaller	O
,	O
higher	O
-	O
quality	O
dataset	O
.	O

In	O
this	O
work	O
,	O
we	O
perform	O
an	O
empirical	O
study	O
to	O
discover	O
how	O
to	O
best	O
incorporate	O
delta	O
-	O
log	O
-	O
perplexity	O
,	O
a	O
type	O
of	O
example	O
scoring	O
,	O
into	O
a	O
training	O
schedule	O
for	O
GEC	NLP-focus
.	O

Models	O
trained	O
on	O
scored	O
data	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
common	O
GEC	NLP-focus
test	O
sets	O
.	O

In	O
this	O
paper	O
we	O
demonstrate	O
that	O
context	O
free	O
grammar	O
(	O
CFG	O
)	O
based	O
methods	O
for	O
grammar	NLP-focus
induction	NLP-focus
benefit	O
from	O
modeling	O
lexical	O
dependencies	O
.	O

This	O
contrasts	O
to	O
the	O
most	O
popular	O
current	O
methods	O
for	O
grammar	NLP-focus
induction	NLP-focus
which	O
focus	O
on	O
discovering	O
either	O
constituents	O
or	O
dependencies	O
.	O

Previous	O
approaches	O
to	O
marry	O
these	O
two	O
disparate	O
syntactic	O
formalisms	O
(	O
e	O
.	O
g	O
.,	O
lexicalized	NLP-algorithm/tool
PCFGs	NLP-algorithm/tool
have	O
been	O
plagued	O
by	O
sparsity	O
,	O
making	O
them	O
unsuitable	O
for	O
unsupervised	O
grammar	O
induction	O
.	O

However	O
,	O
in	O
this	O
work	O
,	O
we	O
present	O
novel	O
neural	O
models	O
of	O
lexicalized	NLP-algorithm/tool
PCFGs	NLP-algorithm/tool
that	O
allow	O
us	O
to	O
overcome	O
sparsity	O
problems	O
and	O
effectively	O
induce	O
both	O
constituents	O
and	O
dependencies	O
within	O
a	O
single	O
model	O
.	O

Innovations	O
in	O
annotation	O
methodology	O
have	O
been	O
a	O
catalyst	O
for	O
Reading	NLP-focus
Comprehension	NLP-focus
(	NLP-focus
RC	NLP-focus
)	NLP-focus
datasets	O
and	O
models	O
.	O

One	O
recent	O
trend	O
to	O
challenge	O
current	O
RC	NLP-algorithm/tool
models	NLP-algorithm/tool
is	O
to	O
involve	O
a	O
model	O
in	O
the	O
annotation	O
process	O
:	O
Humans	O
create	O
questions	O
adversarially	O
,	O
such	O
that	O
the	O
model	O
fails	O
to	O
answer	O
them	O
correctly	O
.	O

When	O
trained	O
on	O
data	O
collected	O
with	O
a	O
BiDAF	NLP-algorithm/tool
model	NLP-algorithm/tool
in	O
the	O
loop	O
,	O
RoBERTa	NLP-algorithm/tool
achieves	O
39	Numerical-result
.	Numerical-result
9	Numerical-result
F1	Classification-metrics
n	O
questions	O
that	O
it	O
cannot	O
answer	O
when	O
trained	O
on	O
SQuAD	O
RoBERTa	NLP-algorithm/tool
ginally	O
lower	O
than	O
when	O
trained	O
on	O
data	O
collected	O
using	O
RoBERTa	O
itself	O
(	O
41	Numerical-result
.	Numerical-result
0	Numerical-result
F1	Classification-metrics
.	O

When	O
trained	O
on	O
data	O
collected	O
with	O
a	O
BiDAF	NLP-algorithm/tool
model	NLP-algorithm/tool
in	O
the	O
loop	O
,	O
RoBERTa	NLP-algorithm/tool
achieves	O
39	Numerical-result
.	Numerical-result
9	Numerical-result
F1	Classification-metrics
n	O
questions	O
that	O
it	O
cannot	O
answer	O
when	O
trained	O
on	O
SQuAD	O
RoBERTa	NLP-algorithm/tool
ginally	O
lower	O
than	O
when	O
trained	O
on	O
data	O
collected	O
using	O
RoBERTa	O
itself	O
(	O
41	Numerical-result
.	Numerical-result
0	Numerical-result
F1	Classification-metrics
.	O

When	O
trained	O
on	O
data	O
collected	O
with	O
a	O
BiDAF	NLP-algorithm/tool
model	NLP-algorithm/tool
in	O
the	O
loop	O
,	O
RoBERTa	NLP-algorithm/tool
achieves	O
39	Numerical-result
.	Numerical-result
9	Numerical-result
F1	Classification-metrics
n	O
questions	O
that	O
it	O
cannot	O
answer	O
when	O
trained	O
on	O
SQuAD	O
RoBERTa	NLP-algorithm/tool
ginally	O
lower	O
than	O
when	O
trained	O
on	O
data	O
collected	O
using	O
RoBERTa	O
itself	O
(	O
41	Numerical-result
.	Numerical-result
0	Numerical-result
F1	Classification-metrics
.	O

First	O
,	O
a	O
semantic	NLP-algorithm/tool
parser	NLP-algorithm/tool
(	O
either	O
grammar	O
-	O
based	O
or	O
neural	O
)	O
maps	O
the	O
natural	O
language	O
description	O
into	O
an	O
intermediate	O
sketch	O
,	O
which	O
is	O
an	O
incomplete	O
regex	O
containing	O
holes	O
to	O
denote	O
missing	O
components	O
.	O

Our	O
semantic	NLP-algorithm/tool
parser	NLP-algorithm/tool
can	O
be	O
trained	O
purely	O
from	O
weak	O
supervision	O
based	O
on	O
correctness	O
of	O
the	O
synthesized	O
regex	O
,	O
or	O
it	O
can	O
leverage	O
heuristically	O
derived	O
sketches	O
.	O

Our	O
system	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
prior	O
datasets	O
and	O
solves	O
57	Numerical-result
\\%	Numerical-result
of	O
the	O
real	O
-	O
world	O
dataset	O
,	O
which	O
existing	O
neural	O
systems	O
completely	O
fail	O
on	O
.	O
1	O
.	O

The	O
conventional	O
paradigm	O
in	O
speech	NLP-focus
translation	NLP-focus
starts	O
with	O
a	O
speech	NLP-focus
recognition	NLP-focus
step	O
to	O
generate	O
transcripts	O
,	O
followed	O
by	O
a	O
translation	O
step	O
with	O
the	O
automatic	O
transcripts	O
as	O
input	O
.	O

We	O
find	O
that	O
direct	O
models	O
are	O
poorly	O
suited	O
to	O
the	O
joint	NLP-focus
transcription	NLP-focus
/	NLP-focus
translation	NLP-focus
task	NLP-focus
but	O
that	O
end	O
-	O
to	O
-	O
end	O
models	O
that	O
feature	O
a	O
coupled	O
inference	O
procedure	O
are	O
able	O
to	O
achieve	O
strong	O
consistency	O
.	O

We	O
further	O
introduce	O
simple	O
techniques	O
for	O
directly	O
optimizing	O
for	O
consistency	O
consistency	O
the	O
resulting	O
trade	O
-	O
offs	O
between	O
consistency	O
,	O
transcription	Classification-metrics
accuracy	Classification-metrics
and	O
translation	Classification-metrics
accuracy	Classification-metrics
1	O
.	O

Neural	NLP-focus
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
NMT	NLP-focus
)	NLP-focus
systems	O
are	O
usually	O
trained	O
on	O
clean	O
parallel	O
data	O
.	O

Given	O
the	O
lack	O
of	O
parallel	O
data	O
of	O
UGT	O
that	O
can	O
be	O
used	O
to	O
train	O
or	O
adapt	O
NMT	NLP-focus
UGT	O
UGT	O
we	O
synthesize	O
parallel	O
data	O
of	O
UGT	O
,	O
exploiting	O
monolingual	O
data	O
of	O
UGT	O
through	O
crosslingual	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
pre	O
-	O
training	O
and	O
zero	NLP-focus
-	NLP-focus
shot	NLP-focus
NMT	NLP-focus
systems	O
.	O

Given	O
the	O
lack	O
of	O
parallel	O
data	O
of	O
UGT	O
that	O
can	O
be	O
used	O
to	O
train	O
or	O
adapt	O
NMT	NLP-focus
UGT	O
UGT	O
we	O
synthesize	O
parallel	O
data	O
of	O
UGT	O
,	O
exploiting	O
monolingual	O
data	O
of	O
UGT	O
through	O
crosslingual	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
pre	O
-	O
training	O
and	O
zero	NLP-focus
-	NLP-focus
shot	NLP-focus
NMT	NLP-focus
systems	O
.	O

On	O
the	O
MTNT	O
translation	O
tasks	O
,	O
we	O
show	O
that	O
our	O
synthesized	O
parallel	O
data	O
can	O
lead	O
to	O
better	O
NMT	NLP-focus
systems	O
for	O
UGT	O
while	O
making	O
them	O
more	O
robust	O
in	O
translating	O
texts	O
from	O
various	O
domains	O
and	O
styles	O
.	O

This	O
paper	O
demonstrates	O
that	O
multilingual	NLP-algorithm/tool
denoising	NLP-algorithm/tool
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
training	NLP-algorithm/tool
produces	O
significant	O
performance	O
gains	O
across	O
a	O
wide	O
variety	O
of	O
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
MT	NLP-focus
)	NLP-focus
tasks	O
.	O

This	O
paper	O
demonstrates	O
that	O
multilingual	NLP-algorithm/tool
denoising	NLP-algorithm/tool
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
training	NLP-algorithm/tool
produces	O
significant	O
performance	O
gains	O
across	O
a	O
wide	O
variety	O
of	O
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
MT	NLP-focus
)	NLP-focus
tasks	O
.	O

We	O
present	O
mBART	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
denoising	O
auto	O
-	O
encoder	O
pre	O
-	O
trained	O
on	O
large	O
-	O
scale	O
monolingual	O
corpora	O
in	O
many	O
languages	O
using	O
the	O
BART	NLP-algorithm/tool
objective	NLP-algorithm/tool
mBART	O
et	O
al	O
.,	O
2019	O
).	O

Pre	O
-	O
training	O
a	O
complete	O
model	O
allows	O
it	O
to	O
be	O
directly	O
fine	O
-	O
tuned	O
for	O
supervised	O
(	O
both	O
sentence	O
-	O
level	O
and	O
document	O
-	O
level	O
)	O
and	O
unsupervised	NLP-focus
machine	NLP-focus
translation	NLP-focus
with	O
no	O
task	O
-	O
specific	O
modifications	O
.	O

We	O
demonstrate	O
that	O
adding	O
mBART	O
initialization	O
produces	O
performance	O
gains	O
in	O
all	O
but	O
the	O
highest	O
-	O
resource	O
settings	O
,	O
including	O
up	O
to	O
12	O
BLEU	NLP-metrics
points	O
for	O
low	NLP-focus
resource	NLP-focus
MT	NLP-focus
BLEU	NLP-metrics
ver	O
5	O
BLEU	O
points	O
for	O
many	O
document	O
-	O
level	O
and	O
unsupervised	O
models	O
.	O

We	O
demonstrate	O
that	O
adding	O
mBART	O
initialization	O
produces	O
performance	O
gains	O
in	O
all	O
but	O
the	O
highest	O
-	O
resource	O
settings	O
,	O
including	O
up	O
to	O
12	O
BLEU	NLP-metrics
points	O
for	O
low	NLP-focus
resource	NLP-focus
MT	NLP-focus
BLEU	NLP-metrics
ver	O
5	O
BLEU	O
points	O
for	O
many	O
document	O
-	O
level	O
and	O
unsupervised	O
models	O
.	O

Recent	O
success	O
of	O
pre	NLP-algorithm/tool
-	NLP-algorithm/tool
trained	NLP-algorithm/tool
language	NLP-algorithm/tool
models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
has	O
spurred	O
widespread	O
interest	O
in	O
the	O
language	O
capabilities	O
that	O
they	O
possess	O
.	O

However	O
,	O
efforts	O
to	O
understand	O
whether	O
LM	NLP-algorithm/tool
representations	O
are	O
useful	O
for	O
symbolic	NLP-focus
reasoning	NLP-focus
tasks	O
have	O
been	O
limited	O
and	O
scattered	O
.	O

However	O
,	O
efforts	O
to	O
understand	O
whether	O
LM	NLP-algorithm/tool
representations	O
are	O
useful	O
for	O
symbolic	NLP-focus
reasoning	NLP-focus
tasks	O
have	O
been	O
limited	O
and	O
scattered	O
.	O

A	O
fundamental	O
challenge	O
is	O
to	O
understand	O
whether	O
the	O
performance	O
of	O
a	O
LM	NLP-algorithm/tool
on	O
a	O
task	O
should	O
be	O
attributed	O
to	O
the	O
pre	O
-	O
trained	O
representations	O
or	O
to	O
the	O
process	O
of	O
fine	O
-	O
tuning	O
on	O
the	O
task	O
data	O
.	O

To	O
address	O
this	O
,	O
we	O
propose	O
an	O
evaluation	O
protocol	O
that	O
includes	O
both	O
zero	O
-	O
shot	O
evaluation	O
(	O
no	O
fine	O
-	O
tuning	O
),	O
as	O
well	O
as	O
comparing	O
the	O
learning	O
curve	O
of	O
a	O
fine	NLP-algorithm/tool
-	NLP-algorithm/tool
tuned	NLP-algorithm/tool
LM	NLP-algorithm/tool
LM	NLP-algorithm/tool
the	O
learning	O
curve	O
of	O
multiple	O
controls	O
,	O
which	O
paints	O
a	O
rich	O
picture	O
of	O
the	O
LM	O
capabilities	O
.	O

Our	O
main	O
findings	O
are	O
that	O
:	O
(	O
a	O
)	O
different	O
LMs	O
exhibit	O
qualitatively	O
different	O
reasoning	O
abilities	O
,	O
e	O
.	O
g	O
.,	O
RoBERTa	NLP-algorithm/tool
BERT	NLP-algorithm/tool
LMs	NLP-algorithm/tool
RoBERTa	NLP-algorithm/tool
ning	O
tasks	O
where	O
BERT	O
fails	O
completely	O
;	O
(	O
b	O
)	O
LMs	O
do	O
not	O
reason	O
in	O
an	O
abstract	O
manner	O
and	O
are	O
context	O
-	O
dependent	O
,	O
e	O
.	O
g	O
.,	O
while	O
RoBERTa	O
can	O
compare	O
ages	O
,	O
it	O
can	O
do	O
so	O
only	O
when	O
the	O
ages	O
are	O
in	O
the	O
typical	O
range	O
of	O
human	O
ages	O
;	O
(	O
c	O
)	O
On	O
half	O
of	O
our	O
reasoning	O
tasks	O
all	O
models	O
fail	O
completely	O
.	O

For	O
many	O
NLP	O
applications	O
,	O
such	O
as	O
question	NLP-focus
answering	NLP-focus
and	O
summarization	NLP-focus
the	O
goal	O
is	O
to	O
select	O
the	O
best	O
solution	O
from	O
a	O
large	O
space	O
of	O
candidates	O
to	O
meet	O
a	O
particular	O
user	O
’	O
s	O
needs	O
.	O

We	O
apply	O
our	O
method	O
to	O
community	NLP-focus
question	NLP-focus
answering	NLP-focus
(	NLP-focus
cQA	NLP-focus
)	NLP-focus
and	O
extractive	NLP-focus
multidocument	NLP-focus
summarization	NLP-focus
finding	O
that	O
it	O
significantly	O
outperforms	O
existing	O
interactive	O
approaches	O
.	O

We	O
also	O
show	O
that	O
the	O
ranking	O
function	O
learned	O
by	O
our	O
method	O
is	O
an	O
effective	O
reward	O
function	O
for	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
which	O
improves	O
the	O
state	O
of	O
the	O
art	O
for	O
interactive	NLP-focus
summarization	NLP-focus
.	O

We	O
also	O
show	O
that	O
the	O
ranking	O
function	O
learned	O
by	O
our	O
method	O
is	O
an	O
effective	O
reward	O
function	O
for	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
which	O
improves	O
the	O
state	O
of	O
the	O
art	O
for	O
interactive	NLP-focus
summarization	NLP-focus
.	O

Textual	NLP-algorithm/tool
representation	NLP-algorithm/tool
learners	NLP-algorithm/tool
trained	O
on	O
large	O
amounts	O
of	O
data	O
have	O
achieved	O
notable	O
success	O
on	O
downstream	O
tasks	O
;	O
intriguingly	O
,	O
they	O
have	O
also	O
performed	O
well	O
on	O
challenging	O
tests	O
of	O
syntactic	O
competence	O
.	O

Hence	O
,	O
it	O
remains	O
an	O
open	O
question	O
whether	O
scalable	O
learners	O
like	O
BERT	NLP-algorithm/tool
can	O
become	O
fully	O
proficient	O
in	O
the	O
syntax	O
of	O
natural	O
language	O
by	O
virtue	O
of	O
data	O
scale	O
alone	O
,	O
or	O
whether	O
they	O
still	O
benefit	O
from	O
more	O
explicit	O
syntactic	O
biases	O
.	O

To	O
answer	O
this	O
question	O
,	O
we	O
introduce	O
a	O
knowledge	O
distillation	O
strategy	O
for	O
injecting	O
syntactic	O
biases	O
into	O
BERT	NLP-algorithm/tool
pretraining	O
by	O
distilling	O
the	O
syntactically	O
informative	O
predictions	O
of	O
a	O
hierarchical	O
—	O
albeit	O
harder	O
to	O
scale	O
—	O
syntactic	NLP-algorithm/tool
language	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Since	O
BERT	NLP-algorithm/tool
models	NLP-algorithm/tool
masked	O
words	O
in	O
bidirectional	O
context	O
,	O
we	O
propose	O
to	O
distill	O
the	O
approximate	O
marginal	O
distribution	O
over	O
words	O
in	O
context	O
from	O
the	O
syntactic	NLP-algorithm/tool
LM	NLP-algorithm/tool
.	O

Our	O
approach	O
reduces	O
relative	O
error	O
by	O
2	Numerical-result
–	Numerical-result
21	Numerical-result
\\%	Numerical-result
on	O
a	O
diverse	O
set	O
of	O
structured	O
prediction	O
tasks	O
,	O
although	O
we	O
obtain	O
mixed	O
results	O
on	O
the	O
GLUE	O
benchmark	O
.	O

We	O
devise	O
effective	O
monotonic	O
approximations	O
to	O
popular	O
nonmonontic	O
scoring	O
functions	O
including	O
length	O
normalization	O
and	O
mutual	NLP-algorithm/tool
information	NLP-algorithm/tool
decoding	NLP-algorithm/tool
.	O

There	O
is	O
an	O
increasing	O
focus	O
on	O
model	O
-	O
based	O
dialog	O
evaluation	O
metrics	O
such	O
as	O
ADEM	NLP-metrics
RUBER	NLP-metrics
and	O
the	O
more	O
recent	O
BERT	NLP-algorithm/tool
based	O
metrics	O
.	O

There	O
is	O
an	O
increasing	O
focus	O
on	O
model	O
-	O
based	O
dialog	O
evaluation	O
metrics	O
such	O
as	O
ADEM	NLP-metrics
RUBER	NLP-metrics
and	O
the	O
more	O
recent	O
BERT	NLP-algorithm/tool
based	O
metrics	O
.	O

Using	O
this	O
dataset	O
,	O
we	O
first	O
show	O
that	O
even	O
in	O
the	O
presence	O
of	O
multiple	O
correct	O
references	O
,	O
n	NLP-metrics
-	NLP-metrics
gram	NLP-metrics
based	NLP-metrics
metrics	NLP-metrics
and	O
embedding	O
based	O
metrics	O
do	O
not	O
perform	O
well	O
at	O
separating	O
relevant	O
responses	O
from	O
even	O
random	O
negatives	O
.	O

While	O
model	O
-	O
based	O
metrics	O
perform	O
better	O
than	O
n	NLP-algorithm/tool
-	NLP-algorithm/tool
gram	NLP-algorithm/tool
and	O
embedding	O
based	O
metrics	O
on	O
random	O
negatives	O
,	O
their	O
performance	O
drops	O
substantially	O
when	O
evaluated	O
on	O
adversarial	O
examples	O
.	O

To	O
check	O
if	O
large	O
scale	O
pretraining	O
could	O
help	O
,	O
we	O
propose	O
a	O
new	O
BERT	NLP-algorithm/tool
based	O
evaluation	O
metric	O
called	O
DEB	NLP-metrics
which	O
is	O
pretrained	O
on	O
727M	O
Reddit	O
conversations	O
DEB	NLP-metrics
then	O
finetuned	O
on	O
our	O
dataset	O
.	O

To	O
check	O
if	O
large	O
scale	O
pretraining	O
could	O
help	O
,	O
we	O
propose	O
a	O
new	O
BERT	NLP-algorithm/tool
based	O
evaluation	O
metric	O
called	O
DEB	NLP-metrics
which	O
is	O
pretrained	O
on	O
727M	O
Reddit	O
conversations	O
DEB	NLP-metrics
then	O
finetuned	O
on	O
our	O
dataset	O
.	O

DEB	O
significantly	O
outperforms	O
existing	O
models	O
,	O
showing	O
better	O
correlation	O
with	O
human	O
judgments	O
and	O
better	O
performance	O
on	O
random	O
negatives	O
(	O
88	Numerical-result
.	Numerical-result
27	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
.	O

DEB	O
significantly	O
outperforms	O
existing	O
models	O
,	O
showing	O
better	O
correlation	O
with	O
human	O
judgments	O
and	O
better	O
performance	O
on	O
random	O
negatives	O
(	O
88	Numerical-result
.	Numerical-result
27	Numerical-result
\\%	Numerical-result
accuracy	Classification-metrics
.	O

We	O
describe	O
an	O
unsupervised	O
method	O
to	O
create	O
pseudo	O
-	O
parallel	O
corpora	O
for	O
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
MT	NLP-focus
)	NLP-focus
from	O
unaligned	O
text	O
.	O

We	O
use	O
multilingual	NLP-algorithm/tool
BERT	NLP-algorithm/tool
to	O
create	O
source	O
and	O
target	O
sentence	O
embeddings	O
for	O
nearest	O
-	O
neighbor	O
search	O
and	O
adapt	O
the	O
model	O
via	O
self	O
-	O
training	O
.	O

We	O
validate	O
our	O
technique	O
by	O
extracting	O
parallel	O
sentence	O
pairs	O
on	O
the	O
BUCC	O
2017	O
bitext	NLP-focus
mining	NLP-focus
task	O
and	O
observe	O
up	O
to	O
a	O
24	O
.	O
5	O
point	O
increase	O
(	O
absolute	O
)	O
in	O
F1	Classification-metrics
scores	O
over	O
previous	O
unsupervised	O
methods	O
.	O

We	O
validate	O
our	O
technique	O
by	O
extracting	O
parallel	O
sentence	O
pairs	O
on	O
the	O
BUCC	O
2017	O
bitext	NLP-focus
mining	NLP-focus
task	O
and	O
observe	O
up	O
to	O
a	O
24	O
.	O
5	O
point	O
increase	O
(	O
absolute	O
)	O
in	O
F1	Classification-metrics
scores	O
over	O
previous	O
unsupervised	O
methods	O
.	O

We	O
then	O
improve	O
an	O
XLM	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
unsupervised	NLP-algorithm/tool
neural	NLP-algorithm/tool
MT	NLP-algorithm/tool
system	NLP-algorithm/tool
pre	O
-	O
trained	O
on	O
Wikipedia	O
by	O
supplementing	O
it	O
with	O
pseudo	O
-	O
parallel	O
text	O
mined	O
from	O
the	O
same	O
corpus	O
,	O
boosting	O
unsupervised	NLP-focus
translation	NLP-focus
performance	O
by	O
up	O
to	O
3	Numerical-result
.	Numerical-result
5	Numerical-result
BLEU	NLP-metrics
on	O
the	O
WMT	O
’	O
14	O
French	O
-	O
English	O
and	O
WMT	O
’	O
16	O
German	O
-	O
English	O
tasks	O
and	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

We	O
then	O
improve	O
an	O
XLM	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
unsupervised	NLP-algorithm/tool
neural	NLP-algorithm/tool
MT	NLP-algorithm/tool
system	NLP-algorithm/tool
pre	O
-	O
trained	O
on	O
Wikipedia	O
by	O
supplementing	O
it	O
with	O
pseudo	O
-	O
parallel	O
text	O
mined	O
from	O
the	O
same	O
corpus	O
,	O
boosting	O
unsupervised	NLP-focus
translation	NLP-focus
performance	O
by	O
up	O
to	O
3	Numerical-result
.	Numerical-result
5	Numerical-result
BLEU	NLP-metrics
on	O
the	O
WMT	O
’	O
14	O
French	O
-	O
English	O
and	O
WMT	O
’	O
16	O
German	O
-	O
English	O
tasks	O
and	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

We	O
then	O
improve	O
an	O
XLM	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
unsupervised	NLP-algorithm/tool
neural	NLP-algorithm/tool
MT	NLP-algorithm/tool
system	NLP-algorithm/tool
pre	O
-	O
trained	O
on	O
Wikipedia	O
by	O
supplementing	O
it	O
with	O
pseudo	O
-	O
parallel	O
text	O
mined	O
from	O
the	O
same	O
corpus	O
,	O
boosting	O
unsupervised	NLP-focus
translation	NLP-focus
performance	O
by	O
up	O
to	O
3	Numerical-result
.	Numerical-result
5	Numerical-result
BLEU	NLP-metrics
on	O
the	O
WMT	O
’	O
14	O
French	O
-	O
English	O
and	O
WMT	O
’	O
16	O
German	O
-	O
English	O
tasks	O
and	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

We	O
then	O
improve	O
an	O
XLM	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
unsupervised	NLP-algorithm/tool
neural	NLP-algorithm/tool
MT	NLP-algorithm/tool
system	NLP-algorithm/tool
pre	O
-	O
trained	O
on	O
Wikipedia	O
by	O
supplementing	O
it	O
with	O
pseudo	O
-	O
parallel	O
text	O
mined	O
from	O
the	O
same	O
corpus	O
,	O
boosting	O
unsupervised	NLP-focus
translation	NLP-focus
performance	O
by	O
up	O
to	O
3	Numerical-result
.	Numerical-result
5	Numerical-result
BLEU	NLP-metrics
on	O
the	O
WMT	O
’	O
14	O
French	O
-	O
English	O
and	O
WMT	O
’	O
16	O
German	O
-	O
English	O
tasks	O
and	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

Finally	O
,	O
we	O
enrich	O
the	O
IWSLT	O
’	O
15	O
English	O
-	O
Vietnamese	O
corpus	O
with	O
pseudo	O
-	O
parallel	O
Wikipedia	O
sentence	O
pairs	O
yielding	O
a	O
1	Numerical-result
.	Numerical-result
2	Numerical-result
BLEU	NLP-metrics
improvement	O
on	O
the	O
low	NLP-focus
-	NLP-focus
resource	NLP-focus
MT	NLP-focus
task	O
.	O

Finally	O
,	O
we	O
enrich	O
the	O
IWSLT	O
’	O
15	O
English	O
-	O
Vietnamese	O
corpus	O
with	O
pseudo	O
-	O
parallel	O
Wikipedia	O
sentence	O
pairs	O
yielding	O
a	O
1	Numerical-result
.	Numerical-result
2	Numerical-result
BLEU	NLP-metrics
improvement	O
on	O
the	O
low	NLP-focus
-	NLP-focus
resource	NLP-focus
MT	NLP-focus
task	O
.	O

Finally	O
,	O
we	O
enrich	O
the	O
IWSLT	O
’	O
15	O
English	O
-	O
Vietnamese	O
corpus	O
with	O
pseudo	O
-	O
parallel	O
Wikipedia	O
sentence	O
pairs	O
yielding	O
a	O
1	Numerical-result
.	Numerical-result
2	Numerical-result
BLEU	NLP-metrics
improvement	O
on	O
the	O
low	NLP-focus
-	NLP-focus
resource	NLP-focus
MT	NLP-focus
task	O
.	O

We	O
demonstrate	O
that	O
unsupervised	NLP-algorithm/tool
bitext	NLP-algorithm/tool
mining	NLP-algorithm/tool
is	O
an	O
effective	O
way	O
of	O
augmenting	O
MT	NLP-focus
datasets	O
and	O
complements	O
existing	O
techniques	O
like	O
initializing	O
with	O
pre	O
-	O
trained	O
contextual	O
embeddings	O
.	O

We	O
demonstrate	O
that	O
unsupervised	NLP-algorithm/tool
bitext	NLP-algorithm/tool
mining	NLP-algorithm/tool
is	O
an	O
effective	O
way	O
of	O
augmenting	O
MT	NLP-focus
datasets	O
and	O
complements	O
existing	O
techniques	O
like	O
initializing	O
with	O
pre	O
-	O
trained	O
contextual	O
embeddings	O
.	O

This	O
paper	O
is	O
the	O
first	O
survey	O
of	O
over	O
150	O
studies	O
of	O
the	O
popular	O
BERT	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

We	O
review	O
the	O
current	O
state	O
of	O
knowledge	O
about	O
how	O
BERT	NLP-algorithm/tool
works	O
,	O
what	O
kind	O
of	O
information	O
it	O
learns	O
and	O
how	O
it	O
is	O
represented	O
,	O
common	O
modifications	O
to	O
its	O
training	O
objectives	O
and	O
architecture	O
,	O
the	O
overparameterization	O
issue	O
,	O
and	O
approaches	O
to	O
compression	O
.	O

Until	O
now	O
,	O
most	O
of	O
the	O
research	O
in	O
grammar	NLP-focus
error	NLP-focus
correction	NLP-focus
focused	O
on	O
English	O
,	O
and	O
the	O
problem	O
has	O
hardly	O
been	O
explored	O
for	O
other	O
languages	O
.	O

Although	O
impressive	O
results	O
have	O
recently	O
been	O
achieved	O
for	O
grammar	NLP-focus
error	NLP-focus
correction	NLP-focus
of	O
non	O
-	O
native	O
English	O
writing	O
,	O
these	O
results	O
are	O
limited	O
to	O
domains	O
where	O
plentiful	O
training	O
data	O
are	O
available	O
.	O

It	O
is	O
intuitive	O
that	O
semantic	O
representations	O
can	O
be	O
useful	O
for	O
machine	NLP-focus
translation	NLP-focus
mainly	O
because	O
they	O
can	O
help	O
in	O
enforcing	O
meaning	O
preservation	O
and	O
handling	O
data	O
sparsity	O
(	O
many	O
sentences	O
correspond	O
to	O
one	O
meaning	O
)	O
of	O
machine	NLP-algorithm/tool
translation	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

It	O
is	O
intuitive	O
that	O
semantic	O
representations	O
can	O
be	O
useful	O
for	O
machine	NLP-focus
translation	NLP-focus
mainly	O
because	O
they	O
can	O
help	O
in	O
enforcing	O
meaning	O
preservation	O
and	O
handling	O
data	O
sparsity	O
(	O
many	O
sentences	O
correspond	O
to	O
one	O
meaning	O
)	O
of	O
machine	NLP-algorithm/tool
translation	NLP-algorithm/tool
models	NLP-algorithm/tool
.	O

On	O
the	O
other	O
hand	O
,	O
little	O
work	O
has	O
been	O
done	O
on	O
leveraging	O
semantics	O
for	O
neural	NLP-focus
machine	NLP-focus
translation	NLP-focus
(	NLP-focus
NMT	NLP-focus
)	NLP-focus
.	O

In	O
this	O
work	O
,	O
we	O
study	O
the	O
usefulness	O
of	O
AMR	O
(	O
abstract	O
meaning	O
representation	O
)	O
on	O
NMT	NLP-focus
.	O

Experiments	O
on	O
a	O
standard	O
English	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
German	NLP-focus
dataset	O
show	O
that	O
incorporating	O
AMR	O
as	O
additional	O
knowledge	O
can	O
significantly	O
improve	O
a	O
strong	O
attention	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
sequence	NLP-algorithm/tool
-	NLP-algorithm/tool
to	NLP-algorithm/tool
-	NLP-algorithm/tool
sequence	NLP-algorithm/tool
neural	NLP-algorithm/tool
translation	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

Experiments	O
on	O
a	O
standard	O
English	NLP-focus
-	NLP-focus
to	NLP-focus
-	NLP-focus
German	NLP-focus
dataset	O
show	O
that	O
incorporating	O
AMR	O
as	O
additional	O
knowledge	O
can	O
significantly	O
improve	O
a	O
strong	O
attention	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
sequence	NLP-algorithm/tool
-	NLP-algorithm/tool
to	NLP-algorithm/tool
-	NLP-algorithm/tool
sequence	NLP-algorithm/tool
neural	NLP-algorithm/tool
translation	NLP-algorithm/tool
model	NLP-algorithm/tool
.	O

In	O
this	O
research	O
,	O
we	O
propose	O
a	O
Reliability	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
driven	Data/Mining/Information/Retrieval-technique
Multi	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
view	Data/Mining/Information/Retrieval-technique
Graph	Data/Mining/Information/Retrieval-technique
Embedding	Data/Mining/Information/Retrieval-technique
framework	Data/Mining/Information/Retrieval-technique
for	Data/Mining/Information/Retrieval-technique
Truth	Data/Mining/Information/Retrieval-technique
inference	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
TiReMGE	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
which	O
explores	O
multiple	O
crowdsourced	O
relationships	O
by	O
organically	O
integrating	O
worker	O
reliabilities	O
into	O
a	O
graph	O
space	O
that	O
is	O
constructed	O
from	O
crowdsourced	O
triples	O
.	O

Extensive	O
experimental	O
results	O
on	O
nine	O
real	O
-	O
world	O
datasets	O
demonstrate	O
that	O
TiReMGE	Data/Mining/Information/Retrieval-technique
significantly	O
outperforms	O
the	O
nine	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
.	O

Moreover	O
,	O
streaming	O
versions	O
of	O
Tucker	O
decomposition	O
are	O
still	O
time	O
-	O
consuming	O
to	O
deal	O
with	O
newly	O
arrived	O
tensors	O
We	O
propose	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Tucker	Data/Mining/Information/Retrieval-technique
and	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
TuckerO	Data/Mining/Information/Retrieval-technique
Tucker	O
decomposition	O
omposition	O
methods	O
for	O
large	O
dense	O
tensors	O
in	O
static	O
and	O
online	O
streaming	O
settings	O
respectively	O
.	O

By	O
decomposing	O
a	O
given	O
large	O
dense	O
tensor	O
with	O
randomized	O
singular	O
value	O
decomposition	O
avoiding	O
the	O
reconstruction	O
from	O
SVD	O
results	O
,	O
and	O
carefully	O
determining	O
the	O
order	O
of	O
operations	O
,	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Tucker	Data/Mining/Information/Retrieval-technique
and	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
TuckerO	Data/Mining/Information/Retrieval-technique
efficiently	O
obtain	O
factor	O
matrices	O
and	O
core	O
tensor	O
.	O

Experimental	O
results	O
show	O
that	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Tucker	Data/Mining/Information/Retrieval-technique
achieves	O
up	O
to	O
38	Numerical-result
.	Numerical-result
4	Numerical-result
\	O
texttimes	O
{}	O
faster	O
running	O
times	O
and	O
requires	O
up	O
to	O
17	Numerical-result
.	Numerical-result
2	Numerical-result
\	O
texttimes	O
{}	O
less	O
space	O
than	O
existing	O
methods	O
while	O
having	O
similar	O
accuracy	Classification-metrics
.	O

Experimental	O
results	O
show	O
that	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Tucker	Data/Mining/Information/Retrieval-technique
achieves	O
up	O
to	O
38	Numerical-result
.	Numerical-result
4	Numerical-result
\	O
texttimes	O
{}	O
faster	O
running	O
times	O
and	O
requires	O
up	O
to	O
17	Numerical-result
.	Numerical-result
2	Numerical-result
\	O
texttimes	O
{}	O
less	O
space	O
than	O
existing	O
methods	O
while	O
having	O
similar	O
accuracy	Classification-metrics
.	O

Experimental	O
results	O
show	O
that	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Tucker	Data/Mining/Information/Retrieval-technique
achieves	O
up	O
to	O
38	Numerical-result
.	Numerical-result
4	Numerical-result
\	O
texttimes	O
{}	O
faster	O
running	O
times	O
and	O
requires	O
up	O
to	O
17	Numerical-result
.	Numerical-result
2	Numerical-result
\	O
texttimes	O
{}	O
less	O
space	O
than	O
existing	O
methods	O
while	O
having	O
similar	O
accuracy	Classification-metrics
.	O

Furthermore	O
,	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
TuckerO	Data/Mining/Information/Retrieval-technique
is	O
up	O
to	O
6	Numerical-result
.	Numerical-result
1	Numerical-result
texttimes	O
{}	O
faster	O
than	O
existing	O
streaming	O
methods	O
for	O
each	O
newly	O
arrived	O
tensor	O
while	O
its	O
running	O
time	O
is	O
proportional	O
to	O
the	O
size	O
of	O
the	O
newly	O
arrived	O
tensor	O
,	O
not	O
the	O
accumulated	O
tensor	O
.	O

Furthermore	O
,	O
D	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
TuckerO	Data/Mining/Information/Retrieval-technique
is	O
up	O
to	O
6	Numerical-result
.	Numerical-result
1	Numerical-result
texttimes	O
{}	O
faster	O
than	O
existing	O
streaming	O
methods	O
for	O
each	O
newly	O
arrived	O
tensor	O
while	O
its	O
running	O
time	O
is	O
proportional	O
to	O
the	O
size	O
of	O
the	O
newly	O
arrived	O
tensor	O
,	O
not	O
the	O
accumulated	O
tensor	O
.	O

In	O
this	O
article	O
,	O
we	O
make	O
use	O
of	O
multi	O
-	O
sourced	O
urban	O
data	O
to	O
develop	O
a	O
data	O
-	O
driven	O
framework	O
U	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Evolve	Data/Mining/Information/Retrieval-technique
to	O
investigate	O
urban	O
vibrancy	O
evolution	O
.	O

Then	O
,	O
we	O
analyze	O
the	O
contextual	O
features	O
and	O
graph	O
patterns	O
of	O
multi	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
view	Data/Mining/Information/Retrieval-technique
time	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
dependent	Data/Mining/Information/Retrieval-technique
graphs	Data/Mining/Information/Retrieval-technique
in	O
terms	O
of	O
informing	O
future	O
urban	O
vibrancy	O
variations	O
.	O

The	O
U	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Evolve	Data/Mining/Information/Retrieval-technique
framework	O
has	O
also	O
been	O
deployed	O
in	O
the	O
production	O
environment	O
to	O
deliver	O
real	O
-	O
world	O
urban	O
development	O
and	O
planning	O
insights	O
for	O
various	O
cities	O
in	O
China	O
.	O

In	O
this	O
work	O
,	O
we	O
design	O
a	O
novel	O
GMVC	O
framework	O
via	O
cOmmoNality	Data/Mining/Information/Retrieval-technique
and	Data/Mining/Information/Retrieval-technique
Individuality	Data/Mining/Information/Retrieval-technique
discOvering	Data/Mining/Information/Retrieval-technique
in	Data/Mining/Information/Retrieval-technique
lateNt	Data/Mining/Information/Retrieval-technique
subspace	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
ONION	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
seeking	O
for	O
a	O
robust	O
and	O
discriminative	O
subspace	O
representation	O
GMVC	O
tible	O
across	O
multiple	O
features	O
for	O
GMVC	O
.	O

Anomaly	Data/Mining/Information/Retrieval-technique
detection	Data/Mining/Information/Retrieval-technique
on	O
multivariate	O
time	O
series	O
(	O
MTS	O
)	O
is	O
an	O
important	O
research	O
topic	O
in	O
data	O
mining	O
which	O
has	O
a	O
wide	O
range	O
of	O
applications	O
in	O
information	O
technology	O
financial	O
management	O
manufacturing	O
system	O
and	O
so	O
on	O
.	O

In	O
this	O
article	O
,	O
we	O
propose	O
a	O
novel	O
Self	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Training	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
Anomaly	Data/Mining/Information/Retrieval-technique
Detection	Data/Mining/Information/Retrieval-technique
with	Data/Mining/Information/Retrieval-technique
Generative	Data/Mining/Information/Retrieval-technique
Adversarial	Data/Mining/Information/Retrieval-technique
Network	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
GAN	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
model	O
called	O
STAD	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
GAN	Data/Mining/Information/Retrieval-technique
to	O
address	O
the	O
practical	O
challenge	O
.	O

The	O
STAD	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
GAN	Data/Mining/Information/Retrieval-technique
model	O
consists	O
of	O
a	O
generator	O
-	O
discriminator	O
structure	O
for	O
adversarial	AI/ML/DL-domain
learning	AI/ML/DL-domain
and	O
a	O
neural	O
network	O
classifier	O
for	O
anomaly	O
classification	O
.	O

The	O
STAD	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
GAN	Data/Mining/Information/Retrieval-technique
model	O
consists	O
of	O
a	O
generator	O
-	O
discriminator	O
structure	O
for	O
adversarial	AI/ML/DL-domain
learning	AI/ML/DL-domain
and	O
a	O
neural	O
network	O
classifier	O
for	O
anomaly	O
classification	O
.	O

Extensive	O
experiments	O
based	O
on	O
six	O
open	O
MTS	O
datasets	O
show	O
that	O
STAD	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
GAN	Data/Mining/Information/Retrieval-technique
is	O
robust	O
to	O
noise	O
and	O
achieves	O
significant	O
performance	O
improvement	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

Ensemble	O
methods	O
have	O
been	O
used	O
for	O
multi	O
-	O
label	O
classification	O
but	O
few	O
methods	O
consider	O
both	O
the	O
accuracy	Classification-metrics
and	O
diversity	O
of	O
base	O
classifiers	O
.	O

To	O
address	O
the	O
above	O
-	O
mentioned	O
problem	O
,	O
a	O
Weighted	Data/Mining/Information/Retrieval-technique
Ensemble	Data/Mining/Information/Retrieval-technique
classification	Data/Mining/Information/Retrieval-technique
algorithm	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
on	Data/Mining/Information/Retrieval-technique
Nearest	Data/Mining/Information/Retrieval-technique
Neighbors	Data/Mining/Information/Retrieval-technique
for	Data/Mining/Information/Retrieval-technique
Multi	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Label	Data/Mining/Information/Retrieval-technique
data	Data/Mining/Information/Retrieval-technique
stream	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
WENNML	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
is	O
proposed	O
.	O

WENNML	Data/Mining/Information/Retrieval-technique
uses	O
data	O
blocks	O
to	O
train	O
Active	O
candidate	O
Ensemble	O
Classifiers	O
(	O
AEC	O
)	O
and	O
Passive	O
candidate	O
Ensemble	O
Classifiers	O
(	O
PEC	O
)	O
.	O

When	O
the	O
difference	O
value	O
between	O
the	O
number	O
of	O
current	O
instances	O
and	O
the	O
number	O
of	O
warning	O
instances	O
reaches	O
the	O
passive	O
warning	O
value	O
,	O
the	O
algorithm	O
selects	O
the	O
optimal	O
base	O
classifiers	O
from	O
AEC	O
and	O
PEC	O
according	O
to	O
the	O
subset	Classification-metrics
accuracy	Classification-metrics
and	O
hamming	Classification-metrics
score	Classification-metrics
and	O
puts	O
them	O
into	O
the	O
predictive	O
ensemble	O
classifiers	O
.	O

The	O
results	O
show	O
that	O
WENNML	Data/Mining/Information/Retrieval-technique
achieves	O
the	O
best	O
average	O
rankings	O
among	O
the	O
four	O
evaluation	O
metrics	O
.	O

In	O
this	O
study	O
,	O
sentiment	NLP-focus
classification	NLP-focus
and	O
emotion	NLP-focus
distribution	NLP-focus
learning	NLP-focus
across	O
domains	O
are	O
both	O
formulated	O
as	O
a	O
semi	O
-	O
supervised	O
domain	O
adaptation	O
problem	O
which	O
utilizes	O
a	O
small	O
amount	O
of	O
labeled	O
documents	O
in	O
the	O
target	O
domain	O
for	O
model	O
training	O
.	O

By	O
introducing	O
a	O
shared	O
matrix	O
that	O
captures	O
the	O
stable	O
association	O
between	O
document	O
clusters	O
and	O
word	O
clusters	O
,	O
non	O
-	O
negative	O
matrix	O
tri	O
-	O
factorization	O
(	O
NMTF	O
)	O
is	O
robust	O
to	O
the	O
labeled	O
target	O
domain	O
data	O
and	O
has	O
shown	O
remarkable	O
performance	O
in	O
cross	NLP-focus
-	NLP-focus
domain	NLP-focus
text	NLP-focus
classification	NLP-focus
.	O

To	O
address	O
these	O
issues	O
,	O
we	O
propose	O
a	O
semi	O
-	O
supervised	O
NMTF	O
framework	O
for	O
sentiment	NLP-focus
classification	NLP-focus
and	O
emotion	NLP-focus
distribution	NLP-focus
learning	O
across	O
domains	O
.	O

Though	O
this	O
manner	O
can	O
enlarge	O
the	O
receptive	O
field	O
receptive	O
field	O
ical	O
problems	O
unsolved	O
:	O
how	O
to	O
find	O
the	O
suitable	O
receptive	O
field	O
to	O
avoid	O
under	O
-	O
smoothing	O
or	O
over	O
-	O
smoothing	O
and	O
how	O
to	O
balance	O
different	O
diffusion	O
operators	O
for	O
better	O
capturing	O
the	O
local	O
and	O
global	O
dependencies	O
We	O
tackle	O
these	O
challenges	O
and	O
propose	O
a	O
Scalable	Data/Mining/Information/Retrieval-technique
,	Data/Mining/Information/Retrieval-technique
Adaptive	Data/Mining/Information/Retrieval-technique
Graph	Data/Mining/Information/Retrieval-technique
Convolutional	Data/Mining/Information/Retrieval-technique
Networks	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
SAGCN	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
with	O
Transformer	O
architecture	O
.	O

Furthermore	O
,	O
we	O
devise	O
smooth2seq	Data/Mining/Information/Retrieval-technique
and	O
diffusion	O
-	O
based	O
position	O
schemes	O
introduced	O
into	O
Transformer	O
architecture	O
for	O
better	O
capturing	O
local	O
and	O
global	O
information	O
among	O
embeddings	O
.	O

Experimental	O
results	O
show	O
that	O
SAGCN	Data/Mining/Information/Retrieval-technique
enjoys	O
high	O
accuracy	Classification-metrics
scalability	O
and	O
efficiency	O
on	O
various	O
open	O
benchmarks	O
and	O
is	O
competitive	O
with	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
competitors	O
.	O

Experimental	O
results	O
show	O
that	O
SAGCN	Data/Mining/Information/Retrieval-technique
enjoys	O
high	O
accuracy	Classification-metrics
scalability	O
and	O
efficiency	O
on	O
various	O
open	O
benchmarks	O
and	O
is	O
competitive	O
with	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
competitors	O
.	O

This	O
article	O
proposes	O
a	O
novel	O
algorithm	O
called	O
truth	Data/Mining/Information/Retrieval-technique
inference	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
on	Data/Mining/Information/Retrieval-technique
label	Data/Mining/Information/Retrieval-technique
confidence	Data/Mining/Information/Retrieval-technique
clustering	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
TILCC	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
to	O
improve	O
the	O
quality	O
of	O
integrated	O
labels	O
for	O
the	O
single	O
-	O
choice	O
classification	O
problem	O
in	O
crowdsourcing	O
labeling	O
tasks	O
.	O

Compared	O
with	O
the	O
performances	O
of	O
six	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
MV	O
ZenCrowd	O
PM	O
CATD	O
GLAD	O
and	O
GTIC	O
on	O
12	O
randomly	O
selected	O
real	O
-	O
world	O
datasets	O
the	O
performance	O
of	O
our	O
algorithm	O
showed	O
many	O
advantages	O
:	O
no	O
need	O
to	O
set	O
complex	O
parameters	O
faster	O
running	O
speed	O
,	O
and	O
significantly	O
higher	O
accuracy	Classification-metrics
.	O

In	O
this	O
article	O
,	O
we	O
introduce	O
DiVA	Data/Mining/Information/Retrieval-technique
—	Data/Mining/Information/Retrieval-technique
Diffusion	Data/Mining/Information/Retrieval-technique
Visualization	Data/Mining/Information/Retrieval-technique
and	Data/Mining/Information/Retrieval-technique
Analysis	Data/Mining/Information/Retrieval-technique
a	O
tool	O
that	O
provides	O
a	O
scalable	O
web	O
interface	O
and	O
extendable	O
APIs	O
to	O
analyze	O
various	O
diffusion	O
trends	O
on	O
networks	O
.	O

Along	O
with	O
performing	O
an	O
exhaustive	O
feature	O
comparison	O
and	O
system	O
evaluation	O
of	O
DiVA	O
against	O
publicly	O
-	O
available	O
web	O
interfaces	O
for	O
information	O
diffusion	O
DiVA	Data/Mining/Information/Retrieval-technique
onducted	O
a	O
user	O
study	O
to	O
understand	O
the	O
strengths	O
and	O
limitations	O
of	O
DiVA	O
.	O

Guided	O
by	O
a	O
key	O
observation	O
that	O
the	O
passenger	O
inflows	O
and	O
arrival	O
flows	O
at	O
different	O
MRT	O
stations	O
and	O
time	O
are	O
spatio	O
-	O
temporally	O
correlated	O
due	O
to	O
behavioral	O
consistency	O
of	O
MRT	O
riders	O
,	O
we	O
design	O
and	O
implement	O
a	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
based	O
solution	O
,	O
CrowdAtlas	Data/Mining/Information/Retrieval-technique
that	O
captures	O
MRT	O
riders	O
’	O
transition	O
probabilities	O
among	O
stations	O
and	O
across	O
time	O
,	O
and	O
based	O
on	O
that	O
accurately	O
estimates	O
the	O
crowd	O
distribution	O
within	O
the	O
MRT	O
system	O
.	O

Guided	O
by	O
a	O
key	O
observation	O
that	O
the	O
passenger	O
inflows	O
and	O
arrival	O
flows	O
at	O
different	O
MRT	O
stations	O
and	O
time	O
are	O
spatio	O
-	O
temporally	O
correlated	O
due	O
to	O
behavioral	O
consistency	O
of	O
MRT	O
riders	O
,	O
we	O
design	O
and	O
implement	O
a	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
based	O
solution	O
,	O
CrowdAtlas	Data/Mining/Information/Retrieval-technique
that	O
captures	O
MRT	O
riders	O
’	O
transition	O
probabilities	O
among	O
stations	O
and	O
across	O
time	O
,	O
and	O
based	O
on	O
that	O
accurately	O
estimates	O
the	O
crowd	O
distribution	O
within	O
the	O
MRT	O
system	O
.	O

Our	O
comprehensive	O
performance	O
evaluations	O
with	O
both	O
trace	O
-	O
driven	O
studies	O
and	O
real	O
-	O
world	O
experiments	O
in	O
MRT	O
disruption	O
cases	O
demonstrate	O
the	O
effectiveness	O
of	O
CrowdAtlas	Data/Mining/Information/Retrieval-technique
.	O

We	O
present	O
GRASP	Data/Mining/Information/Retrieval-technique
a	O
method	O
that	O
first	O
establishes	O
a	O
correspondence	O
between	O
functions	O
derived	O
from	O
Laplacian	O
matrix	O
eigenvectors	O
which	O
capture	O
multiscale	O
structural	O
characteristics	O
and	O
then	O
exploits	O
this	O
correspondence	O
to	O
align	O
nodes	O
.	O

We	O
enhance	O
the	O
basic	O
form	O
of	O
GRASP	Data/Mining/Information/Retrieval-technique
by	O
altering	O
two	O
of	O
its	O
components	O
,	O
namely	O
the	O
embedding	O
method	O
and	O
the	O
assignment	O
procedure	O
it	O
employs	O
,	O
leveraging	O
its	O
modular	O
,	O
hence	O
adaptable	O
design	O
.	O

Our	O
experimental	O
study	O
,	O
featuring	O
noise	O
levels	O
higher	O
than	O
anything	O
used	O
in	O
previous	O
studies	O
,	O
shows	O
that	O
the	O
enhanced	O
form	O
of	O
GRASP	Data/Mining/Information/Retrieval-technique
outperforms	O
scalable	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
graph	O
alignment	O
across	O
noise	O
levels	O
and	O
graph	O
types	O
,	O
and	O
performs	O
competitively	O
with	O
respect	O
to	O
the	O
best	O
non	O
-	O
scalable	O
ones	O
.	O

Our	O
results	O
show	O
that	O
the	O
proposed	O
estimators	O
reduce	O
biases	O
induced	O
by	O
private	O
nodes	O
in	O
the	O
existing	O
estimators	O
by	O
up	O
to	O
92	Numerical-result
.	Numerical-result
6	Numerical-result
%	Numerical-result
on	O
social	O
network	O
datasets	O
private	O
nodes	O
ate	O
nodes	O
.	O

We	O
propose	O
a	O
novel	O
theoretical	O
principled	O
framework	O
lifelong	Data/Mining/Information/Retrieval-technique
online	Data/Mining/Information/Retrieval-technique
learning	Data/Mining/Information/Retrieval-technique
where	O
the	O
learning	O
process	O
for	O
each	O
task	O
is	O
in	O
an	O
incremental	O
manner	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
dynamic	O
multi	O
-	O
view	O
graph	O
neural	O
network	O
for	O
citywide	O
traffic	O
inference	O
with	O
the	O
method	O
CTVI	Data/Mining/Information/Retrieval-technique
+	Data/Mining/Information/Retrieval-technique
.	O

In	O
our	O
evaluation	O
,	O
CTVI	Data/Mining/Information/Retrieval-technique
+	Data/Mining/Information/Retrieval-technique
achieves	O
consistent	O
better	O
performance	O
compared	O
with	O
different	O
baselines	O
on	O
real	O
-	O
world	O
traffic	O
volume	O
datasets	O
.	O

Further	O
ablation	O
study	O
validates	O
the	O
effectiveness	O
of	O
key	O
components	O
in	O
CTVI	Data/Mining/Information/Retrieval-technique
+	Data/Mining/Information/Retrieval-technique
.	O

Previous	O
research	O
considers	O
synchronous	O
influences	O
from	O
spatial	O
and	O
temporal	O
relationships	O
in	O
a	O
homogeneous	O
fashion	O
while	O
compound	O
spatial	O
relationships	O
are	O
not	O
captured	O
for	O
this	O
synchronicity	O
To	O
address	O
the	O
aforementioned	O
perspectives	O
,	O
we	O
propose	O
the	O
Spatio	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Temporal	Data/Mining/Information/Retrieval-technique
Heterogeneous	Data/Mining/Information/Retrieval-technique
graph	Data/Mining/Information/Retrieval-technique
Attention	Data/Mining/Information/Retrieval-technique
Network	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
STHAN	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
compound	O
spatial	O
relationships	O
turing	O
the	O
compound	O
spatial	O
relationships	O
via	O
meta	O
-	O
paths	O
explicitly	O
.	O

Our	O
framework	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
by	O
reducing	O
6	Numerical-result
.	Numerical-result
58	Numerical-result
%	Numerical-result
4	Numerical-result
.	Numerical-result
57	Numerical-result
%	Numerical-result
and	O
4	Numerical-result
.	Numerical-result
20	Numerical-result
%	Numerical-result
of	O
WMAPE	Statistical/Mathematical-metrics
in	O
experiments	O
on	O
three	O
real	O
-	O
world	O
datasets	O
respectively	O
.	O

Our	O
framework	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
by	O
reducing	O
6	Numerical-result
.	Numerical-result
58	Numerical-result
%	Numerical-result
4	Numerical-result
.	Numerical-result
57	Numerical-result
%	Numerical-result
and	O
4	Numerical-result
.	Numerical-result
20	Numerical-result
%	Numerical-result
of	O
WMAPE	Statistical/Mathematical-metrics
in	O
experiments	O
on	O
three	O
real	O
-	O
world	O
datasets	O
respectively	O
.	O

Then	O
,	O
we	O
formalize	O
the	O
problem	O
and	O
propose	O
an	O
efficient	O
solution	O
,	O
namely	O
MIRROR	Data/Mining/Information/Retrieval-technique
a	O
graph	O
convolutional	O
network	O
(	O
GCN	O
)	O
model	O
to	O
infer	O
implicit	O
ties	O
under	O
explicit	O
connections	O
.	O

MIRROR	Data/Mining/Information/Retrieval-technique
captures	O
rich	O
information	O
in	O
learning	O
node	O
-	O
level	O
representations	O
by	O
incorporating	O
attributes	O
from	O
heterogeneous	O
neighbors	O
.	O

Furthermore	O
,	O
MIRROR	Data/Mining/Information/Retrieval-technique
is	O
tolerant	O
of	O
missing	O
n	O
ode	O
attribute	O
information	O
because	O
it	O
is	O
able	O
to	O
utilize	O
network	O
structure	O
.	O

We	O
empirically	O
evaluate	O
MIRROR	Data/Mining/Information/Retrieval-technique
on	O
four	O
different	O
genres	O
of	O
networks	O
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
for	O
target	O
relations	O
mining	O
.	O

The	O
underlying	O
information	O
revealed	O
by	O
MIRROR	Data/Mining/Information/Retrieval-technique
contributes	O
to	O
enriching	O
existing	O
knowledge	O
and	O
leading	O
to	O
novel	O
domain	O
insights	O
.	O

To	O
this	O
end	O
,	O
in	O
this	O
article	O
,	O
we	O
present	O
a	O
GRM	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
RTrip	Data/Mining/Information/Retrieval-technique
(	O
short	O
for	O
Graph	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
Representation	Data/Mining/Information/Retrieval-technique
Method	Data/Mining/Information/Retrieval-technique
for	Data/Mining/Information/Retrieval-technique
Reinforce	Data/Mining/Information/Retrieval-technique
Trip	Data/Mining/Information/Retrieval-technique
Recommendation	Data/Mining/Information/Retrieval-technique
framework	O
.	O

GRM	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
RTrip	Data/Mining/Information/Retrieval-technique
learns	O
POI	O
representations	O
from	O
incoming	O
and	O
outgoing	O
views	O
to	O
obtain	O
asymmetric	O
POI	O
-	O
POI	O
transition	O
probability	O
via	O
POI	O
-	O
POI	O
graph	O
networks	O
and	O
then	O
fuses	O
the	O
trained	O
POI	O
representation	O
into	O
a	O
user	O
-	O
POI	O
graph	O
network	O
to	O
estimate	O
user	O
preferences	O
.	O

Finally	O
,	O
after	O
formulating	O
the	O
personalized	O
trip	O
recommendation	O
as	O
a	O
Markov	O
Decision	O
Process	O
(	O
MDP	O
)	O
we	O
utilize	O
a	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
algorithm	O
for	O
generating	O
a	O
personalized	O
trip	O
with	O
maximal	O
user	O
travel	O
experience	O
.	O

Extensive	O
experiments	O
are	O
performed	O
on	O
the	O
public	O
datasets	O
and	O
the	O
results	O
demonstrate	O
the	O
superiority	O
of	O
GRM	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
RTrip	Data/Mining/Information/Retrieval-technique
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
trip	O
recommendation	O
methods	O
.	O

A	O
novel	O
network	O
(	O
PDGCN	Data/Mining/Information/Retrieval-technique
is	O
proposed	O
to	O
learn	O
the	O
representations	O
of	O
users	O
and	O
items	O
in	O
these	O
dynamic	O
graphs	O
.	O

The	O
results	O
indicate	O
that	O
our	O
approach	O
outperforms	O
several	O
competing	O
methods	O
in	O
terms	O
of	O
Hit	Statistical/Mathematical-metrics
Ratio	Statistical/Mathematical-metrics
(	Statistical/Mathematical-metrics
HR	Statistical/Mathematical-metrics
)	Statistical/Mathematical-metrics
and	O
Normalized	Statistical/Mathematical-metrics
Discounted	Statistical/Mathematical-metrics
Cumulative	Statistical/Mathematical-metrics
Gain	Statistical/Mathematical-metrics
(	Statistical/Mathematical-metrics
NDCG	Statistical/Mathematical-metrics
)	Statistical/Mathematical-metrics
.	O

To	O
this	O
end	O
,	O
we	O
design	O
an	O
effective	O
solution	O
of	O
Simultaneous	Data/Mining/Information/Retrieval-technique
Demand	Data/Mining/Information/Retrieval-technique
Prediction	Data/Mining/Information/Retrieval-technique
And	Data/Mining/Information/Retrieval-technique
Planning	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
SPAP	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
discriminative	O
features	O
are	O
extracted	O
from	O
multi	O
-	O
source	O
data	O
,	O
and	O
fed	O
into	O
an	O
Attention	O
-	O
based	O
Spatial	O
-	O
Temporal	O
City	O
Domain	O
Adaptation	O
Network	O
(	O
AST	O
-	O
CDAN	O
)	O
for	O
cross	O
-	O
city	O
demand	O
prediction	O
a	O
novel	O
Transfer	O
Iterative	O
Optimization	O
(	O
TIO	O
)	O
algorithm	O
is	O
designed	O
for	O
charger	O
planning	O
AST	O
-	O
CDAN	O
ively	O
utilizing	O
AST	O
-	O
CDAN	O
and	O
a	O
charger	O
plan	O
fine	O
-	O
tuning	O
algorithm	O
.	O

Extensive	O
experiments	O
on	O
real	O
-	O
world	O
datasets	O
collected	O
from	O
three	O
cities	O
in	O
China	O
validate	O
the	O
effectiveness	O
and	O
efficiency	O
of	O
SPAP	Data/Mining/Information/Retrieval-technique
.	O

Specially	O
,	O
SPAP	Data/Mining/Information/Retrieval-technique
improves	O
at	O
most	O
72	Numerical-result
.	Numerical-result
5	Numerical-result
%	Numerical-result
revenue	O
compared	O
with	O
the	O
real	O
-	O
world	O
charger	O
deployment	O
.	O

Specially	O
,	O
SPAP	Data/Mining/Information/Retrieval-technique
improves	O
at	O
most	O
72	Numerical-result
.	Numerical-result
5	Numerical-result
%	Numerical-result
revenue	O
compared	O
with	O
the	O
real	O
-	O
world	O
charger	O
deployment	O
.	O

In	O
this	O
article	O
,	O
we	O
propose	O
GRACE	Data/Mining/Information/Retrieval-technique
an	O
extended	O
graph	O
convolution	O
framework	O
for	O
AGC	O
tasks	O
.	O

By	O
building	O
suitable	O
graph	O
Laplacians	O
for	O
each	O
of	O
the	O
aforementioned	O
graph	O
types	O
,	O
GRACE	Data/Mining/Information/Retrieval-technique
can	O
seamlessly	O
perform	O
graph	O
convolution	O
on	O
node	O
attributes	O
to	O
fuse	O
all	O
available	O
information	O
for	O
clustering	O
.	O

The	O
experimental	O
results	O
show	O
that	O
GRACE	Data/Mining/Information/Retrieval-technique
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
AGC	O
methods	O
on	O
the	O
different	O
graph	O
types	O
in	O
terms	O
of	O
clustering	O
quality	O
,	O
time	O
,	O
and	O
memory	O
usage	O
.	O

Second	O
,	O
we	O
propose	O
a	O
Coordinated	Data/Mining/Information/Retrieval-technique
Decision	Data/Mining/Information/Retrieval-technique
of	Data/Mining/Information/Retrieval-technique
Loading	Data/Mining/Information/Retrieval-technique
and	Data/Mining/Information/Retrieval-technique
Routing	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
CDLR	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
mechanism	O
to	O
determine	O
the	O
loading	O
rate	O
dynamically	O
after	O
the	O
vehicle	O
returns	O
to	O
the	O
depot	O
,	O
thus	O
avoiding	O
the	O
influence	O
of	O
improper	O
loading	O
rate	O
settings	O
.	O

Finally	O
,	O
the	O
model	O
equipped	O
with	O
a	O
GNN	O
encoder	O
and	O
CDLR	Data/Mining/Information/Retrieval-technique
simultaneously	O
can	O
adapt	O
to	O
the	O
changes	O
in	O
the	O
proportion	O
of	O
deliveries	O
and	O
pickups	O
.	O

To	O
efficiently	O
mine	O
patterns	O
,	O
this	O
article	O
proposes	O
the	O
ONP	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Miner	Data/Mining/Information/Retrieval-technique
algorithm	O
,	O
which	O
employs	O
depth	O
-	O
first	O
and	O
backtracking	O
strategies	O
to	O
calculate	O
the	O
support	O
.	O

Therefore	O
,	O
ONP	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Miner	Data/Mining/Information/Retrieval-technique
can	O
effectively	O
avoid	O
creating	O
redundant	O
nodes	O
and	O
parent	O
-	O
child	O
relationships	O
.	O

Moreover	O
,	O
to	O
effectively	O
reduce	O
the	O
number	O
of	O
candidate	O
patterns	O
,	O
ONP	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Miner	Data/Mining/Information/Retrieval-technique
uses	O
pattern	O
join	O
and	O
pruning	O
strategies	O
to	O
generate	O
and	O
further	O
prune	O
the	O
candidate	O
patterns	O
,	O
respectively	O
.	O

Experimental	O
results	O
show	O
that	O
ONP	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Miner	Data/Mining/Information/Retrieval-technique
not	O
only	O
improves	O
the	O
mining	O
efficiency	O
but	O
also	O
has	O
better	O
mining	O
performance	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
algorithms	O
.	O

More	O
importantly	O
,	O
ONP	Data/Mining/Information/Retrieval-technique
mining	Data/Mining/Information/Retrieval-technique
can	O
find	O
more	O
interesting	O
patterns	O
in	O
traffic	O
volume	O
data	O
to	O
predict	O
future	O
traffic	O
.	O

In	O
this	O
article	O
,	O
we	O
present	O
LB	Data/Mining/Information/Retrieval-technique
–	Data/Mining/Information/Retrieval-technique
GDM	Data/Mining/Information/Retrieval-technique
a	O
novel	O
approach	O
that	O
leverages	O
Geometric	AI/ML/DL-domain
Deep	AI/ML/DL-domain
Learning	AI/ML/DL-domain
to	O
tackle	O
the	O
link	O
-	O
building	O
problem	O
.	O

In	O
this	O
article	O
,	O
we	O
present	O
LB	Data/Mining/Information/Retrieval-technique
–	Data/Mining/Information/Retrieval-technique
GDM	Data/Mining/Information/Retrieval-technique
a	O
novel	O
approach	O
that	O
leverages	O
Geometric	AI/ML/DL-domain
Deep	AI/ML/DL-domain
Learning	AI/ML/DL-domain
to	O
tackle	O
the	O
link	O
-	O
building	O
problem	O
.	O

To	O
validate	O
our	O
proposal	O
,	O
31	O
real	O
-	O
world	O
networks	O
were	O
considered	O
;	O
tests	O
show	O
that	O
LB	Data/Mining/Information/Retrieval-technique
–	Data/Mining/Information/Retrieval-technique
GDM	Data/Mining/Information/Retrieval-technique
performs	O
significantly	O
better	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
heuristics	O
,	O
while	O
having	O
a	O
comparable	O
or	O
even	O
lower	O
computational	O
complexity	O
which	O
allows	O
it	O
to	O
scale	O
well	O
even	O
to	O
large	O
networks	O
.	O

To	O
this	O
end	O
,	O
we	O
propose	O
a	O
general	O
and	O
robust	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
based	O
model	O
,	O
L2MM	Data/Mining/Information/Retrieval-technique
to	O
tackle	O
these	O
issues	O
at	O
all	O
.	O

To	O
this	O
end	O
,	O
we	O
propose	O
a	O
general	O
and	O
robust	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
based	O
model	O
,	O
L2MM	Data/Mining/Information/Retrieval-technique
to	O
tackle	O
these	O
issues	O
at	O
all	O
.	O

Extensive	O
experiments	O
are	O
conducted	O
based	O
on	O
a	O
range	O
of	O
datasets	O
,	O
which	O
demonstrate	O
the	O
superiority	O
of	O
L2MM	Data/Mining/Information/Retrieval-technique
and	O
validate	O
the	O
significance	O
of	O
high	O
-	O
quality	O
representations	O
as	O
well	O
as	O
mobility	O
patterns	O
.	O

With	O
real	O
-	O
world	O
social	O
media	O
and	O
e	O
-	O
commerce	O
data	O
,	O
we	O
show	O
that	O
the	O
integration	O
can	O
improve	O
accuracy	Classification-metrics
by	O
up	O
to	O
14	Numerical-result
%	Numerical-result
while	O
using	O
the	O
same	O
data	O
.	O

With	O
real	O
-	O
world	O
social	O
media	O
and	O
e	O
-	O
commerce	O
data	O
,	O
we	O
show	O
that	O
the	O
integration	O
can	O
improve	O
accuracy	Classification-metrics
by	O
up	O
to	O
14	Numerical-result
%	Numerical-result
while	O
using	O
the	O
same	O
data	O
.	O

In	O
this	O
regard	O
,	O
we	O
propose	O
a	O
dual	Data/Mining/Information/Retrieval-technique
subgraph	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
pairwise	Data/Mining/Information/Retrieval-technique
graph	Data/Mining/Information/Retrieval-technique
neural	Data/Mining/Information/Retrieval-technique
network	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
DSGNN	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
for	O
friendship	O
prediction	O
in	O
LBSNs	O
which	O
extracts	O
a	O
pairwise	O
social	O
subgraph	O
and	O
a	O
trajectory	O
subgraph	O
to	O
model	O
the	O
social	O
proximity	O
and	O
mobility	O
similarity	O
respectively	O
.	O

In	O
particular	O
,	O
the	O
comparative	O
experiments	O
on	O
the	O
trajectory	O
level	O
mobility	O
similarity	O
further	O
validate	O
the	O
effectiveness	O
of	O
the	O
designed	O
trajectory	Data/Mining/Information/Retrieval-technique
subgraph	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
method	Data/Mining/Information/Retrieval-technique
which	O
can	O
extract	O
predictive	O
mobility	O
features	O
.	O

In	O
this	O
article	O
,	O
we	O
propose	O
a	O
novel	O
transfer	O
learning	O
model	O
,	O
called	O
DNformer	Data/Mining/Information/Retrieval-technique
to	O
predict	O
temporal	O
link	O
sequence	O
in	O
dynamic	O
networks	O
.	O

It	O
achieves	O
the	O
three	O
highest	O
AUC	Classification-metrics
and	O
four	O
highest	O
precision	Classification-metrics
scores	O
in	O
five	O
different	O
representative	O
dynamic	O
networks	O
problems	O
.	O

We	O
introduce	O
CODEtect	Data/Mining/Information/Retrieval-technique
the	O
first	O
approach	O
that	O
addresses	O
the	O
anomaly	O
detection	O
task	O
for	O
graph	O
databases	O
with	O
such	O
complex	O
nature	O
.	O

CODEtect	Data/Mining/Information/Retrieval-technique
exhibits	O
two	O
novel	O
building	O
blocks	O
:	O
(	O
i	O
)	O
a	O
motif	O
-	O
based	O
lossless	O
graph	O
encoding	O
scheme	O
,	O
and	O
(	O
ii	O
)	O
fast	O
memory	O
-	O
efficient	O
search	O
algorithms	O
for	O
P	O
.	O

We	O
show	O
the	O
effectiveness	O
of	O
CODEtect	Data/Mining/Information/Retrieval-technique
on	O
transaction	O
graph	O
databases	O
from	O
three	O
different	O
corporations	O
and	O
statistically	O
similar	O
synthetic	O
datasets	O
,	O
where	O
existing	O
baselines	O
adjusted	O
for	O
the	O
task	O
fall	O
behind	O
significantly	O
,	O
across	O
different	O
types	O
of	O
anomalies	O
and	O
performance	O
metrics	O
.	O

Large	O
-	O
scale	O
,	O
well	O
-	O
labeled	O
datasets	O
are	O
difficult	O
to	O
obtain	O
,	O
and	O
building	O
label	O
correlation	O
maps	O
requires	O
task	O
-	O
specific	O
semantic	NLP-algorithm/tool
information	NLP-algorithm/tool
as	O
prior	O
knowledge	O
.	O

To	O
address	O
these	O
limitations	O
,	O
we	O
propose	O
a	O
general	O
and	O
compact	O
Multi	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Label	Data/Mining/Information/Retrieval-technique
Correlation	Data/Mining/Information/Retrieval-technique
Learning	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
MUCO	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
framework	O
.	O

MUCO	Data/Mining/Information/Retrieval-technique
explicitly	O
and	O
effectively	O
learns	O
the	O
latent	O
label	O
correlations	O
by	O
updating	O
a	O
label	O
correlation	O
tensor	O
which	O
provides	O
highly	O
accurate	O
and	O
interpretable	O
prediction	O
results	O
.	O

Extensive	O
experiments	O
illustrate	O
the	O
effectiveness	O
and	O
efficiency	O
of	O
MUCO	Data/Mining/Information/Retrieval-technique
.	O

We	O
propose	O
an	O
anchor	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
bipartite	Data/Mining/Information/Retrieval-technique
graph	Data/Mining/Information/Retrieval-technique
embedding	Data/Mining/Information/Retrieval-technique
approach	Data/Mining/Information/Retrieval-technique
to	O
accelerate	O
the	O
learning	O
process	O
.	O

First	O
,	O
we	O
lay	O
out	O
a	O
set	O
of	O
desirable	O
properties	O
that	O
such	O
an	O
equivalence	O
criterion	O
should	O
have	O
and	O
why	O
;	O
second	O
,	O
we	O
propose	O
Gaussian	Data/Mining/Information/Retrieval-technique
Equivalence	Data/Mining/Information/Retrieval-technique
Criterion	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
GEC	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
as	O
equivalence	O
criterion	O
and	O
show	O
mathematically	O
that	O
it	O
has	O
the	O
desirable	O
properties	O
previously	O
mentioned	O
.	O

For	O
the	O
real	O
-	O
world	O
dataset	O
,	O
we	O
show	O
how	O
GEC	Data/Mining/Information/Retrieval-technique
can	O
provide	O
insight	O
about	O
the	O
anomaly	O
detection	O
algorithms	O
as	O
well	O
as	O
the	O
dataset	O
.	O

Structured	NLP-focus
text	NLP-focus
classification	NLP-focus
is	O
attracting	O
more	O
attention	O
in	O
natural	O
language	O
processing	O
due	O
to	O
the	O
increasing	O
complexity	O
of	O
application	O
scenarios	O
.	O

To	O
integrate	O
the	O
global	O
hierarchical	O
features	O
with	O
fused	O
structured	O
text	O
information	O
,	O
we	O
design	O
a	O
hierarchical	NLP-algorithm/tool
LDA	NLP-algorithm/tool
module	NLP-algorithm/tool
and	O
a	O
structured	NLP-algorithm/tool
text	NLP-algorithm/tool
embedding	NLP-algorithm/tool
module	NLP-algorithm/tool
.	O

Finally	O
,	O
the	O
fusion	O
embedding	O
and	O
the	O
meta	O
-	O
information	O
can	O
be	O
straightforwardly	O
incorporated	O
for	O
structured	NLP-focus
text	NLP-focus
classification	NLP-focus
.	O

TD	O
DNN	O
74	Numerical-result
.	Numerical-result
4	Numerical-result
%	Numerical-result
are	O
feasible	O
.	O

10	O
–	O
17	O
y	O
/	O
old	O
CNN	O
80	Numerical-result
.	Numerical-result
5	Numerical-result
%	Numerical-result
,	O
gender	O
(	O
Male	O
vs	O
.	O

Female	O
ASD	O
:	O
DNN	O
78	Numerical-result
.	Numerical-result
0	Numerical-result
%	Numerical-result
and	O
the	O
mixture	O
of	O
age	O
and	O
gender	O
(	O
5	O
–	O
9	O
y	O
/	O
old	O
Male	O
vs	O
.	O

5	O
–	O
9	O
y	O
/	O
old	O
Female	O
ASD	O
DNN	O
98	Numerical-result
.	Numerical-result
8	Numerical-result
%	Numerical-result
.	O

Spatio	O
-	O
Temporal	O
scan	O
-	O
paths	O
that	O
incorporate	O
velocity	O
of	O
eye	O
movement	O
in	O
their	O
images	O
of	O
eye	O
-	O
gaze	O
are	O
shown	O
to	O
outperform	O
other	O
feature	O
representation	O
methods	O
achieving	O
classification	O
accuracy	Classification-metrics
of	O
80	Numerical-result
.	Numerical-result
25	Numerical-result
%	Numerical-result
Conclusion	O
:	O
The	O
results	O
indicate	O
the	O
feasibility	O
of	O
scan	O
-	O
path	O
images	O
to	O
stratify	O
ASD	O
and	O
TD	O
diagnosis	O
in	O
children	O
of	O
varying	O
ages	O
and	O
gender	O
.	O

Spatio	O
-	O
Temporal	O
scan	O
-	O
paths	O
that	O
incorporate	O
velocity	O
of	O
eye	O
movement	O
in	O
their	O
images	O
of	O
eye	O
-	O
gaze	O
are	O
shown	O
to	O
outperform	O
other	O
feature	O
representation	O
methods	O
achieving	O
classification	O
accuracy	Classification-metrics
of	O
80	Numerical-result
.	Numerical-result
25	Numerical-result
%	Numerical-result
Conclusion	O
:	O
The	O
results	O
indicate	O
the	O
feasibility	O
of	O
scan	O
-	O
path	O
images	O
to	O
stratify	O
ASD	O
and	O
TD	O
diagnosis	O
in	O
children	O
of	O
varying	O
ages	O
and	O
gender	O
.	O

Infusion	O
of	O
temporal	O
information	O
and	O
velocity	O
data	O
improves	O
the	O
classification	O
performance	O
of	O
our	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
models	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
deep	O
hybrid	O
probabilistic	O
graph	O
-	O
based	O
forecasting	O
framework	O
called	O
Graph	Data/Mining/Information/Retrieval-technique
Deep	Data/Mining/Information/Retrieval-technique
Factors	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
GraphDF	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
that	O
goes	O
beyond	O
these	O
two	O
extremes	O
by	O
allowing	O
nodes	O
and	O
their	O
time	O
-	O
series	O
to	O
be	O
connected	O
to	O
others	O
in	O
an	O
arbitrary	O
fashion	O
.	O

GraphDF	Data/Mining/Information/Retrieval-technique
is	O
a	O
hybrid	O
forecasting	O
framework	O
that	O
consists	O
of	O
a	O
relational	O
global	O
and	O
relational	O
local	O
model	O
.	O

In	O
particular	O
,	O
a	O
relational	O
global	O
model	O
learns	O
complex	O
non	O
-	O
linear	O
time	O
-	O
series	O
patterns	O
globally	O
using	O
the	O
structure	O
of	O
the	O
graph	O
to	O
improve	O
both	O
forecasting	O
accuracy	Classification-metrics
and	O
computational	O
efficiency	O
.	O

The	O
experiments	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
deep	O
hybrid	O
graph	O
-	O
based	O
forecasting	O
model	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
terms	O
of	O
its	O
forecasting	O
accuracy	Classification-metrics
runtime	O
and	O
scalability	O
.	O

Our	O
case	O
study	O
reveals	O
that	O
GraphDF	Data/Mining/Information/Retrieval-technique
can	O
successfully	O
generate	O
cloud	O
usage	O
forecasts	O
and	O
opportunistically	O
schedule	O
workloads	O
to	O
increase	O
cloud	O
cluster	O
utilization	O
by	O
47	Numerical-result
.	Numerical-result
5	Numerical-result
%	Numerical-result
on	O
average	O
.	O

Our	O
case	O
study	O
reveals	O
that	O
GraphDF	Data/Mining/Information/Retrieval-technique
can	O
successfully	O
generate	O
cloud	O
usage	O
forecasts	O
and	O
opportunistically	O
schedule	O
workloads	O
to	O
increase	O
cloud	O
cluster	O
utilization	O
by	O
47	Numerical-result
.	Numerical-result
5	Numerical-result
%	Numerical-result
on	O
average	O
.	O

The	O
framework	O
can	O
be	O
universally	O
applied	O
to	O
many	O
other	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
based	O
methods	O
.	O

Recent	O
advancements	O
in	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
techniques	O
have	O
transformed	O
the	O
area	O
of	O
semantic	NLP-focus
text	NLP-focus
matching	NLP-focus
(	NLP-focus
STM	NLP-focus
)	NLP-focus
.	O

Recent	O
advancements	O
in	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
techniques	O
have	O
transformed	O
the	O
area	O
of	O
semantic	NLP-focus
text	NLP-focus
matching	NLP-focus
(	NLP-focus
STM	NLP-focus
)	NLP-focus
.	O

Besides	O
outperforming	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
the	O
document	NLP-focus
matching	NLP-focus
task	O
,	O
CoLDE	O
is	O
also	O
robust	O
to	O
changes	O
in	O
document	O
length	O
and	O
text	O
perturbations	O
and	O
provides	O
interpretable	O
results	O
.	O

We	O
propose	O
a	O
dynamic	O
pricing	O
mechanism	O
named	O
CrowdPricer	Data/Mining/Information/Retrieval-technique
for	O
incentively	O
delivering	O
bonuses	O
to	O
the	O
crowd	O
workers	O
of	O
completing	O
tasks	O
,	O
in	O
addition	O
to	O
offering	O
a	O
base	O
payment	O
for	O
completing	O
a	O
task	O
.	O

CrowdPricer	Data/Mining/Information/Retrieval-technique
makes	O
decisions	O
on	O
whether	O
to	O
provide	O
bonuses	O
on	O
workers	O
,	O
so	O
as	O
to	O
maximize	O
the	O
requester	O
’	O
s	O
utility	O
in	O
expectation	O
.	O

Extensive	O
experiments	O
using	O
both	O
a	O
real	O
crowdsourcing	O
platform	O
and	O
simulations	O
demonstrate	O
that	O
CrowdPricer	Data/Mining/Information/Retrieval-technique
yields	O
the	O
higher	O
utility	O
for	O
the	O
requester	O
.	O

In	O
experiments	O
,	O
the	O
proposed	O
method	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
session	O
-	O
based	O
recommendation	O
systems	O
on	O
three	O
real	O
-	O
world	O
datasets	O
,	O
achieving	O
4	O
%	O
improvement	O
of	O
Recall	Classification-metrics
over	O
the	O
SOTAs	O
on	O
Jdata	O
dataset	O
.	O

Furthermore	O
,	O
we	O
propose	O
a	O
new	O
recommendation	O
model	O
namely	O
AHOR	Data/Mining/Information/Retrieval-technique
to	O
jointly	O
distill	O
rating	O
-	O
based	O
features	O
and	O
review	O
-	O
based	O
features	O
which	O
are	O
derived	O
from	O
ratings	O
and	O
reviews	O
,	O
respectively	O
.	O

As	O
an	O
important	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
task	O
,	O
multi	O
-	O
label	O
feature	O
selection	O
has	O
received	O
considerable	O
attention	O
in	O
recent	O
years	O
due	O
to	O
its	O
promising	O
performance	O
in	O
dealing	O
with	O
high	O
-	O
dimensional	O
multi	O
-	O
label	O
data	O
.	O

They	O
can	O
also	O
enhance	O
the	O
training	O
of	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
algorithms	O
on	O
time	O
series	O
data	O
through	O
noise	O
reduction	O
and	O
reduced	O
sensitivity	O
to	O
hyperparameters	O
.	O

Here	O
,	O
we	O
present	O
a	O
new	O
variant	O
of	O
the	O
ABBA	O
method	O
,	O
called	O
fABBA	Data/Mining/Information/Retrieval-technique
.	O

Through	O
extensive	O
tests	O
,	O
we	O
demonstrate	O
that	O
the	O
new	O
method	O
significantly	O
outperforms	O
ABBA	O
with	O
a	O
considerable	O
reduction	O
in	O
runtime	O
while	O
also	O
outperforming	O
the	O
popular	O
SAX	O
and	O
1d	O
-	O
SAX	O
representations	O
in	O
terms	O
of	O
reconstruction	Classification-metrics
accuracy	Classification-metrics
.	O

We	O
further	O
demonstrate	O
that	O
fABBA	Data/Mining/Information/Retrieval-technique
can	O
compress	O
other	O
data	O
types	O
such	O
as	O
images	O
.	O

In	O
this	O
article	O
,	O
as	O
for	O
the	O
process	O
of	O
graph	O
pattern	O
matching	O
and	O
rematching	O
with	O
a	O
preferred	O
expert	O
set	O
,	O
i	O
.	O
e	O
.,	O
the	O
DM	O
hopes	O
that	O
one	O
or	O
more	O
experts	O
in	O
this	O
set	O
will	O
appear	O
in	O
matched	O
subgraphs	O
,	O
we	O
propose	O
a	O
Dual	Data/Mining/Information/Retrieval-technique
Simulation	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
Edge	Data/Mining/Information/Retrieval-technique
Sequencing	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
oriented	Data/Mining/Information/Retrieval-technique
Semi	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Supervised	Data/Mining/Information/Retrieval-technique
GPM	Data/Mining/Information/Retrieval-technique
method	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
DsEs	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
ssGPM	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
.	O

In	O
addition	O
,	O
considering	O
a	O
preferred	O
expert	O
set	O
and	O
a	O
dispreferred	O
expert	O
set	O
together	O
,	O
the	O
DM	O
dispreferred	O
expert	O
set	O
e	O
dispreferred	O
expert	O
set	O
will	O
not	O
appear	O
in	O
final	O
matches	O
,	O
so	O
we	O
have	O
the	O
DsEs	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
ssGPM	Data/Mining/Information/Retrieval-technique
+	Data/Mining/Information/Retrieval-technique
method	O
.	O

Technically	O
,	O
these	O
DsEs	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
ssGPM	Data/Mining/Information/Retrieval-technique
methods	O
conduct	O
the	O
matching	O
process	O
from	O
the	O
preferred	O
expert	O
set	O
during	O
dual	O
simulation	O
-	O
based	O
edge	O
sequencing	O
and	O
based	O
on	O
the	O
edge	O
sequence	O
these	O
edges	O
are	O
searched	O
recursively	O
.	O

Especially	O
,	O
as	O
for	O
the	O
rematching	O
process	O
,	O
when	O
the	O
preferred	O
and	O
/	O
or	O
the	O
dispreferred	O
expert	O
sets	O
change	O
continuously	O
,	O
to	O
process	O
the	O
GPM	O
again	O
is	O
unnecessary	O
and	O
it	O
is	O
possible	O
to	O
revise	O
the	O
previous	O
matched	O
results	O
partially	O
with	O
DsEs	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
ssGPM	Data/Mining/Information/Retrieval-technique
methods	O
.	O

Experiments	O
on	O
four	O
large	O
datasets	O
demonstrate	O
the	O
effectiveness	O
,	O
efficiency	O
and	O
stability	O
of	O
our	O
proposed	O
DsEs	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
ssGPM	Data/Mining/Information/Retrieval-technique
methods	O
,	O
and	O
the	O
necessity	O
of	O
introducing	O
an	O
edge	O
sequencing	O
mechanism	O
.	O

To	O
this	O
end	O
,	O
we	O
propose	O
a	O
novel	O
algorithm	O
named	O
View	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
specific	Data/Mining/Information/Retrieval-technique
and	Data/Mining/Information/Retrieval-technique
Consensus	Data/Mining/Information/Retrieval-technique
Graph	Data/Mining/Information/Retrieval-technique
Alignment	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
VCGA	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
for	O
multi	O
-	O
view	O
clustering	O
which	O
simultaneously	O
formulates	O
the	O
multi	O
-	O
view	O
individuality	O
and	O
the	O
multi	O
-	O
view	O
commonality	O
into	O
a	O
unified	O
framework	O
to	O
effectively	O
partition	O
data	O
points	O
.	O

To	O
be	O
specific	O
,	O
the	O
VCGA	Data/Mining/Information/Retrieval-technique
model	O
constructs	O
the	O
view	O
-	O
specific	O
graphs	O
and	O
the	O
shared	O
graph	O
from	O
original	O
multi	O
-	O
view	O
data	O
and	O
hidden	O
latent	O
representation	O
respectively	O
.	O

Inspired	O
by	O
the	O
recent	O
success	O
of	O
Generative	O
Adversarial	O
Network	O
(	O
GAN	O
)	O
GAN	O
d	O
models	O
in	O
several	O
applications	O
,	O
we	O
propose	O
a	O
GAN	O
based	O
model	O
for	O
signed	O
networks	O
SigGAN	Data/Mining/Information/Retrieval-technique
.	O

Comparing	O
the	O
performance	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
techniques	O
on	O
five	O
real	O
-	O
world	O
datasets	O
validates	O
the	O
effectiveness	O
of	O
SigGAN	Data/Mining/Information/Retrieval-technique
.	O

To	O
address	O
the	O
above	O
challenges	O
,	O
in	O
this	O
article	O
,	O
we	O
propose	O
a	O
novel	O
traffic	O
prediction	O
framework	O
,	O
named	O
Dynamic	Data/Mining/Information/Retrieval-technique
Graph	Data/Mining/Information/Retrieval-technique
Convolutional	Data/Mining/Information/Retrieval-technique
Recurrent	Data/Mining/Information/Retrieval-technique
Network	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
DGCRN	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
.	O

In	O
DGCRN	Data/Mining/Information/Retrieval-technique
hyper	O
-	O
networks	O
are	O
designed	O
to	O
leverage	O
and	O
extract	O
dynamic	O
characteristics	O
from	O
node	O
attributes	O
while	O
the	O
parameters	O
of	O
dynamic	O
filters	O
are	O
generated	O
at	O
each	O
time	O
step	O
.	O

Furthermore	O
,	O
to	O
enhance	O
efficiency	O
and	O
performance	O
,	O
we	O
employ	O
a	O
training	O
strategy	O
for	O
DGCRN	Data/Mining/Information/Retrieval-technique
by	O
restricting	O
the	O
iteration	O
number	O
of	O
decoder	O
during	O
forward	O
and	O
backward	O
propagation	O
.	O

In	O
this	O
article	O
,	O
we	O
propose	O
a	O
faster	O
algorithm	O
called	O
US	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Rule	Data/Mining/Information/Retrieval-technique
to	O
efficiently	O
mine	O
high	O
-	O
utility	O
sequential	O
rules	O
.	O

US	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Rule	Data/Mining/Information/Retrieval-technique
also	O
proposes	O
the	O
rule	O
estimated	O
utility	O
recomputing	O
pruning	O
strategy	O
(	O
REURP	O
)	O
to	O
deal	O
with	O
sparse	O
datasets	O
.	O

Finally	O
,	O
a	O
large	O
number	O
of	O
experiments	O
on	O
different	O
datasets	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
algorithm	O
demonstrate	O
that	O
US	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Rule	Data/Mining/Information/Retrieval-technique
can	O
achieve	O
better	O
performance	O
in	O
terms	O
of	O
execution	O
time	O
,	O
memory	O
consumption	O
and	O
scalability	O
.	O

In	O
this	O
article	O
,	O
we	O
propose	O
a	O
novel	O
Multi	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
concept	Data/Mining/Information/Retrieval-technique
Representation	Data/Mining/Information/Retrieval-technique
Learning	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
McRL	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
method	O
for	O
the	O
KGC	O
task	O
,	O
which	O
mainly	O
consists	O
of	O
a	O
multi	O
-	O
concept	O
representation	O
module	O
a	O
deep	O
residual	O
attention	O
module	O
and	O
an	O
interaction	O
embedding	O
module	O
.	O

Entity	NLP-focus
resolution	NLP-focus
(	NLP-focus
ER	NLP-focus
)	NLP-focus
is	O
the	O
process	O
of	O
linking	O
records	O
that	O
refer	O
to	O
the	O
same	O
entity	O
.	O

Text	NLP-focus
augmentation	NLP-focus
is	O
a	O
strategy	O
for	O
increasing	O
the	O
diversity	O
of	O
training	O
examples	O
without	O
explicitly	O
collecting	O
new	O
data	O
.	O

Owing	O
to	O
the	O
efficiency	O
and	O
effectiveness	O
of	O
text	NLP-focus
augmentation	NLP-focus
numerous	O
augmentation	O
methodologies	O
have	O
been	O
proposed	O
.	O

Therefore	O
,	O
in	O
this	O
study	O
,	O
we	O
propose	O
an	O
effective	O
text	NLP-focus
augmentation	NLP-focus
technique	O
that	O
explicitly	O
derives	O
the	O
importance	O
of	O
manipulated	O
words	O
and	O
reflects	O
this	O
importance	O
in	O
the	O
labeling	O
of	O
augmented	O
data	O
.	O

To	O
bridge	O
the	O
gap	O
,	O
we	O
propose	O
an	O
unbiased	O
and	O
robust	O
method	O
called	O
DENC	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
De	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Bias	Data/Mining/Information/Retrieval-technique
Network	Data/Mining/Information/Retrieval-technique
Confounding	Data/Mining/Information/Retrieval-technique
in	Data/Mining/Information/Retrieval-technique
Recommendation	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
inspired	O
by	O
confounder	O
analysis	O
in	O
causal	O
inference	O
.	O

In	O
general	O
,	O
DENC	Data/Mining/Information/Retrieval-technique
provides	O
a	O
causal	O
analysis	O
on	O
MNAR	O
from	O
both	O
the	O
inherent	O
factors	O
(	O
e	O
.	O
g	O
.,	O
latent	O
user	O
or	O
item	O
factors	O
)	O
and	O
auxiliary	O
network	O
’	O
s	O
perspective	O
.	O

Particularly	O
,	O
the	O
proposed	O
exposure	O
model	O
in	O
DENC	Data/Mining/Information/Retrieval-technique
can	O
control	O
the	O
social	O
network	O
confounder	O
meanwhile	O
preserve	O
the	O
observed	O
exposure	O
information	O
.	O

We	O
also	O
develop	O
a	O
deconfounding	O
model	O
through	O
the	O
balanced	O
representation	O
learning	O
to	O
retain	O
the	O
primary	O
user	O
and	O
item	O
features	O
,	O
which	O
enables	O
DENC	Data/Mining/Information/Retrieval-technique
generalize	O
well	O
on	O
the	O
rating	O
prediction	O
.	O

With	O
the	O
development	O
of	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
deep	O
hashing	O
methods	O
show	O
more	O
advantages	O
than	O
traditional	O
methods	O
.	O

After	O
training	O
the	O
accuracy	Classification-metrics
of	O
the	O
prediction	O
model	O
can	O
reach	O
up	O
to	O
82	Numerical-result
%	Numerical-result
.	O

After	O
training	O
the	O
accuracy	Classification-metrics
of	O
the	O
prediction	O
model	O
can	O
reach	O
up	O
to	O
82	Numerical-result
%	Numerical-result
.	O

The	O
result	O
has	O
proven	O
not	O
only	O
the	O
outperformance	O
of	O
our	O
proposed	O
rainfall	O
prediction	O
method	O
in	O
terms	O
of	O
cost	O
and	O
prediction	O
time	O
,	O
but	O
also	O
its	O
accuracy	Classification-metrics
and	O
feasibility	O
compared	O
with	O
general	O
prediction	O
methods	O
.	O

One	O
key	O
objective	O
of	O
artificial	AI/ML/DL-domain
intelligence	AI/ML/DL-domain
involves	O
the	O
continuous	O
adaptation	O
of	O
machine	O
learning	O
models	O
to	O
new	O
tasks	O
.	O

To	O
address	O
this	O
limitation	O
,	O
we	O
propose	O
a	O
novel	O
Graph	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
based	Data/Mining/Information/Retrieval-technique
Spatial	Data/Mining/Information/Retrieval-technique
Dependency	Data/Mining/Information/Retrieval-technique
modeling	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
GSD	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
module	O
,	O
which	O
focuses	O
on	O
explicitly	O
modeling	O
complex	O
geographical	O
influences	O
by	O
leveraging	O
graph	O
embedding	O
.	O

GSD	Data/Mining/Information/Retrieval-technique
captures	O
two	O
types	O
of	O
geographical	O
influences	O
,	O
i	O
.	O
e	O
.,	O
distance	O
-	O
based	O
and	O
transition	O
-	O
based	O
influences	O
from	O
designed	O
POI	O
semantic	O
graphs	O
.	O

Additionally	O
,	O
we	O
propose	O
a	O
novel	O
Graph	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
enhanced	Data/Mining/Information/Retrieval-technique
Spatial	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Temporal	Data/Mining/Information/Retrieval-technique
network	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
GSTN	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
which	O
incorporates	O
user	O
spatial	O
and	O
temporal	O
dependencies	O
for	O
next	O
POI	O
recommendation	O
.	O

Specifically	O
,	O
GSTN	Data/Mining/Information/Retrieval-technique
consists	O
of	O
a	O
Long	O
Short	O
-	O
Term	O
Memory	O
(	O
LSTM	O
)	O
network	O
for	O
user	O
-	O
specific	O
temporal	O
dependencies	O
modeling	O
and	O
GSD	Data/Mining/Information/Retrieval-technique
for	O
user	O
spatial	O
dependencies	O
learning	O
.	O

Extensive	O
experiments	O
demonstrate	O
the	O
effectiveness	O
of	O
GSD	O
in	O
capturing	O
various	O
geographical	O
influences	O
and	O
the	O
improvement	O
of	O
GSTN	Data/Mining/Information/Retrieval-technique
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

To	O
tackle	O
this	O
problem	O
,	O
we	O
propose	O
a	O
Self	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
supervised	Data/Mining/Information/Retrieval-technique
Transformer	Data/Mining/Information/Retrieval-technique
for	Data/Mining/Information/Retrieval-technique
Time	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
Series	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
STraTS	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
model	O
,	O
which	O
overcomes	O
these	O
pitfalls	O
by	O
treating	O
time	O
-	O
series	O
as	O
a	O
set	O
of	O
observation	O
triplets	O
instead	O
of	O
using	O
the	O
standard	O
dense	O
matrix	O
representation	O
.	O

In	O
addition	O
,	O
to	O
tackle	O
the	O
problem	O
of	O
limited	O
availability	O
of	O
labeled	O
data	O
(	O
which	O
is	O
typically	O
observed	O
in	O
many	O
healthcare	O
applications	O
),	O
STraTS	Data/Mining/Information/Retrieval-technique
utilizes	O
self	O
-	O
supervision	O
by	O
leveraging	O
unlabeled	O
data	O
to	O
learn	O
better	O
representations	O
by	O
using	O
time	O
-	O
series	O
forecasting	O
as	O
an	O
auxiliary	O
proxy	O
task	O
.	O

Experiments	O
on	O
real	O
-	O
world	O
multivariate	O
clinical	O
time	O
-	O
series	O
benchmark	O
datasets	O
demonstrate	O
that	O
STraTS	Data/Mining/Information/Retrieval-technique
has	O
better	O
prediction	O
performance	O
than	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
mortality	O
prediction	O
,	O
especially	O
when	O
labeled	O
data	O
is	O
limited	O
.	O

Finally	O
,	O
we	O
also	O
present	O
an	O
interpretable	O
version	O
of	O
STraTS	Data/Mining/Information/Retrieval-technique
which	O
can	O
identify	O
important	O
measurements	O
in	O
the	O
time	O
-	O
series	O
data	O
.	O

To	O
tackle	O
this	O
problem	O
,	O
we	O
propose	O
a	O
novel	O
incremental	O
Feature	Data/Mining/Information/Retrieval-technique
spaces	Data/Mining/Information/Retrieval-technique
Learning	Data/Mining/Information/Retrieval-technique
with	Data/Mining/Information/Retrieval-technique
Label	Data/Mining/Information/Retrieval-technique
Scarcity	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
FLLS	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
algorithm	O
,	O
together	O
with	O
its	O
two	O
variants	O
.	O

We	O
theoretically	O
analyze	O
the	O
error	O
bounds	O
of	O
FLLS	Data/Mining/Information/Retrieval-technique
and	O
its	O
two	O
variants	O
.	O

Named	NLP-focus
Entity	NLP-focus
Recognition	NLP-focus
(	NLP-focus
NER	NLP-focus
)	NLP-focus
the	O
first	O
step	O
of	O
information	NLP-focus
extraction	NLP-focus
mainly	O
identifies	O
names	O
of	O
persons	O
,	O
locations	O
,	O
and	O
organizations	O
in	O
text	O
.	O

Although	O
existing	O
neural	NLP-algorithm/tool
-	NLP-algorithm/tool
based	NLP-algorithm/tool
NER	NLP-algorithm/tool
approaches	O
achieve	O
great	O
success	O
in	O
many	O
language	O
domains	O
,	O
most	O
of	O
them	O
normally	O
ignore	O
the	O
nested	O
nature	O
of	O
named	O
entities	O
.	O

Recently	O
,	O
diverse	O
studies	O
focus	O
on	O
the	O
nested	NLP-focus
NER	NLP-focus
problem	NLP-focus
and	O
yield	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

This	O
survey	O
attempts	O
to	O
provide	O
a	O
comprehensive	O
review	O
on	O
existing	O
approaches	O
for	O
nested	NLP-focus
NER	NLP-focus
from	O
the	O
perspectives	O
of	O
the	O
model	O
architecture	O
and	O
the	O
model	O
property	O
,	O
which	O
may	O
help	O
readers	O
have	O
a	O
better	O
understanding	O
of	O
the	O
current	O
research	O
status	O
and	O
ideas	O
.	O

In	O
this	O
survey	O
,	O
we	O
first	O
introduce	O
the	O
background	O
of	O
nested	NLP-focus
NER	NLP-focus
nested	NLP-focus
NER	NLP-focus
NER	NLP-focus
differences	O
between	O
nested	O
NER	O
and	O
traditional	O
(	O
i	O
.	O
e	O
.,	O
flat	O
)	O
NER	O
.	O

We	O
then	O
review	O
the	O
existing	O
nested	NLP-focus
NER	NLP-focus
approaches	O
from	O
2002	O
to	O
2020	O
and	O
mainly	O
classify	O
them	O
into	O
five	O
categories	O
according	O
to	O
the	O
model	O
architecture	O
including	O
early	O
rule	O
-	O
based	O
layered	O
-	O
based	O
region	O
-	O
based	O
hypergraph	O
-	O
based	O
and	O
transition	O
-	O
based	O
approaches	O
.	O

We	O
also	O
explore	O
in	O
greater	O
depth	O
the	O
impact	O
of	O
key	O
properties	O
unique	O
to	O
nested	NLP-focus
NER	NLP-focus
approaches	O
from	O
the	O
model	O
property	O
perspective	O
,	O
namely	NLP-focus
entity	NLP-focus
dependency	NLP-focus
stage	O
framework	O
error	O
propagation	O
and	O
tag	O
scheme	O
.	O

This	O
survey	O
would	O
be	O
useful	O
for	O
three	O
kinds	O
of	O
readers	O
:	O
(	O
i	O
)	O
Newcomers	O
in	O
the	O
field	O
who	O
want	O
to	O
learn	O
about	O
NER	NLP-focus
especially	O
for	O
nested	NLP-focus
NER	NLP-focus
.	O

(	O
ii	O
)	O
Researchers	O
who	O
want	O
to	O
clarify	O
the	O
relationship	O
and	O
advantages	O
between	O
flat	O
NER	NLP-focus
and	O
nested	NLP-focus
NER	NLP-focus
.	O

(	O
iii	O
)	O
Practitioners	O
who	O
just	O
need	O
to	O
determine	O
which	O
NER	NLP-focus
technique	O
(	O
i	O
.	O
e	O
.,	O
nested	O
or	O
not	O
)	O
works	O
best	O
in	O
their	O
applications	O
.	O

Under	O
such	O
an	O
environment	O
,	O
many	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
problems	O
can	O
be	O
reformulated	O
as	O
a	O
consensus	O
optimization	O
problem	O
,	O
which	O
consists	O
of	O
one	O
objective	O
and	O
constraint	O
terms	O
splitting	O
into	O
N	O
parts	O
(	O
each	O
corresponds	O
to	O
a	O
node	O
).	O

Experimentally	O
,	O
we	O
show	O
that	O
the	O
proposed	O
framework	O
is	O
more	O
efficient	O
and	O
effective	O
than	O
existing	O
ADMM	O
based	O
solutions	O
on	O
both	O
synthetic	O
and	O
real	O
-	O
world	O
datasets	O
due	O
to	O
its	O
faster	O
convergence	O
rate	O
and	O
higher	O
accuracy	Classification-metrics
.	O

To	O
solve	O
the	O
above	O
problems	O
,	O
a	O
two	O
-	O
stage	O
data	O
pre	O
-	O
processing	O
framework	O
for	O
noise	O
identification	O
and	O
data	O
reduction	O
,	O
called	O
ARIS	Data/Mining/Information/Retrieval-technique
is	O
proposed	O
in	O
this	O
article	O
.	O

The	O
performance	O
of	O
ARIS	Data/Mining/Information/Retrieval-technique
is	O
verified	O
by	O
experiments	O
on	O
artificial	O
and	O
real	O
datasets	O
.	O

Experimental	O
results	O
show	O
that	O
ARIS	Data/Mining/Information/Retrieval-technique
effectively	O
weakens	O
the	O
impact	O
of	O
noise	O
and	O
reduces	O
the	O
amount	O
of	O
data	O
and	O
significantly	O
improves	O
the	O
accuracy	Classification-metrics
of	O
data	O
analysis	O
within	O
a	O
reasonable	O
time	O
cost	O
range	O
.	O

Experimental	O
results	O
show	O
that	O
ARIS	Data/Mining/Information/Retrieval-technique
effectively	O
weakens	O
the	O
impact	O
of	O
noise	O
and	O
reduces	O
the	O
amount	O
of	O
data	O
and	O
significantly	O
improves	O
the	O
accuracy	Classification-metrics
of	O
data	O
analysis	O
within	O
a	O
reasonable	O
time	O
cost	O
range	O
.	O

We	O
propose	O
the	O
HIN	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
assisted	Data/Mining/Information/Retrieval-technique
upper	Data/Mining/Information/Retrieval-technique
confidence	Data/Mining/Information/Retrieval-technique
bound	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
HUCB	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
algorithm	O
to	O
address	O
such	O
a	O
challenge	O
.	O

For	O
each	O
meta	O
-	O
path	O
the	O
HUCB	Data/Mining/Information/Retrieval-technique
meta	O
-	O
path	O
employs	O
an	O
independent	O
base	O
bandit	O
algorithm	O
to	O
handle	O
online	O
item	O
recommendations	O
by	O
leveraging	O
the	O
relationship	O
captured	O
in	O
this	O
meta	O
-	O
path	O
.	O

We	O
theoretically	O
prove	O
that	O
the	O
HUCB	Data/Mining/Information/Retrieval-technique
algorithm	O
algorithm	O
e	O
similar	O
performance	O
compared	O
with	O
the	O
optimal	O
algorithm	O
where	O
each	O
user	O
is	O
served	O
according	O
to	O
his	O
true	O
preference	O
over	O
meta	O
-	O
paths	O
algorithm	O
the	O
optimal	O
algorithm	O
knows	O
the	O
preference	O
).	O

Moreover	O
,	O
we	O
prove	O
that	O
the	O
HUCB	Data/Mining/Information/Retrieval-technique
algorithm	O
benefits	O
from	O
leveraging	O
HIN	O
in	O
achieving	O
a	O
smaller	O
regret	O
upper	O
bound	O
algorithm	O
HIN	O
ine	O
algorithm	O
without	O
leveraging	O
HIN	O
.	O

Experimental	O
results	O
on	O
a	O
synthetic	O
dataset	O
as	O
well	O
as	O
real	O
datasets	O
from	O
LastFM	O
and	O
Yelp	O
demonstrate	O
the	O
fast	O
learning	O
speed	O
of	O
the	O
HUCB	Data/Mining/Information/Retrieval-technique
algorithm	O
.	O

We	O
introduce	O
a	O
method	O
for	O
robust	O
Boolean	O
model	O
selection	O
called	O
BMFk	Data/Mining/Information/Retrieval-technique
BMFk	Data/Mining/Information/Retrieval-technique
show	O
on	O
numerical	O
examples	O
that	O
BMFk	O
not	O
only	O
accurately	O
determines	O
the	O
correct	O
number	O
of	O
Boolean	O
latent	O
features	O
but	O
reconstruct	O
the	O
pre	O
-	O
determined	O
factors	O
accurately	O
.	O

Although	O
there	O
exist	O
few	O
methods	O
to	O
summarize	O
a	O
large	O
-	O
scale	O
graph	O
,	O
they	O
do	O
not	O
deal	O
with	O
heterogeneous	O
graphs	O
with	O
hierarchical	O
node	O
labels	O
We	O
propose	O
GSHL	Data/Mining/Information/Retrieval-technique
heterogeneous	O
graph	O
summarizes	O
a	O
heterogeneous	O
graph	O
with	O
hierarchical	O
labels	O
.	O

GSHL	Data/Mining/Information/Retrieval-technique
exploits	O
the	O
formulation	O
to	O
identify	O
and	O
segment	O
subgraphs	O
and	O
discovers	O
compact	O
and	O
consistent	O
structures	O
in	O
the	O
graph	O
.	O

Experiments	O
on	O
a	O
large	O
real	O
-	O
world	O
MMORPG	O
graph	O
with	O
multi	O
-	O
million	O
edges	O
show	O
that	O
GSHL	Data/Mining/Information/Retrieval-technique
is	O
a	O
useful	O
and	O
scalable	O
tool	O
for	O
summarizing	O
the	O
graph	O
,	O
finding	O
important	O
structures	O
in	O
the	O
graph	O
,	O
and	O
finding	O
similar	O
users	O
.	O

To	O
this	O
end	O
,	O
in	O
this	O
article	O
,	O
we	O
propose	O
a	O
dynamic	Data/Mining/Information/Retrieval-technique
region	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
relation	Data/Mining/Information/Retrieval-technique
-	Data/Mining/Information/Retrieval-technique
aware	Data/Mining/Information/Retrieval-technique
graph	Data/Mining/Information/Retrieval-technique
neural	Data/Mining/Information/Retrieval-technique
network	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
DRRGNN	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
for	O
exploring	O
individual	O
mobility	O
behaviors	O
over	O
ARs	O
.	O

Regularization	O
that	O
incorporates	O
the	O
linear	O
combination	O
of	O
empirical	O
loss	O
and	O
explicit	O
regularization	O
terms	O
as	O
the	O
loss	O
function	O
has	O
been	O
frequently	O
used	O
for	O
many	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
tasks	O
.	O

While	O
regularized	O
learning	O
often	O
boost	O
the	O
performance	O
with	O
higher	O
accuracy	Classification-metrics
and	O
faster	O
convergence	O
,	O
the	O
regularization	O
would	O
sometimes	O
hurt	O
the	O
empirical	O
loss	O
minimization	O
and	O
lead	O
to	O
poor	O
performance	O
.	O

We	O
propose	O
a	O
framework	O
for	O
building	O
data	O
generation	O
models	O
and	O
evaluating	O
their	O
effectiveness	O
regarding	O
those	O
accuracy	Classification-metrics
and	O
privacy	O
measures	O
.	O

This	O
comparison	O
reveals	O
the	O
memory	O
versus	O
accuracy	Classification-metrics
trade	O
-	O
off	O
involved	O
in	O
achieving	O
good	O
segmentation	O
performance	O
.	O

Against	O
this	O
backdrop	O
,	O
the	O
broad	O
success	O
of	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	AI/ML/DL-domain
DL	AI/ML/DL-domain
)	AI/ML/DL-domain
has	O
prompted	O
the	O
development	O
of	O
new	O
image	O
segmentation	O
approaches	O
leveraging	O
DL	O
models	O
.	O

Contrary	O
to	O
conventional	O
approaches	O
to	O
AI	AI/ML/DL-domain
where	O
tasks	O
are	O
solved	O
from	O
scratch	O
using	O
a	O
fixed	O
learning	O
algorithm	O
meta	O
-	O
learning	O
learning	O
algorithm	O
learning	O
algorithm	O
itself	O
,	O
given	O
the	O
experience	O
of	O
multiple	O
learning	O
episodes	O
.	O

This	O
paradigm	O
provides	O
an	O
opportunity	O
to	O
tackle	O
many	O
conventional	O
challenges	O
of	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
including	O
data	O
and	O
computation	O
bottlenecks	O
,	O
as	O
well	O
as	O
generalization	O
.	O

We	O
conduct	O
human	O
evaluation	O
on	O
a	O
standard	O
8	O
×	O
face	O
super	O
-	O
resolution	O
task	O
on	O
CelebA	O
-	O
HQ	O
for	O
which	O
SR3	O
achieves	O
a	O
fool	O
rate	O
close	O
to	O
50	O
%,	O
suggesting	O
photo	O
-	O
realistic	O
outputs	O
,	O
while	O
GAN	O
baselines	O
do	O
not	O
exceed	O
a	O
fool	O
rate	O
of	O
34	Numerical-result
%	Numerical-result
.	O

We	O
evaluate	O
SR3	O
on	O
a	O
4	O
×	O
super	O
-	O
resolution	O
task	O
on	O
ImageNet	O
SR3	O
re	O
SR3	O
outperforms	O
baselines	O
in	O
human	O
evaluation	O
and	O
classification	Classification-metrics
accuracy	Classification-metrics
of	O
a	O
ResNet	O
-	O
50	O
classifier	O
trained	O
on	O
high	O
-	O
resolution	O
images	O
.	O

We	O
further	O
show	O
the	O
effectiveness	O
of	O
SR3	O
in	O
cascaded	O
image	O
generation	O
where	O
a	O
generative	O
model	O
is	O
chained	O
with	O
super	O
-	O
resolution	O
models	O
to	O
synthesize	O
high	O
-	O
resolution	O
images	O
with	O
competitive	O
FID	Statistical/Mathematical-metrics
scores	O
on	O
the	O
class	O
-	O
conditional	O
256	O
×	O
256	O
ImageNet	O
generation	O
challenge	O
.	O

Computational	O
biology	O
and	O
bioinformatics	O
provide	O
vast	O
data	O
gold	O
-	O
mines	O
from	O
protein	O
sequences	O
,	O
ideal	O
for	O
Language	NLP-algorithm/tool
Models	NLP-algorithm/tool
(	NLP-algorithm/tool
LMs	NLP-algorithm/tool
)	NLP-algorithm/tool
taken	O
from	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
.	O

These	O
LMs	NLP-algorithm/tool
reach	O
for	O
new	O
prediction	O
frontiers	O
at	O
low	O
inference	O
costs	O
.	O

Here	O
,	O
we	O
trained	O
two	O
auto	O
-	O
regressive	O
models	O
(	O
Transformer	O
-	O
XL	O
XLNet	O
and	O
four	O
auto	O
-	O
encoder	O
models	O
(	O
BERT	NLP-algorithm/tool
Albert	NLP-algorithm/tool
Electra	NLP-algorithm/tool
T5	NLP-algorithm/tool
on	O
data	O
from	O
UniRef	O
and	O
BFD	O
containing	O
up	O
to	O
393	O
billion	O
amino	O
acids	O
.	O

We	O
validated	O
the	O
advantage	O
of	O
using	O
the	O
embeddings	O
as	O
exclusive	O
input	O
for	O
several	O
subsequent	O
tasks	O
:	O
(	O
1	O
)	O
a	O
per	O
-	O
residue	O
(	O
per	O
-	O
token	O
)	O
prediction	O
of	O
protein	O
secondary	O
structure	O
(	O
3	Classification-metrics
-	Classification-metrics
state	Classification-metrics
accuracy	Classification-metrics
Q3	O
=	O
81	Numerical-result
%-	Numerical-result
87	Numerical-result
%	Numerical-result
;	O
(	O
2	O
)	O
per	O
-	O
protein	O
(	O
pooling	O
)	O
predictions	O
of	O
protein	O
sub	O
-	O
cellular	O
location	O
(	O
ten	Classification-metrics
-	Classification-metrics
state	Classification-metrics
accuracy	Classification-metrics
81	Numerical-result
%	Numerical-result
=	O
81	O
%)	O
and	O
membrane	O
versus	O
water	O
-	O
soluble	O
(	O
2	Classification-metrics
-	Classification-metrics
state	Classification-metrics
accuracy	Classification-metrics
Q2	O
=	O
91	Numerical-result
%	Numerical-result
.	O

We	O
validated	O
the	O
advantage	O
of	O
using	O
the	O
embeddings	O
as	O
exclusive	O
input	O
for	O
several	O
subsequent	O
tasks	O
:	O
(	O
1	O
)	O
a	O
per	O
-	O
residue	O
(	O
per	O
-	O
token	O
)	O
prediction	O
of	O
protein	O
secondary	O
structure	O
(	O
3	Classification-metrics
-	Classification-metrics
state	Classification-metrics
accuracy	Classification-metrics
Q3	O
=	O
81	Numerical-result
%-	Numerical-result
87	Numerical-result
%	Numerical-result
;	O
(	O
2	O
)	O
per	O
-	O
protein	O
(	O
pooling	O
)	O
predictions	O
of	O
protein	O
sub	O
-	O
cellular	O
location	O
(	O
ten	Classification-metrics
-	Classification-metrics
state	Classification-metrics
accuracy	Classification-metrics
81	Numerical-result
%	Numerical-result
=	O
81	O
%)	O
and	O
membrane	O
versus	O
water	O
-	O
soluble	O
(	O
2	Classification-metrics
-	Classification-metrics
state	Classification-metrics
accuracy	Classification-metrics
Q2	O
=	O
91	Numerical-result
%	Numerical-result
.	O

For	O
the	O
very	O
deep	O
VGG	O
-	O
16	O
model	O
[	O
3	O
],	O
our	O
detection	O
system	O
has	O
a	O
frame	O
rate	O
of	O
5	O
fps	O
(	O
including	O
all	O
steps	O
)	O
on	O
a	O
GPU	O
,	O
while	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
object	O
detection	O
accuracy	Classification-metrics
on	O
PASCAL	O
VOC	O
2007	O
,	O
2012	O
and	O
MS	O
COCO	O
datasets	O
with	O
only	O
300	O
proposals	O
per	O
image	O
.	O

It	O
covers	O
several	O
novel	O
and	O
insightful	O
components	O
:	O
1	O
)	O
besides	O
supervision	O
with	O
binary	O
label	O
(	O
e	O
.	O
g	O
.,	O
‘	O
0	O
’	O
for	O
bonafide	O
versus	O
‘	O
1	O
’	O
for	O
PAs	O
,	O
we	O
also	O
investigate	O
recent	O
methods	O
with	O
pixel	O
-	O
wise	O
supervision	O
(	O
e	O
.	O
g	O
.,	O
pseudo	O
depth	O
map	O
;	O
2	O
)	O
in	O
addition	O
to	O
traditional	O
intra	O
-	O
dataset	O
evaluation	O
,	O
we	O
collect	O
and	O
analyze	O
the	O
latest	O
methods	O
specially	O
designed	O
for	O
domain	O
generalization	O
and	O
open	O
-	O
set	O
FAS	O
;	O
and	O
3	O
)	O
besides	O
commercial	O
RGB	O
camera	O
,	O
we	O
summarize	O
the	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
applications	O
under	O
multi	O
-	O
modal	O
(	O
e	O
.	O
g	O
.,	O
depth	O
and	O
infrared	O
)	O
or	O
specialized	O
(	O
e	O
.	O
g	O
.,	O
light	O
field	O
and	O
flash	O
)	O
sensors	O
.	O

In	O
recent	O
years	O
,	O
advancements	O
in	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	AI/ML/DL-domain
ML	AI/ML/DL-domain
)	AI/ML/DL-domain
techniques	O
,	O
in	O
particular	O
,	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
(	AI/ML/DL-domain
DL	AI/ML/DL-domain
)	AI/ML/DL-domain
methods	O
have	O
gained	O
a	O
lot	O
of	O
momentum	O
in	O
solving	O
inverse	O
imaging	O
problems	O
,	O
often	O
surpassing	O
the	O
performance	O
provided	O
by	O
hand	O
-	O
crafted	O
approaches	O
.	O

Unlike	O
analytical	O
methods	O
for	O
which	O
the	O
problem	O
is	O
explicitly	O
defined	O
and	O
the	O
domain	O
knowledge	O
is	O
carefully	O
engineered	O
into	O
the	O
solution	O
,	O
DL	AI/ML/DL-domain
models	O
do	O
not	O
benefit	O
from	O
such	O
prior	O
knowledge	O
and	O
instead	O
make	O
use	O
of	O
large	O
datasets	O
to	O
predict	O
an	O
unknown	O
solution	O
to	O
the	O
inverse	O
problem	O
.	O

Hamiltonian	O
learning	O
as	O
an	O
important	O
quantum	AI/ML/DL-domain
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
technique	O
,	O
provides	O
a	O
significant	O
approach	O
for	O
determining	O
an	O
accurate	O
quantum	O
system	O
.	O

Most	O
value	O
function	O
learning	O
algorithms	O
in	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
are	O
based	O
on	O
the	O
mean	O
squared	O
(	O
projected	O
)	O
Bellman	O
error	O
.	O

As	O
a	O
dominating	O
technique	O
in	O
AI	AI/ML/DL-domain
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
has	O
been	O
successfully	O
used	O
to	O
solve	O
various	O
2D	O
vision	O
problems	O
.	O

Recently	O
,	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
on	O
point	O
clouds	O
has	O
become	O
even	O
thriving	O
,	O
with	O
numerous	O
methods	O
being	O
proposed	O
to	O
address	O
different	O
problems	O
in	O
this	O
area	O
.	O

To	O
stimulate	O
future	O
research	O
,	O
this	O
paper	O
presents	O
a	O
comprehensive	O
review	O
of	O
recent	O
progress	O
in	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
methods	O
for	O
point	O
clouds	O
.	O

In	O
this	O
work	O
we	O
address	O
the	O
task	O
of	O
semantic	O
image	O
segmentation	O
with	O
Deep	AI/ML/DL-domain
Learning	AI/ML/DL-domain
and	O
make	O
three	O
main	O
contributions	O
that	O
are	O
experimentally	O
shown	O
to	O
have	O
substantial	O
practical	O
merit	O
.	O

Our	O
proposed	O
“	O
DeepLab	O
”	O
system	O
sets	O
the	O
new	O
state	O
-	O
of	O
-	O
art	O
at	O
the	O
PASCAL	O
VOC	O
-	O
2012	O
semantic	O
image	O
segmentation	O
task	O
,	O
reaching	O
79	Numerical-result
.	Numerical-result
7	Numerical-result
percent	O
mIOU	Statistical/Mathematical-metrics
in	O
the	O
test	O
set	O
,	O
and	O
advances	O
the	O
results	O
on	O
three	O
other	O
datasets	O
PASCAL	O
-	O
Context	O
PASCAL	O
-	O
Person	O
-	O
Part	O
and	O
Cityscapes	O
.	O

Our	O
proposed	O
“	O
DeepLab	O
”	O
system	O
sets	O
the	O
new	O
state	O
-	O
of	O
-	O
art	O
at	O
the	O
PASCAL	O
VOC	O
-	O
2012	O
semantic	O
image	O
segmentation	O
task	O
,	O
reaching	O
79	Numerical-result
.	Numerical-result
7	Numerical-result
percent	O
mIOU	Statistical/Mathematical-metrics
in	O
the	O
test	O
set	O
,	O
and	O
advances	O
the	O
results	O
on	O
three	O
other	O
datasets	O
PASCAL	O
-	O
Context	O
PASCAL	O
-	O
Person	O
-	O
Part	O
and	O
Cityscapes	O
.	O

In	O
order	O
for	O
Artificial	AI/ML/DL-domain
Intelligence	AI/ML/DL-domain
to	O
make	O
progress	O
in	O
understanding	O
the	O
world	O
around	O
us	O
,	O
it	O
needs	O
to	O
be	O
able	O
to	O
interpret	O
such	O
multimodal	O
signals	O
together	O
.	O

Instead	O
of	O
focusing	O
on	O
specific	O
multimodal	O
applications	O
,	O
this	O
paper	O
surveys	O
the	O
recent	O
advances	O
in	O
multimodal	AI/ML/DL-domain
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
itself	O
and	O
presents	O
them	O
in	O
a	O
common	O
taxonomy	O
.	O

The	O
success	O
of	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
algorithms	O
generally	O
depends	O
on	O
data	O
representation	O
,	O
and	O
we	O
hypothesize	O
that	O
this	O
is	O
because	O
different	O
representations	O
can	O
entangle	O
and	O
hide	O
more	O
or	O
less	O
the	O
different	O
explanatory	O
factors	O
of	O
variation	O
behind	O
the	O
data	O
.	O

Although	O
specific	O
domain	O
knowledge	O
can	O
be	O
used	O
to	O
help	O
design	O
representations	O
,	O
learning	O
with	O
generic	O
priors	O
can	O
also	O
be	O
used	O
,	O
and	O
the	O
quest	O
for	O
AI	AI/ML/DL-domain
is	O
motivating	O
the	O
design	O
of	O
more	O
powerful	O
representation	O
-	O
learning	O
algorithms	O
implementing	O
such	O
priors	O
.	O

Over	O
the	O
last	O
ten	O
years	O
,	O
research	O
in	O
DG	O
DG	O
made	O
great	O
progress	O
,	O
leading	O
to	O
a	O
broad	O
spectrum	O
of	O
methodologies	O
,	O
e	O
.	O
g	O
.,	O
those	O
based	O
on	O
domain	O
alignment	O
,	O
meta	O
-	O
learning	O
,	O
data	O
augmentation	O
,	O
or	O
ensemble	O
learning	O
,	O
to	O
name	O
a	O
few	O
;	O
DG	O
has	O
also	O
been	O
studied	O
in	O
various	O
application	O
areas	O
including	O
computer	O
vision	O
speech	O
recognition	O
natural	O
language	O
processing	O
medical	O
imaging	O
and	O
reinforcement	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

Transformer	O
is	O
a	O
promising	O
neural	O
network	O
learner	O
,	O
and	O
has	O
achieved	O
great	O
success	O
in	O
various	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
tasks	O
.	O

Thanks	O
to	O
the	O
recent	O
prevalence	O
of	O
multimodal	O
applications	O
and	O
Big	AI/ML/DL-domain
Data	AI/ML/DL-domain
Transformer	O
-	O
based	O
multimodal	O
learning	O
has	O
become	O
a	O
hot	O
topic	O
in	O
AI	AI/ML/DL-domain
research	O
.	O

We	O
propose	O
a	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
method	O
for	O
single	O
image	O
super	O
-	O
resolution	O
(	O
SR	O
)	O
.	O

In	O
this	O
article	O
,	O
we	O
present	O
a	O
comprehensive	O
survey	O
of	O
recent	O
progress	O
in	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
methods	O
for	O
HAR	O
based	O
on	O
the	O
type	O
of	O
input	O
data	O
modality	O
.	O

Specifically	O
,	O
we	O
review	O
the	O
current	O
mainstream	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
methods	O
for	O
single	O
data	O
modalities	O
and	O
multiple	O
data	O
modalities	O
including	O
the	O
fusion	O
-	O
based	O
and	O
the	O
co	O
-	O
learning	O
-	O
based	O
frameworks	O
.	O

Recent	O
years	O
have	O
witnessed	O
remarkable	O
progress	O
of	O
image	O
super	O
-	O
resolution	O
using	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
techniques	O
.	O

This	O
article	O
aims	O
to	O
provide	O
a	O
comprehensive	O
survey	O
on	O
recent	O
advances	O
of	O
image	O
super	O
-	O
resolution	O
using	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
approaches	O
.	O

The	O
experiments	O
show	O
that	O
the	O
presented	O
approach	O
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
direct	O
and	O
indirect	O
methods	O
in	O
a	O
variety	O
of	O
real	O
-	O
world	O
settings	O
,	O
both	O
in	O
terms	O
of	O
tracking	O
accuracy	Classification-metrics
and	O
robustness	O
.	O

Deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
methods	O
are	O
achieving	O
ever	O
-	O
increasing	O
performance	O
on	O
many	O
artificial	AI/ML/DL-domain
intelligence	AI/ML/DL-domain
tasks	O
.	O

This	O
paper	O
provides	O
an	O
extensive	O
review	O
of	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
based	O
self	O
-	O
supervised	O
general	O
visual	O
feature	O
learning	O
methods	O
from	O
images	O
or	O
videos	O
.	O

Therefore	O
,	O
the	O
stumbling	O
blocks	O
in	O
applying	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
for	O
image	O
fusion	O
e	O
.	O
g	O
.,	O
the	O
requirement	O
of	O
ground	O
-	O
truth	O
and	O
specifically	O
designed	O
metrics	O
,	O
are	O
greatly	O
mitigated	O
.	O

This	O
bottom	O
-	O
up	O
system	O
achieves	O
high	O
accuracy	Classification-metrics
and	O
realtime	O
performance	O
,	O
regardless	O
of	O
the	O
number	O
of	O
people	O
in	O
the	O
image	O
.	O

We	O
demonstrate	O
that	O
a	O
PAF	O
PAF	O
refinement	O
rather	O
than	O
both	O
PAF	O
and	O
body	O
part	O
location	O
refinement	O
results	O
in	O
a	O
substantial	O
increase	O
in	O
both	O
runtime	O
performance	O
and	O
accuracy	Classification-metrics
.	O

We	O
show	O
that	O
the	O
combined	O
detector	O
not	O
only	O
reduces	O
the	O
inference	O
time	O
compared	O
to	O
running	O
them	O
sequentially	O
,	O
but	O
also	O
maintains	O
the	O
accuracy	Classification-metrics
of	O
each	O
component	O
individually	O
.	O

The	O
highest	O
accuracy	Classification-metrics
object	O
detectors	O
to	O
date	O
are	O
based	O
on	O
a	O
two	O
-	O
stage	O
approach	O
popularized	O
by	O
R	O
-	O
CNN	O
where	O
a	O
classifier	O
is	O
applied	O
to	O
a	O
sparse	O
set	O
of	O
candidate	O
object	O
locations	O
.	O

Our	O
results	O
show	O
that	O
when	O
trained	O
with	O
the	O
focal	O
loss	O
,	O
RetinaNet	O
is	O
able	O
to	O
match	O
the	O
speed	O
of	O
previous	O
one	O
-	O
stage	O
detectors	O
while	O
surpassing	O
the	O
accuracy	Classification-metrics
of	O
all	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
two	O
-	O
stage	O
detectors	O
.	O

The	O
results	O
confirm	O
that	O
mRMR	O
leads	O
to	O
promising	O
improvement	O
on	O
feature	O
selection	O
and	O
classification	O
accuracy	Classification-metrics
.	O

However	O
,	O
the	O
breakthrough	O
in	O
neural	O
networks	O
accuracy	Classification-metrics
is	O
always	O
accompanied	O
by	O
explosive	O
growth	O
of	O
computation	O
and	O
parameters	O
,	O
which	O
leads	O
to	O
a	O
severe	O
limitation	O
of	O
model	O
deployment	O
.	O

On	O
average	O
,	O
3	Numerical-result
.	Numerical-result
49	Numerical-result
and	O
2	O
.	O
32	O
percent	O
accuracy	Classification-metrics
boost	O
are	O
observed	O
on	O
CIFAR100	O
and	O
ImageNet	O
.	O

On	O
average	O
,	O
3	Numerical-result
.	Numerical-result
49	Numerical-result
and	O
2	O
.	O
32	O
percent	O
accuracy	Classification-metrics
boost	O
are	O
observed	O
on	O
CIFAR100	O
and	O
ImageNet	O
.	O

The	O
widely	O
studied	O
closed	O
-	O
world	O
setting	O
is	O
usually	O
applied	O
under	O
various	O
research	O
-	O
oriented	O
assumptions	O
,	O
and	O
has	O
achieved	O
inspiring	O
success	O
using	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
techniques	O
on	O
a	O
number	O
of	O
datasets	O
.	O

Starting	O
from	O
2015	O
the	O
task	O
has	O
generally	O
been	O
addressed	O
with	O
pipelines	O
composed	O
of	O
a	O
visual	O
encoder	O
and	O
a	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
for	O
text	NLP-focus
generation	NLP-focus
.	O

Starting	O
from	O
2015	O
the	O
task	O
has	O
generally	O
been	O
addressed	O
with	O
pipelines	O
composed	O
of	O
a	O
visual	O
encoder	O
and	O
a	O
language	NLP-algorithm/tool
model	NLP-algorithm/tool
for	O
text	NLP-focus
generation	NLP-focus
.	O

During	O
these	O
years	O
,	O
both	O
components	O
have	O
evolved	O
considerably	O
through	O
the	O
exploitation	O
of	O
object	O
regions	O
,	O
attributes	O
,	O
the	O
introduction	O
of	O
multi	O
-	O
modal	O
connections	O
,	O
fully	O
-	O
attentive	O
approaches	O
,	O
and	O
BERT	NLP-algorithm/tool
like	O
early	O
-	O
fusion	O
strategies	O
.	O

Though	O
network	O
pruning	O
receives	O
popularity	O
in	O
reducing	O
the	O
complexity	O
of	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
it	O
remains	O
an	O
open	O
issue	O
to	O
concurrently	O
maintain	O
model	O
accuracy	Classification-metrics
as	O
well	O
as	O
achieve	O
significant	O
speedups	O
on	O
general	O
CPUs	O
.	O

We	O
also	O
provide	O
a	O
workflow	O
of	O
filter	O
rearrangement	O
that	O
first	O
rearranges	O
the	O
weight	O
matrix	O
in	O
the	O
output	O
channel	O
dimension	O
to	O
derive	O
more	O
influential	O
blocks	O
for	O
accuracy	Classification-metrics
improvements	O
and	O
then	O
applies	O
similar	O
rearrangement	O
to	O
the	O
next	O
-	O
layer	O
weights	O
in	O
the	O
input	O
channel	O
dimension	O
to	O
ensure	O
correct	O
convolutional	O
operations	O
.	O

For	O
example	O
,	O
given	O
the	O
pruning	O
rate	O
of	O
50	O
%	O
and	O
N	O
=	O
4	O
,	O
our	O
pattern	O
obtains	O
about	O
3	O
.	O
0	O
%	O
improvements	O
over	O
filter	O
pruning	O
in	O
the	O
top	Classification-metrics
-	Classification-metrics
1	Classification-metrics
accuracy	Classification-metrics
of	O
MobileNet	O
-	O
V2	O
.	O

We	O
propose	O
a	O
novel	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
approach	O
for	O
monocular	O
dense	O
human	O
performance	O
capture	O
.	O

Thanks	O
to	O
the	O
numerical	O
stability	O
of	O
deterministic	O
training	O
,	O
our	O
method	O
also	O
improves	O
prediction	O
accuracy	Classification-metrics
.	O

Hyperparameter	O
optimization	O
(	O
HPO	O
)	O
characterized	O
by	O
hyperparameter	O
tuning	O
is	O
not	O
only	O
a	O
critical	O
step	O
for	O
effective	O
modeling	O
but	O
also	O
is	O
the	O
most	O
time	O
-	O
consuming	O
process	O
in	O
machine	AI/ML/DL-domain
learning	AI/ML/DL-domain
.	O

Furthermore	O
,	O
we	O
propose	O
a	O
novel	O
knowledge	O
distillation	O
framework	O
to	O
jointly	O
train	O
the	O
encoder	O
and	O
the	O
action	O
recognition	O
model	O
action	O
recognition	O
roposed	O
training	O
approach	O
improves	O
the	O
action	O
recognition	O
accuracy	Classification-metrics
by	O
an	O
absolute	O
margin	O
of	O
6	Numerical-result
.	Numerical-result
2	Numerical-result
%	Numerical-result
2	Numerical-result
.	Numerical-result
9	Numerical-result
%	Numerical-result
and	O
7	Numerical-result
.	Numerical-result
9	Numerical-result
%	Numerical-result
on	O
Something	O
$^{	O
2	O
}$	O
2	O
-	O
v2	O
Kinetics	O
-	O
400	O
and	O
UCF	O
-	O
101	O
datasets	O
respectively	O
,	O
in	O
comparison	O
to	O
our	O
previous	O
approach	O
.	O

Furthermore	O
,	O
we	O
propose	O
a	O
novel	O
knowledge	O
distillation	O
framework	O
to	O
jointly	O
train	O
the	O
encoder	O
and	O
the	O
action	O
recognition	O
model	O
action	O
recognition	O
roposed	O
training	O
approach	O
improves	O
the	O
action	O
recognition	O
accuracy	Classification-metrics
by	O
an	O
absolute	O
margin	O
of	O
6	Numerical-result
.	Numerical-result
2	Numerical-result
%	Numerical-result
2	Numerical-result
.	Numerical-result
9	Numerical-result
%	Numerical-result
and	O
7	Numerical-result
.	Numerical-result
9	Numerical-result
%	Numerical-result
on	O
Something	O
$^{	O
2	O
}$	O
2	O
-	O
v2	O
Kinetics	O
-	O
400	O
and	O
UCF	O
-	O
101	O
datasets	O
respectively	O
,	O
in	O
comparison	O
to	O
our	O
previous	O
approach	O
.	O

Open	O
set	O
recognition	O
enables	O
deep	O
neural	O
networks	O
(	O
DNNs	O
)	O
to	O
identify	O
samples	O
of	O
unknown	O
classes	O
,	O
while	O
maintaining	O
high	O
classification	O
accuracy	Classification-metrics
on	O
samples	O
of	O
known	O
classes	O
.	O

This	O
article	O
provides	O
an	O
extensive	O
overview	O
of	O
deep	AI/ML/DL-domain
learning	AI/ML/DL-domain
based	O
algorithms	O
to	O
tackle	O
temporal	O
action	O
detection	O
in	O
untrimmed	O
videos	O
with	O
different	O
supervision	O
levels	O
including	O
fully	O
-	O
supervised	O
weakly	O
-	O
supervised	O
unsupervised	O
self	O
-	O
supervised	O
and	O
semi	O
-	O
supervised	O
.	O

We	O
argue	O
that	O
such	O
a	O
mechanism	O
has	O
fundamental	O
limitations	O
in	O
building	O
an	O
effective	O
regression	O
loss	O
for	O
rotation	O
detection	O
,	O
especially	O
for	O
high	O
-	O
precision	O
detection	O
with	O
high	O
IoU	Statistical/Mathematical-metrics
(	O
e	O
.	O
g	O
.,	O
0	Numerical-result
.	Numerical-result
75	Numerical-result
.	O

We	O
argue	O
that	O
such	O
a	O
mechanism	O
has	O
fundamental	O
limitations	O
in	O
building	O
an	O
effective	O
regression	O
loss	O
for	O
rotation	O
detection	O
,	O
especially	O
for	O
high	O
-	O
precision	O
detection	O
with	O
high	O
IoU	Statistical/Mathematical-metrics
(	O
e	O
.	O
g	O
.,	O
0	Numerical-result
.	Numerical-result
75	Numerical-result
.	O

Spell	NLP-algorithm/tool
checkers	NLP-algorithm/tool
are	O
widely	O
used	O
to	O
highlight	O
textual	O
errors	O
,	O
yet	O
no	O
equivalent	O
tool	O
exists	O
to	O
detect	O
algebraically	O
incorrect	O
formulae	O
.	O

Additionally	O
,	O
$\	O
text	O
{	O
L	O
}{	O
A	O
}\	O
text	O
{	O
C	O
}{\	O
scriptsize	O
\	O
text	O
{	O
AS	O
}}\	O
text	O
{	O
T	O
}$	O
LACAST	O
achieved	O
an	O
F1	Classification-metrics
score	O
of	O
.	Numerical-result
495	Numerical-result
for	O
annotating	O
mathematical	O
expressions	O
with	O
relevant	O
textual	O
descriptions	O
,	O
which	O
is	O
a	O
significant	O
step	O
towards	O
advancing	O
searchability	O
,	O
readability	O
,	O
and	O
accessibility	O
of	O
mathematical	O
formulae	O
in	O
Wikipedia	O
.	O

Additionally	O
,	O
$\	O
text	O
{	O
L	O
}{	O
A	O
}\	O
text	O
{	O
C	O
}{\	O
scriptsize	O
\	O
text	O
{	O
AS	O
}}\	O
text	O
{	O
T	O
}$	O
LACAST	O
achieved	O
an	O
F1	Classification-metrics
score	O
of	O
.	Numerical-result
495	Numerical-result
for	O
annotating	O
mathematical	O
expressions	O
with	O
relevant	O
textual	O
descriptions	O
,	O
which	O
is	O
a	O
significant	O
step	O
towards	O
advancing	O
searchability	O
,	O
readability	O
,	O
and	O
accessibility	O
of	O
mathematical	O
formulae	O
in	O
Wikipedia	O
.	O

Typically	O
,	O
we	O
achieves	O
2	O
-	O
4	O
×	O
computation	O
reduction	O
and	O
up	O
to	O
61	O
.	O
5	O
%	O
real	O
-	O
world	O
acceleration	O
on	O
MobileNet	O
ResNet	O
-	O
50	O
and	O
Vision	O
Transformer	O
with	O
minimal	O
accuracy	Classification-metrics
drops	O
on	O
ImageNet	O
.	O

The	O
main	O
contributions	O
of	O
the	O
present	O
work	O
include	O
:	O
(	O
i	O
)	O
a	O
new	O
descent	O
direction	O
for	O
the	O
rank	O
-	O
one	O
SNMF	O
is	O
derived	O
and	O
a	O
strategy	O
for	O
choosing	O
the	O
step	O
size	O
along	O
this	O
descent	O
direction	O
is	O
established	O
;	O
(	O
ii	O
)	O
a	O
progressive	Data/Mining/Information/Retrieval-technique
hierarchical	Data/Mining/Information/Retrieval-technique
alternating	Data/Mining/Information/Retrieval-technique
least	Data/Mining/Information/Retrieval-technique
squares	Data/Mining/Information/Retrieval-technique
(	Data/Mining/Information/Retrieval-technique
PHALS	Data/Mining/Information/Retrieval-technique
)	Data/Mining/Information/Retrieval-technique
SNMF	O
d	O
for	O
SNMF	O
is	O
developed	O
,	O
which	O
is	O
parameter	O
-	O
free	O
and	O
updates	O
the	O
variables	O
column	O
by	O
column	O
.	O

Moreover	O
,	O
every	O
column	O
is	O
updated	O
by	O
solving	O
a	O
rank	O
-	O
one	O
SNMF	O
subproblem	O
;	O
and	O
(	O
iii	O
)	O
the	O
convergence	O
to	O
the	O
Karush	O
-	O
Kuhn	O
-	O
Tucker	O
(	O
KKT	O
)	O
point	O
set	O
(	O
or	O
the	O
stationary	O
point	O
set	O
)	O
is	O
proved	O
for	O
PHALS	Data/Mining/Information/Retrieval-technique
.	O

Our	O
PHALS	Data/Mining/Information/Retrieval-technique
provides	O
better	O
performance	O
in	O
terms	O
of	O
the	O
computational	O
accuracy	Classification-metrics
the	O
optimality	O
gap	O
,	O
and	O
the	O
CPU	O
time	O
,	O
compared	O
with	O
a	O
number	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
SNMF	O
methods	O
.	O

Our	O
PHALS	Data/Mining/Information/Retrieval-technique
provides	O
better	O
performance	O
in	O
terms	O
of	O
the	O
computational	O
accuracy	Classification-metrics
the	O
optimality	O
gap	O
,	O
and	O
the	O
CPU	O
time	O
,	O
compared	O
with	O
a	O
number	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
SNMF	O
methods	O
.	O

